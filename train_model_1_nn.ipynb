{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net model 1\n",
    "\n",
    "- V0: HyperOpt NN with one layer\n",
    "    - Minimum achieved around 15 epochs (test with that)\n",
    "- V1: very sensitive to random seed, redid gridsearch with new random seed\n",
    "\n",
    "Scores:\n",
    "- V0: CV: 2.2372, LB: 2.2428\n",
    "    - The solver is very sensitive random seed! \n",
    "    -{'seed1': 239, 'seed2': 1234, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 179.31129328655433, 'dropout2': 0.27147227595028317}, 'optimizer': 'adamax', 'layer_1': {'units1': 141.28833823317106, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.6945806925899082}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'n_epoch': 9, 'batch_size': 400}\n",
    "- V1: CV: LB: (not uploaded directly to Kaggle yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='tensorflow'\n",
    "#os.environ['THEANO_FLAGS'] = 'device=cpu'\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "import keras\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from ml_toolbox.kaggle import KaggleResult\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_in = 'data_ori'\n",
    "dir_feat = 'data'\n",
    "sub_dir = 'model_1_nn'\n",
    "\n",
    "description = 'model_1_nn_V2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_files = ['features_brand_model_bag',\n",
    "                 'features_brand_bag',\n",
    "                 'features_appid_installed',\n",
    "                 'features_label_app_installed']\n",
    "\n",
    "# Function to read feature file\n",
    "def open_feature_file(fname, samples='train'):\n",
    "    if fname[-3:] == 'csv':\n",
    "        if samples=='train':\n",
    "            X = gatrain[['device_id']].merge( pd.read_csv(os.path.join(dir_feat, fname)), on='device_id', how='left')\n",
    "        else:\n",
    "            X = gatest[['device_id']].merge( pd.read_csv(os.path.join(dir_feat, fname)), on='device_id', how='left')\n",
    "            \n",
    "        X.drop('device_id', axis=1, inplace=True)\n",
    "        X.fillna(0, inplace=True)\n",
    "        \n",
    "        for c in X.columns:\n",
    "            if X[c].max()>1:\n",
    "                X[c] = StandardScaler().fit_transform(X)\n",
    "                \n",
    "        return csr_matrix(X.values)\n",
    "    else:\n",
    "        # Assume it is a pickle file\n",
    "        with open(os.path.join(dir_feat, '{}_{}.pickle'.format(fname,samples)), 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv(os.path.join(dir_in,'gender_age_train.csv'))\n",
    "gatest = pd.read_csv(os.path.join(dir_in,'gender_age_test.csv'))\n",
    "targetencoder = LabelEncoder().fit(gatrain.group)\n",
    "y = targetencoder.transform(gatrain.group)\n",
    "nclasses = len(targetencoder.classes_)\n",
    "\n",
    "dummy_y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21527 features\n",
      "Test on 21527 features\n"
     ]
    }
   ],
   "source": [
    "fw = [1, 1, 1, 1]\n",
    "Xtrain = hstack([open_feature_file(f) for f in feature_files],format='csr')\n",
    "Xtest = hstack([open_feature_file(f,'test') for f in feature_files],format='csr')\n",
    "\n",
    "print('Train on {} features'.format(Xtrain.shape[1]))\n",
    "print('Test on {} features'.format(Xtrain.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With selection (15xxx features): 2.27427\n",
    "# Without selection (21527 features): 2.27427\n",
    "selector = VarianceThreshold().fit(Xtrain)\n",
    "Xtrain = selector.transform(Xtrain)\n",
    "Xtest = selector.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load CV sets\n",
    "train_cv = pd.read_csv(os.path.join(dir_in, 'gender_age_train_cv.csv'))\n",
    "test_cv = pd.read_csv(os.path.join(dir_in, 'gender_age_test_cv.csv'))\n",
    "\n",
    "X_train, X_val = Xtrain[train_cv.sample_nr.values, :], Xtrain[test_cv.sample_nr.values, :]\n",
    "y_train, y_val = dummy_y[train_cv.sample_nr], dummy_y[test_cv.sample_nr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch#, np.ones(X_batch.shape[0])\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0\n",
    "            \n",
    "def nn_model(n_feat, params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['layer_1']['units1'], \n",
    "                    input_dim=n_feat, \n",
    "                    init='normal', \n",
    "                    activation=params['layer_1']['activation1']))\n",
    "\n",
    "    model.add(Dropout(params['layer_1']['dropout1']))\n",
    "\n",
    "    if params['layer_2']['on2']:\n",
    "        model.add(Dense(params['layer_2']['units2'], \n",
    "                        init='normal', \n",
    "                        activation=params['layer_2']['activation2']))\n",
    "        #https://www.kaggle.com/poiss0nriot/talkingdata-mobile-user-demographics/bag-of-apps-keras-11-08-16-no-val/run/328610\n",
    "        #model.add(PReLU())\n",
    "        model.add(Dropout(params['layer_2']['dropout2']))\n",
    "\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))    \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])  #logloss\n",
    "    \n",
    "    return model\n",
    "\n",
    "def score_nn(X_train, X_val, y_train, y_val, params, description):\n",
    "    # create model\n",
    "    print ('Model with following parameters: %s' % (params))\n",
    "    \n",
    "    np.random.seed(params['seed'])\n",
    "    random.seed(params['seed']) # not sure if needed or not\n",
    "\n",
    "    model = nn_model(X_train.shape[1], params)\n",
    "\n",
    "    hist_all = prediction_check('test', os.path.join(sub_dir, description + '_model.h5'))\n",
    "\n",
    "    fit=model.fit(X_train.todense(), y_train, nb_epoch = int(params['n_epoch']), batch_size=int(params['batch_size']),\n",
    "                             validation_data=(X_val.todense(), y_val), callbacks=[hist_all], verbose=2) \n",
    "    \n",
    "    # evaluate the model\n",
    "    pred_val = model.predict_generator(generator=batch_generatorp(X_val, 32, False), val_samples=X_val.shape[0])\n",
    "\n",
    "    cv_score = log_loss(y_val, pred_val)\n",
    "\n",
    "    print('logloss val {}'.format(cv_score))\n",
    "    \n",
    "    return cv_score, model, pred_val, hist_all.filepath\n",
    "\n",
    "class prediction_check(Callback):\n",
    "    def __init__(self, tag='', filepath=''):\n",
    "        self.predhist = pd.DataFrame(columns=['acc','logloss','val_acc','val_logloss'])\n",
    "        self.tag = str(tag)\n",
    "        self.counter = 0\n",
    "        self.best = 10.0\n",
    "        self.best_rounds = 1\n",
    "        self.wait = 0\n",
    "        self.patience = 3\n",
    "        self.filepath = filepath\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #print logs.values()\n",
    "        self.counter += 1\n",
    "        self.predhist.loc[epoch] = logs.values()\n",
    "        \n",
    "        current_score = logs['val_loss']\n",
    "        \n",
    "        if current_score < self.best:\n",
    "            self.best = current_score\n",
    "            self.best_train = logs['loss']\n",
    "            self.best_rounds = self.counter\n",
    "            self.wait = 0\n",
    "            self.model.save_weights(self.filepath, overwrite=True)\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                self.model.load_weights(self.filepath)\n",
    "                print('Best number of rounds: %d \\n Val loss: %f \\n' % (self.best_rounds, self.best))\n",
    "            self.wait += 1\n",
    "            \n",
    "    def on_train_end(self, logs={}):\n",
    "        self.model.load_weights(self.filepath)\n",
    "        self.model.save(self.filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'optimizer': 'adam',\n",
    "          'batch_size': 32,\n",
    "          'n_epoch': 4,\n",
    "         'layer_1': {'on1': True,\n",
    "                     'units1': 150,\n",
    "                     'activation1': 'relu',\n",
    "                     'dropout1': 0.6},\n",
    "         'layer_2': {'on2': False,\n",
    "                    'units2': 150,\n",
    "                    'activation2': 'tanh',\n",
    "                    'dropout2': 0.2},\n",
    "          'layer_3': {'on3': False,\n",
    "                     'units3': 12,\n",
    "                     'activation3': 'sigmoid',\n",
    "                     'dropout3': 0.2}\n",
    "          }\n",
    "# According: \n",
    "#https://www.kaggle.com/agavranis/talkingdata-mobile-user-demographics/bag-of-apps-keras-11-08-16-no-val/code\n",
    "params = {'optimizer': 'adadelta',\n",
    "          'batch_size': 400,\n",
    "          'n_epoch': 16,\n",
    "         'layer_1': {'on1': True,\n",
    "                     'units1': 150,\n",
    "                     'activation1': 'relu',\n",
    "                     'dropout1': 0.4},\n",
    "         'layer_2': {'on2': True,\n",
    "                    'units2': 50,\n",
    "                    'activation2': 'relu',\n",
    "                    'dropout2': 0.2},\n",
    "          'layer_3': {'on3': False,\n",
    "                     'units3': 12,\n",
    "                     'activation3': 'sigmoid',\n",
    "                     'dropout3': 0.2}\n",
    "          }\n",
    "\n",
    "# After 80 rounds op hyperopt evals:\n",
    "params = {'optimizer': 'adamax',\n",
    "          'batch_size': 400,\n",
    "          'n_epoch': 17,\n",
    "          'seed': 1234,\n",
    "         'layer_1': {'on1': True,\n",
    "                     'units1': 221,\n",
    "                     'activation1': 'tanh',\n",
    "                     'dropout1': 0.84},\n",
    "         'layer_2': {'on2': True,\n",
    "                    'units2': 232,\n",
    "                    'activation2': 'relu',\n",
    "                    'dropout2': 0.23},\n",
    "          'layer_3': {'on3': False,\n",
    "                     'units3': 12,\n",
    "                     'activation3': 'sigmoid',\n",
    "                     'dropout3': 0.2}\n",
    "          }\n",
    "\n",
    "# V1\n",
    "params = {'type': 'keras_nn',\n",
    "          'n_epoch': 50, # 11.406643551001192, \n",
    "          'optimizer': 'adadelta', \n",
    "          'seed': 89171,\n",
    "          'layer_1': {'units1': 400, \n",
    "                      'activation1': 'tanh', \n",
    "                      'on1': True, \n",
    "                      'dropout1': 0.8},\n",
    "          'layer_2': {'on2': True, \n",
    "                      'activation2': 'relu', \n",
    "                      'units2': 130, \n",
    "                      'dropout2': 0.35}, \n",
    "          'batch_size': 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with following parameters: {'optimizer': 'adadelta', 'layer_1': {'units1': 400, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 130, 'dropout2': 0.35}, 'batch_size': 400, 'seed': 53207, 'n_epoch': 50, 'type': 'keras_nn'}\n",
      "Train on 67229 samples, validate on 7416 samples\n",
      "Epoch 1/50\n",
      "32s - loss: 2.4423 - acc: 0.1280 - val_loss: 2.3907 - val_acc: 0.1593\n",
      "Epoch 2/50\n",
      "29s - loss: 2.3787 - acc: 0.1543 - val_loss: 2.3366 - val_acc: 0.1761\n",
      "Epoch 3/50\n",
      "27s - loss: 2.3442 - acc: 0.1665 - val_loss: 2.3121 - val_acc: 0.1824\n",
      "Epoch 4/50\n",
      "26s - loss: 2.3260 - acc: 0.1746 - val_loss: 2.2983 - val_acc: 0.1838\n",
      "Epoch 5/50\n",
      "26s - loss: 2.3114 - acc: 0.1791 - val_loss: 2.2883 - val_acc: 0.1900\n",
      "Epoch 6/50\n",
      "26s - loss: 2.3017 - acc: 0.1832 - val_loss: 2.2829 - val_acc: 0.1907\n",
      "Epoch 7/50\n",
      "25s - loss: 2.2948 - acc: 0.1873 - val_loss: 2.2842 - val_acc: 0.1904\n",
      "Epoch 8/50\n",
      "26s - loss: 2.2884 - acc: 0.1879 - val_loss: 2.2798 - val_acc: 0.1897\n",
      "Epoch 9/50\n",
      "27s - loss: 2.2831 - acc: 0.1928 - val_loss: 2.2716 - val_acc: 0.1946\n",
      "Epoch 10/50\n",
      "28s - loss: 2.2793 - acc: 0.1928 - val_loss: 2.2716 - val_acc: 0.1911\n",
      "Epoch 11/50\n",
      "28s - loss: 2.2727 - acc: 0.1960 - val_loss: 2.2699 - val_acc: 0.1963\n",
      "Epoch 12/50\n",
      "28s - loss: 2.2694 - acc: 0.1973 - val_loss: 2.2643 - val_acc: 0.1986\n",
      "Epoch 13/50\n",
      "30s - loss: 2.2680 - acc: 0.1963 - val_loss: 2.2635 - val_acc: 0.1942\n",
      "Epoch 14/50\n",
      "30s - loss: 2.2625 - acc: 0.1990 - val_loss: 2.2638 - val_acc: 0.1954\n",
      "Epoch 15/50\n",
      "32s - loss: 2.2600 - acc: 0.2007 - val_loss: 2.2595 - val_acc: 0.1965\n",
      "Epoch 16/50\n",
      "32s - loss: 2.2560 - acc: 0.2046 - val_loss: 2.2587 - val_acc: 0.1985\n",
      "Epoch 17/50\n",
      "31s - loss: 2.2545 - acc: 0.2020 - val_loss: 2.2576 - val_acc: 0.1989\n",
      "Epoch 18/50\n",
      "31s - loss: 2.2502 - acc: 0.2056 - val_loss: 2.2567 - val_acc: 0.1989\n",
      "Epoch 19/50\n",
      "31s - loss: 2.2472 - acc: 0.2076 - val_loss: 2.2581 - val_acc: 0.2011\n",
      "Epoch 20/50\n",
      "32s - loss: 2.2458 - acc: 0.2061 - val_loss: 2.2576 - val_acc: 0.1996\n",
      "Epoch 21/50\n",
      "32s - loss: 2.2429 - acc: 0.2081 - val_loss: 2.2562 - val_acc: 0.2016\n",
      "Epoch 22/50\n",
      "32s - loss: 2.2405 - acc: 0.2091 - val_loss: 2.2596 - val_acc: 0.1980\n",
      "Epoch 23/50\n",
      "33s - loss: 2.2383 - acc: 0.2091 - val_loss: 2.2545 - val_acc: 0.1970\n",
      "Epoch 24/50\n",
      "33s - loss: 2.2357 - acc: 0.2089 - val_loss: 2.2557 - val_acc: 0.2004\n",
      "Epoch 25/50\n",
      "32s - loss: 2.2322 - acc: 0.2119 - val_loss: 2.2535 - val_acc: 0.2032\n",
      "Epoch 26/50\n",
      "33s - loss: 2.2291 - acc: 0.2132 - val_loss: 2.2544 - val_acc: 0.2027\n",
      "Epoch 27/50\n",
      "31s - loss: 2.2266 - acc: 0.2144 - val_loss: 2.2546 - val_acc: 0.2005\n",
      "Epoch 28/50\n",
      "31s - loss: 2.2272 - acc: 0.2121 - val_loss: 2.2534 - val_acc: 0.2012\n",
      "Epoch 29/50\n",
      "32s - loss: 2.2266 - acc: 0.2136 - val_loss: 2.2529 - val_acc: 0.2009\n",
      "Epoch 30/50\n",
      "33s - loss: 2.2227 - acc: 0.2171 - val_loss: 2.2564 - val_acc: 0.2024\n",
      "Epoch 31/50\n",
      "32s - loss: 2.2196 - acc: 0.2160 - val_loss: 2.2562 - val_acc: 0.2029\n",
      "Epoch 32/50\n",
      "32s - loss: 2.2185 - acc: 0.2177 - val_loss: 2.2542 - val_acc: 0.2043\n",
      "Epoch 33/50\n",
      "Best number of rounds: 29 \n",
      " Val loss: 2.252864 \n",
      "\n",
      "33s - loss: 2.2159 - acc: 0.2181 - val_loss: 2.2540 - val_acc: 0.2027\n",
      "logloss val 2.25286361565\n",
      "('Validation score:', 2.252863615646028)\n",
      "Model with following parameters: {'optimizer': 'adadelta', 'layer_1': {'units1': 400, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 130, 'dropout2': 0.35}, 'batch_size': 400, 'seed': 55198, 'n_epoch': 50, 'type': 'keras_nn'}\n",
      "Train on 67229 samples, validate on 7416 samples\n",
      "Epoch 1/50\n",
      "30s - loss: 2.4381 - acc: 0.1310 - val_loss: 2.3882 - val_acc: 0.1584\n",
      "Epoch 2/50\n",
      "27s - loss: 2.3766 - acc: 0.1558 - val_loss: 2.3382 - val_acc: 0.1727\n",
      "Epoch 3/50\n",
      "28s - loss: 2.3442 - acc: 0.1661 - val_loss: 2.3100 - val_acc: 0.1837\n",
      "Epoch 4/50\n",
      "27s - loss: 2.3218 - acc: 0.1742 - val_loss: 2.3011 - val_acc: 0.1850\n",
      "Epoch 5/50\n",
      "27s - loss: 2.3124 - acc: 0.1797 - val_loss: 2.2916 - val_acc: 0.1901\n",
      "Epoch 6/50\n",
      "27s - loss: 2.3017 - acc: 0.1832 - val_loss: 2.2819 - val_acc: 0.1920\n",
      "Epoch 7/50\n",
      "27s - loss: 2.2944 - acc: 0.1872 - val_loss: 2.2821 - val_acc: 0.1927\n",
      "Epoch 8/50\n",
      "28s - loss: 2.2894 - acc: 0.1856 - val_loss: 2.2732 - val_acc: 0.1947\n",
      "Epoch 9/50\n",
      "29s - loss: 2.2822 - acc: 0.1929 - val_loss: 2.2697 - val_acc: 0.1963\n",
      "Epoch 10/50\n",
      "29s - loss: 2.2776 - acc: 0.1934 - val_loss: 2.2678 - val_acc: 0.1980\n",
      "Epoch 11/50\n",
      "31s - loss: 2.2718 - acc: 0.1949 - val_loss: 2.2679 - val_acc: 0.1993\n",
      "Epoch 12/50\n",
      "30s - loss: 2.2671 - acc: 0.1959 - val_loss: 2.2644 - val_acc: 0.1974\n",
      "Epoch 13/50\n",
      "30s - loss: 2.2652 - acc: 0.1986 - val_loss: 2.2606 - val_acc: 0.1988\n",
      "Epoch 14/50\n",
      "31s - loss: 2.2610 - acc: 0.1989 - val_loss: 2.2612 - val_acc: 0.2000\n",
      "Epoch 15/50\n",
      "31s - loss: 2.2591 - acc: 0.2014 - val_loss: 2.2586 - val_acc: 0.1998\n",
      "Epoch 16/50\n",
      "31s - loss: 2.2548 - acc: 0.2008 - val_loss: 2.2561 - val_acc: 0.1969\n",
      "Epoch 17/50\n",
      "31s - loss: 2.2513 - acc: 0.2052 - val_loss: 2.2619 - val_acc: 0.1980\n",
      "Epoch 18/50\n",
      "31s - loss: 2.2487 - acc: 0.2052 - val_loss: 2.2565 - val_acc: 0.1982\n",
      "Epoch 19/50\n",
      "32s - loss: 2.2476 - acc: 0.2027 - val_loss: 2.2542 - val_acc: 0.1998\n",
      "Epoch 20/50\n",
      "31s - loss: 2.2420 - acc: 0.2070 - val_loss: 2.2545 - val_acc: 0.1985\n",
      "Epoch 21/50\n",
      "31s - loss: 2.2421 - acc: 0.2070 - val_loss: 2.2546 - val_acc: 0.2002\n",
      "Epoch 22/50\n",
      "33s - loss: 2.2401 - acc: 0.2074 - val_loss: 2.2568 - val_acc: 0.2017\n",
      "Epoch 23/50\n",
      "39s - loss: 2.2371 - acc: 0.2099 - val_loss: 2.2508 - val_acc: 0.1988\n",
      "Epoch 24/50\n",
      "33s - loss: 2.2343 - acc: 0.2095 - val_loss: 2.2517 - val_acc: 0.2016\n",
      "Epoch 25/50\n",
      "31s - loss: 2.2310 - acc: 0.2122 - val_loss: 2.2529 - val_acc: 0.2017\n",
      "Epoch 26/50\n",
      "32s - loss: 2.2290 - acc: 0.2131 - val_loss: 2.2514 - val_acc: 0.2023\n",
      "Epoch 27/50\n",
      "Best number of rounds: 23 \n",
      " Val loss: 2.250791 \n",
      "\n",
      "30s - loss: 2.2279 - acc: 0.2134 - val_loss: 2.2540 - val_acc: 0.1981\n",
      "logloss val 2.25079094159\n",
      "('Validation score:', 2.2507909415862417)\n",
      "Model with following parameters: {'optimizer': 'adadelta', 'layer_1': {'units1': 400, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 130, 'dropout2': 0.35}, 'batch_size': 400, 'seed': 12418, 'n_epoch': 50, 'type': 'keras_nn'}\n",
      "Train on 67229 samples, validate on 7416 samples\n",
      "Epoch 1/50\n",
      "32s - loss: 2.4441 - acc: 0.1283 - val_loss: 2.3908 - val_acc: 0.1675\n",
      "Epoch 2/50\n",
      "28s - loss: 2.3818 - acc: 0.1553 - val_loss: 2.3423 - val_acc: 0.1766\n",
      "Epoch 3/50\n",
      "26s - loss: 2.3478 - acc: 0.1672 - val_loss: 2.3173 - val_acc: 0.1811\n",
      "Epoch 4/50\n",
      "25s - loss: 2.3287 - acc: 0.1740 - val_loss: 2.3012 - val_acc: 0.1838\n",
      "Epoch 5/50\n",
      "26s - loss: 2.3137 - acc: 0.1794 - val_loss: 2.2897 - val_acc: 0.1862\n",
      "Epoch 6/50\n",
      "27s - loss: 2.3048 - acc: 0.1834 - val_loss: 2.2834 - val_acc: 0.1896\n",
      "Epoch 7/50\n",
      "26s - loss: 2.2956 - acc: 0.1865 - val_loss: 2.2793 - val_acc: 0.1911\n",
      "Epoch 8/50\n",
      "26s - loss: 2.2906 - acc: 0.1890 - val_loss: 2.2761 - val_acc: 0.1915\n",
      "Epoch 9/50\n",
      "27s - loss: 2.2827 - acc: 0.1924 - val_loss: 2.2711 - val_acc: 0.1916\n",
      "Epoch 10/50\n",
      "27s - loss: 2.2787 - acc: 0.1923 - val_loss: 2.2678 - val_acc: 0.1954\n",
      "Epoch 11/50\n",
      "28s - loss: 2.2754 - acc: 0.1938 - val_loss: 2.2701 - val_acc: 0.1961\n",
      "Epoch 12/50\n",
      "29s - loss: 2.2713 - acc: 0.1958 - val_loss: 2.2627 - val_acc: 0.1969\n",
      "Epoch 13/50\n",
      "29s - loss: 2.2663 - acc: 0.1960 - val_loss: 2.2617 - val_acc: 0.1969\n",
      "Epoch 14/50\n",
      "30s - loss: 2.2619 - acc: 0.1993 - val_loss: 2.2613 - val_acc: 0.1988\n",
      "Epoch 15/50\n",
      "30s - loss: 2.2585 - acc: 0.2018 - val_loss: 2.2588 - val_acc: 0.2009\n",
      "Epoch 16/50\n",
      "30s - loss: 2.2563 - acc: 0.2012 - val_loss: 2.2585 - val_acc: 0.1989\n",
      "Epoch 17/50\n",
      "30s - loss: 2.2532 - acc: 0.2040 - val_loss: 2.2560 - val_acc: 0.2028\n",
      "Epoch 18/50\n",
      "31s - loss: 2.2506 - acc: 0.2044 - val_loss: 2.2547 - val_acc: 0.1994\n",
      "Epoch 19/50\n",
      "31s - loss: 2.2485 - acc: 0.2061 - val_loss: 2.2546 - val_acc: 0.1986\n",
      "Epoch 20/50\n",
      "31s - loss: 2.2447 - acc: 0.2072 - val_loss: 2.2540 - val_acc: 0.1990\n",
      "Epoch 21/50\n",
      "31s - loss: 2.2416 - acc: 0.2085 - val_loss: 2.2519 - val_acc: 0.2019\n",
      "Epoch 22/50\n",
      "31s - loss: 2.2399 - acc: 0.2084 - val_loss: 2.2526 - val_acc: 0.2019\n",
      "Epoch 23/50\n",
      "31s - loss: 2.2380 - acc: 0.2097 - val_loss: 2.2527 - val_acc: 0.1975\n",
      "Epoch 24/50\n",
      "31s - loss: 2.2356 - acc: 0.2107 - val_loss: 2.2528 - val_acc: 0.2037\n",
      "Epoch 25/50\n",
      "31s - loss: 2.2323 - acc: 0.2125 - val_loss: 2.2507 - val_acc: 0.2046\n",
      "Epoch 26/50\n",
      "31s - loss: 2.2310 - acc: 0.2126 - val_loss: 2.2524 - val_acc: 0.2048\n",
      "Epoch 27/50\n",
      "30s - loss: 2.2296 - acc: 0.2156 - val_loss: 2.2516 - val_acc: 0.2031\n",
      "Epoch 28/50\n",
      "31s - loss: 2.2256 - acc: 0.2145 - val_loss: 2.2531 - val_acc: 0.1996\n",
      "Epoch 29/50\n",
      "Best number of rounds: 25 \n",
      " Val loss: 2.250689 \n",
      "\n",
      "31s - loss: 2.2247 - acc: 0.2141 - val_loss: 2.2513 - val_acc: 0.2039\n",
      "logloss val 2.2506893995\n",
      "('Validation score:', 2.2506893994964803)\n",
      "Model with following parameters: {'optimizer': 'adadelta', 'layer_1': {'units1': 400, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 130, 'dropout2': 0.35}, 'batch_size': 400, 'seed': 80359, 'n_epoch': 50, 'type': 'keras_nn'}\n",
      "Train on 67229 samples, validate on 7416 samples\n",
      "Epoch 1/50\n",
      "33s - loss: 2.4378 - acc: 0.1305 - val_loss: 2.3839 - val_acc: 0.1622\n",
      "Epoch 2/50\n",
      "30s - loss: 2.3751 - acc: 0.1589 - val_loss: 2.3377 - val_acc: 0.1735\n",
      "Epoch 3/50\n",
      "26s - loss: 2.3432 - acc: 0.1690 - val_loss: 2.3117 - val_acc: 0.1818\n",
      "Epoch 4/50\n",
      "28s - loss: 2.3231 - acc: 0.1728 - val_loss: 2.2982 - val_acc: 0.1882\n",
      "Epoch 5/50\n",
      "31s - loss: 2.3102 - acc: 0.1794 - val_loss: 2.2846 - val_acc: 0.1951\n",
      "Epoch 6/50\n",
      "39s - loss: 2.3008 - acc: 0.1834 - val_loss: 2.2803 - val_acc: 0.1928\n",
      "Epoch 7/50\n",
      "40s - loss: 2.2937 - acc: 0.1882 - val_loss: 2.2761 - val_acc: 0.1948\n",
      "Epoch 8/50\n",
      "40s - loss: 2.2879 - acc: 0.1894 - val_loss: 2.2718 - val_acc: 0.1961\n",
      "Epoch 9/50\n",
      "41s - loss: 2.2825 - acc: 0.1919 - val_loss: 2.2713 - val_acc: 0.1946\n",
      "Epoch 10/50\n",
      "41s - loss: 2.2775 - acc: 0.1929 - val_loss: 2.2693 - val_acc: 0.1994\n",
      "Epoch 11/50\n",
      "43s - loss: 2.2742 - acc: 0.1941 - val_loss: 2.2646 - val_acc: 0.1994\n",
      "Epoch 12/50\n",
      "42s - loss: 2.2678 - acc: 0.1949 - val_loss: 2.2654 - val_acc: 0.1996\n",
      "Epoch 13/50\n",
      "45s - loss: 2.2654 - acc: 0.1990 - val_loss: 2.2622 - val_acc: 0.1954\n",
      "Epoch 14/50\n",
      "46s - loss: 2.2630 - acc: 0.1992 - val_loss: 2.2631 - val_acc: 0.1984\n",
      "Epoch 15/50\n",
      "47s - loss: 2.2579 - acc: 0.2017 - val_loss: 2.2588 - val_acc: 0.2005\n",
      "Epoch 16/50\n",
      "48s - loss: 2.2560 - acc: 0.2005 - val_loss: 2.2607 - val_acc: 0.1986\n",
      "Epoch 17/50\n",
      "46s - loss: 2.2527 - acc: 0.2018 - val_loss: 2.2577 - val_acc: 0.2012\n",
      "Epoch 18/50\n",
      "45s - loss: 2.2489 - acc: 0.2060 - val_loss: 2.2546 - val_acc: 0.2011\n",
      "Epoch 19/50\n",
      "46s - loss: 2.2484 - acc: 0.2067 - val_loss: 2.2554 - val_acc: 0.2016\n",
      "Epoch 20/50\n",
      "46s - loss: 2.2448 - acc: 0.2059 - val_loss: 2.2571 - val_acc: 0.2004\n",
      "Epoch 21/50\n",
      "46s - loss: 2.2413 - acc: 0.2102 - val_loss: 2.2560 - val_acc: 0.2001\n",
      "Epoch 22/50\n",
      "Best number of rounds: 18 \n",
      " Val loss: 2.254637 \n",
      "\n",
      "45s - loss: 2.2415 - acc: 0.2073 - val_loss: 2.2558 - val_acc: 0.1973\n",
      "logloss val 2.25463669354\n",
      "('Validation score:', 2.2546366935393931)\n",
      "Model with following parameters: {'optimizer': 'adadelta', 'layer_1': {'units1': 400, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 130, 'dropout2': 0.35}, 'batch_size': 400, 'seed': 76149, 'n_epoch': 50, 'type': 'keras_nn'}\n",
      "Train on 67229 samples, validate on 7416 samples\n",
      "Epoch 1/50\n",
      "60s - loss: 2.4435 - acc: 0.1294 - val_loss: 2.3977 - val_acc: 0.1541\n",
      "Epoch 2/50\n",
      "56s - loss: 2.3863 - acc: 0.1512 - val_loss: 2.3477 - val_acc: 0.1750\n",
      "Epoch 3/50\n",
      "45s - loss: 2.3514 - acc: 0.1625 - val_loss: 2.3163 - val_acc: 0.1776\n",
      "Epoch 4/50\n",
      "45s - loss: 2.3292 - acc: 0.1741 - val_loss: 2.3003 - val_acc: 0.1854\n",
      "Epoch 5/50\n",
      "45s - loss: 2.3152 - acc: 0.1791 - val_loss: 2.2923 - val_acc: 0.1841\n",
      "Epoch 6/50\n",
      "43s - loss: 2.3068 - acc: 0.1801 - val_loss: 2.2845 - val_acc: 0.1926\n",
      "Epoch 7/50\n",
      "45s - loss: 2.2965 - acc: 0.1854 - val_loss: 2.2813 - val_acc: 0.1943\n",
      "Epoch 8/50\n",
      "45s - loss: 2.2896 - acc: 0.1878 - val_loss: 2.2771 - val_acc: 0.1912\n",
      "Epoch 9/50\n",
      "48s - loss: 2.2838 - acc: 0.1891 - val_loss: 2.2695 - val_acc: 0.1951\n",
      "Epoch 10/50\n",
      "51s - loss: 2.2803 - acc: 0.1921 - val_loss: 2.2678 - val_acc: 0.1958\n",
      "Epoch 11/50\n",
      "50s - loss: 2.2765 - acc: 0.1968 - val_loss: 2.2682 - val_acc: 0.1992\n",
      "Epoch 12/50\n",
      "53s - loss: 2.2714 - acc: 0.1964 - val_loss: 2.2618 - val_acc: 0.2004\n",
      "Epoch 13/50\n",
      "49s - loss: 2.2663 - acc: 0.1984 - val_loss: 2.2609 - val_acc: 0.1980\n",
      "Epoch 14/50\n",
      "49s - loss: 2.2637 - acc: 0.1987 - val_loss: 2.2615 - val_acc: 0.1997\n",
      "Epoch 15/50\n",
      "50s - loss: 2.2596 - acc: 0.2018 - val_loss: 2.2597 - val_acc: 0.1990\n",
      "Epoch 16/50\n",
      "54s - loss: 2.2582 - acc: 0.2013 - val_loss: 2.2584 - val_acc: 0.1971\n",
      "Epoch 17/50\n",
      "53s - loss: 2.2540 - acc: 0.2025 - val_loss: 2.2577 - val_acc: 0.2015\n",
      "Epoch 18/50\n",
      "54s - loss: 2.2508 - acc: 0.2045 - val_loss: 2.2618 - val_acc: 0.1974\n",
      "Epoch 19/50\n",
      "49s - loss: 2.2482 - acc: 0.2056 - val_loss: 2.2586 - val_acc: 0.2013\n",
      "Epoch 20/50\n",
      "49s - loss: 2.2438 - acc: 0.2074 - val_loss: 2.2543 - val_acc: 0.1996\n",
      "Epoch 21/50\n",
      "46s - loss: 2.2420 - acc: 0.2088 - val_loss: 2.2540 - val_acc: 0.2027\n",
      "Epoch 22/50\n",
      "48s - loss: 2.2406 - acc: 0.2086 - val_loss: 2.2536 - val_acc: 0.1986\n",
      "Epoch 23/50\n",
      "46s - loss: 2.2379 - acc: 0.2105 - val_loss: 2.2523 - val_acc: 0.1982\n",
      "Epoch 24/50\n",
      "46s - loss: 2.2373 - acc: 0.2098 - val_loss: 2.2560 - val_acc: 0.2017\n",
      "Epoch 25/50\n",
      "49s - loss: 2.2331 - acc: 0.2105 - val_loss: 2.2533 - val_acc: 0.2011\n",
      "Epoch 26/50\n",
      "49s - loss: 2.2321 - acc: 0.2132 - val_loss: 2.2539 - val_acc: 0.2024\n",
      "Epoch 27/50\n",
      "48s - loss: 2.2287 - acc: 0.2132 - val_loss: 2.2521 - val_acc: 0.2017\n",
      "Epoch 28/50\n",
      "52s - loss: 2.2264 - acc: 0.2145 - val_loss: 2.2534 - val_acc: 0.1989\n",
      "Epoch 29/50\n",
      "49s - loss: 2.2233 - acc: 0.2153 - val_loss: 2.2529 - val_acc: 0.2024\n",
      "Epoch 30/50\n",
      "46s - loss: 2.2239 - acc: 0.2157 - val_loss: 2.2536 - val_acc: 0.2008\n",
      "Epoch 31/50\n",
      "Best number of rounds: 27 \n",
      " Val loss: 2.252123 \n",
      "\n",
      "46s - loss: 2.2214 - acc: 0.2163 - val_loss: 2.2527 - val_acc: 0.2024\n",
      "logloss val 2.25212282191\n",
      "('Validation score:', 2.2521228219092118)\n"
     ]
    }
   ],
   "source": [
    "models_out = []\n",
    "scores = []\n",
    "\n",
    "for s in np.random.randint(99999,size=5):\n",
    "    params['seed'] = s\n",
    "    score, model, pred_val,modelfile = score_nn(X_train, X_val, y_train, y_val, params, description + '_' + str(s))\n",
    "    \n",
    "    print('Validation score:',score)\n",
    "\n",
    "    model_out = {'model': modelfile,\n",
    "                 'score': score,\n",
    "                 'params': params}\n",
    "    \n",
    "    models_out.append(model_out)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputfile = 'models_nn_1_V2_{}_{:.4f}_{:.4f}.pickle'.format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\"),\n",
    "                                                                cv_score,\n",
    "                                                                -1.)\n",
    "output = {'script': 'train_model1_nn',\n",
    "          'model_params': params,\n",
    "          'no_models': 1,\n",
    "          'cross_validation': {'type': 'gender_age_train_cv.csv'},\n",
    "          'models': models_out}\n",
    "\n",
    "\n",
    "with open(os.path.join(sub_dir, outputfile), 'wb') as f:\n",
    "    pickle.dump(output,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with following parameters: {'seed1': 239, 'seed2': 1234, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 179.31129328655433, 'dropout2': 0.27147227595028317}, 'optimizer': 'adamax', 'layer_1': {'units1': 141.28833823317106, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.6945806925899082}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'n_epoch': 9, 'batch_size': 400}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/9\n",
      "18s - loss: 2.3804 - acc: 0.1563 - val_loss: 2.3042 - val_acc: 0.1846\n",
      "Epoch 2/9\n",
      "15s - loss: 2.3067 - acc: 0.1805 - val_loss: 2.2756 - val_acc: 0.1956\n",
      "Epoch 3/9\n",
      "13s - loss: 2.2816 - acc: 0.1921 - val_loss: 2.2623 - val_acc: 0.2025\n",
      "Epoch 4/9\n",
      "13s - loss: 2.2646 - acc: 0.1978 - val_loss: 2.2538 - val_acc: 0.2045\n",
      "Epoch 5/9\n",
      "13s - loss: 2.2537 - acc: 0.2034 - val_loss: 2.2471 - val_acc: 0.2056\n",
      "Epoch 6/9\n",
      "14s - loss: 2.2425 - acc: 0.2078 - val_loss: 2.2448 - val_acc: 0.2061\n",
      "Epoch 7/9\n",
      "16s - loss: 2.2350 - acc: 0.2105 - val_loss: 2.2417 - val_acc: 0.2025\n",
      "Epoch 8/9\n",
      "16s - loss: 2.2254 - acc: 0.2160 - val_loss: 2.2405 - val_acc: 0.2060\n",
      "Epoch 9/9\n",
      "16s - loss: 2.2189 - acc: 0.2177 - val_loss: 2.2372 - val_acc: 0.2104\n"
     ]
    }
   ],
   "source": [
    "print ('Model with following parameters: %s' % (params))\n",
    "\n",
    "np.random.seed(params['seed1'])\n",
    "random.seed(params['seed1']) # not sure if needed or not\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(Xtrain, dummy_y, stratify=y,\n",
    "                                                  test_size=0.1, random_state=params['seed2'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(params['layer_1']['units1'], \n",
    "                input_dim=X_train.shape[1], \n",
    "                init='normal', \n",
    "                activation=params['layer_1']['activation1']))\n",
    "\n",
    "model.add(Dropout(params['layer_1']['dropout1']))\n",
    "\n",
    "if params['layer_2']['on2']:\n",
    "    model.add(Dense(params['layer_2']['units2'], \n",
    "                    init='normal', \n",
    "                    activation=params['layer_2']['activation2']))\n",
    "    #https://www.kaggle.com/poiss0nriot/talkingdata-mobile-user-demographics/bag-of-apps-keras-11-08-16-no-val/run/328610\n",
    "    #model.add(PReLU())\n",
    "    model.add(Dropout(params['layer_2']['dropout2']))\n",
    "\n",
    "if params['layer_3']['on3']:\n",
    "    model.add(Dense(params['layer_3']['units3'], \n",
    "                    init='normal', \n",
    "                    activation=params['layer_3']['activation3']))\n",
    "\n",
    "    model.add(Dropout(params['layer_3']['dropout3']))\n",
    "\n",
    "model.add(Dense(12, init='normal', activation='softmax'))   \n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "\n",
    "fit=model.fit(X_train.todense(), y_train, nb_epoch = params['n_epoch'] , batch_size=400,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss val 2.23722598618\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 32, False), val_samples=X_val.shape[0])\n",
    "\n",
    "cv_score = log_loss(y_val, scores_val)\n",
    "\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "pred_test = model.predict_generator(generator=batch_generatorp(Xtest, 32, False), val_samples=Xtest.shape[0])\n",
    "pred = pd.DataFrame(pred_test, index = gatest['device_id'], columns=targetencoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kag = KaggleResult(pred.values, pred.index.values, cv_score=cv_score, description='NN Model 1 - with seeds', subdir=sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2428\n"
     ]
    }
   ],
   "source": [
    "if kag.validate():\n",
    "    kag.upload()\n",
    "print kag.lb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a good cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(dir_in,'gender_age_train.csv'), dtype={'device_id': np.str})\n",
    "train['sample_nr'] = train.index\n",
    "test = pd.read_csv(os.path.join(dir_in,'gender_age_test.csv'), dtype={'device_id': np.str})\n",
    "events = pd.read_csv(os.path.join(dir_in,'events.csv'), dtype={'device_id': np.str})\n",
    "unique_device_id = events.device_id.unique()\n",
    "\n",
    "index = train.device_id.isin(unique_device_id)\n",
    "index2 = [not i for i in index]\n",
    "train1 = train[index].sort_values(by='device_id').reset_index(drop=True)\n",
    "train2 = train[index2].sort_values(by='device_id').reset_index(drop=True)\n",
    "\n",
    "train1, test1 = train_test_split(train1, test_size=0.100, random_state=1974,     stratify=train1['group'].values)\n",
    "train2, test2 = train_test_split(train2, test_size=0.099, random_state=1974, stratify=train2['group'].values)\n",
    "train = pd.concat([train1, train2], ignore_index=True).sort_values(by='device_id').reset_index(drop=True)\n",
    "test = pd.concat([test1, test2], ignore_index=True).sort_values(by='device_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>sample_nr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1001337759327042486</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>M29-31</td>\n",
       "      <td>39902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1007249684892521754</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>M39+</td>\n",
       "      <td>7310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1008484851265061759</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>M29-31</td>\n",
       "      <td>65975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1014599235541061307</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>M29-31</td>\n",
       "      <td>27471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1021073753429555564</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>M32-38</td>\n",
       "      <td>67125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              device_id gender  age   group  sample_nr\n",
       "0  -1001337759327042486      M   30  M29-31      39902\n",
       "1  -1007249684892521754      M   40    M39+       7310\n",
       "2  -1008484851265061759      M   29  M29-31      65975\n",
       "3  -1014599235541061307      M   30  M29-31      27471\n",
       "4  -1021073753429555564      M   36  M32-38      67125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val = Xtrain.tocsc()[train.sample_nr, :], Xtrain.tocsc()[test.sample_nr, :]\n",
    "y_train, y_val = dummy_y[train.sample_nr], dummy_y[test.sample_nr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42782, 44836,  3898, 38776, 92747])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(99999, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class prediction_check(Callback):\n",
    "    def __init__(self, tag='', filepath='./'):\n",
    "        super(Callback, self).__init__()\n",
    "        \n",
    "        self.predhist = pd.DataFrame(columns=['acc','logloss','val_acc','val_logloss'])\n",
    "        self.tag = str(tag)\n",
    "        self.counter = 0\n",
    "        self.best = 10.0\n",
    "        self.best_rounds = 1\n",
    "        self.wait = 0\n",
    "        self.patience = 3\n",
    "        self.filepath = filepath\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #print logs.values()\n",
    "        self.counter += 1\n",
    "        self.predhist.loc[epoch] = logs.values()\n",
    "        \n",
    "        current_score = logs['val_loss']\n",
    "        \n",
    "        if current_score < self.best:\n",
    "            self.best = current_score\n",
    "            self.best_train = logs['loss']\n",
    "            self.best_rounds = self.counter\n",
    "            self.wait = 0\n",
    "            self.model.save_weights(self.filepath, overwrite=True)\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                self.model.load_weights(self.filepath)\n",
    "                print('Best number of rounds: %d \\n Val loss: %f \\n' % (self.best_rounds, self.best))\n",
    "            self.wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with following parameters: {'optimizer': 'adamax', 'layer_1': {'units1': 141.28833823317106, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.6945806925899082}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 179.31129328655433, 'dropout2': 0.27147227595028317}, 'batch_size': 400, 'seed1': 239, 'n_epoch': 9} and seed 89171\n",
      "Train on 67229 samples, validate on 7416 samples\n",
      "Epoch 1/20\n",
      "17s - loss: 2.3863 - acc: 0.1531 - val_loss: 2.3097 - val_acc: 0.1806\n",
      "Epoch 2/20\n",
      "13s - loss: 2.3065 - acc: 0.1826 - val_loss: 2.2770 - val_acc: 0.1923\n",
      "Epoch 3/20\n",
      "13s - loss: 2.2812 - acc: 0.1926 - val_loss: 2.2655 - val_acc: 0.1986\n",
      "Epoch 4/20\n",
      "13s - loss: 2.2656 - acc: 0.1994 - val_loss: 2.2596 - val_acc: 0.1967\n",
      "Epoch 5/20\n",
      "14s - loss: 2.2532 - acc: 0.2032 - val_loss: 2.2548 - val_acc: 0.2027\n",
      "Epoch 6/20\n",
      "14s - loss: 2.2432 - acc: 0.2087 - val_loss: 2.2558 - val_acc: 0.1988\n",
      "Epoch 7/20\n",
      "15s - loss: 2.2330 - acc: 0.2103 - val_loss: 2.2510 - val_acc: 0.2020\n",
      "Epoch 8/20\n",
      "15s - loss: 2.2260 - acc: 0.2164 - val_loss: 2.2501 - val_acc: 0.2046\n",
      "Epoch 9/20\n",
      "15s - loss: 2.2162 - acc: 0.2177 - val_loss: 2.2502 - val_acc: 0.2043\n",
      "Epoch 10/20\n",
      "16s - loss: 2.2101 - acc: 0.2240 - val_loss: 2.2503 - val_acc: 0.2070\n",
      "Epoch 11/20\n",
      "16s - loss: 2.2027 - acc: 0.2218 - val_loss: 2.2494 - val_acc: 0.2052\n",
      "Epoch 12/20\n",
      "16s - loss: 2.1942 - acc: 0.2283 - val_loss: 2.2509 - val_acc: 0.2004\n",
      "Epoch 13/20\n",
      "16s - loss: 2.1874 - acc: 0.2299 - val_loss: 2.2545 - val_acc: 0.2015\n",
      "Epoch 14/20\n",
      "16s - loss: 2.1796 - acc: 0.2339 - val_loss: 2.2556 - val_acc: 0.2067\n",
      "Epoch 15/20\n",
      "Best number of rounds: 11 \n",
      " Val loss: 2.249418 \n",
      "\n",
      "16s - loss: 2.1750 - acc: 0.2379 - val_loss: 2.2592 - val_acc: 0.2048\n",
      "logloss val 2.24941839454\n"
     ]
    }
   ],
   "source": [
    "for s in [89171]: #np.random.randint(99999, size=5):\n",
    "    np.random.seed(s)\n",
    "    random.seed(s) # not\n",
    "    \n",
    "    \n",
    "\n",
    "    # create model\n",
    "    print ('Model with following parameters: %s and seed %d' % (params, s))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['layer_1']['units1'], \n",
    "                    input_dim=X_train.shape[1], \n",
    "                    init='normal', \n",
    "                    activation=params['layer_1']['activation1']))\n",
    "\n",
    "    model.add(Dropout(params['layer_1']['dropout1']))\n",
    "\n",
    "    if params['layer_2']['on2']:\n",
    "        model.add(Dense(params['layer_2']['units2'], \n",
    "                        init='normal', \n",
    "                        activation=params['layer_2']['activation2']))\n",
    "        #https://www.kaggle.com/poiss0nriot/talkingdata-mobile-user-demographics/bag-of-apps-keras-11-08-16-no-val/run/328610\n",
    "        #model.add(PReLU())\n",
    "        model.add(Dropout(params['layer_2']['dropout2']))\n",
    "\n",
    "    if params['layer_3']['on3']:\n",
    "        model.add(Dense(params['layer_3']['units3'], \n",
    "                        init='normal', \n",
    "                        activation=params['layer_3']['activation3']))\n",
    "\n",
    "        model.add(Dropout(params['layer_3']['dropout3']))\n",
    "\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))    \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])  #logloss\n",
    "\n",
    "    hist_all = prediction_check('test', filepath=sub_dir + '/test.h5' )\n",
    "\n",
    "    fit=model.fit(X_train.todense(), y_train, nb_epoch = 20, batch_size=int(params['batch_size']),\n",
    "                             validation_data=(X_val.todense(), y_val), verbose=2,\n",
    "                 callbacks=[hist_all]) \n",
    "\n",
    "    # evaluate the model\n",
    "    scores_val = model.predict_generator(generator=batch_generatorp(X_val, 32, False), val_samples=X_val.shape[0])\n",
    "\n",
    "    cv_score = log_loss(y_val, scores_val)\n",
    "\n",
    "    print('logloss val {}'.format(log_loss(y_val, scores_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24941839454\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 32, False), val_samples=X_val.shape[0])\n",
    "\n",
    "cv_score = log_loss(y_val, scores_val)\n",
    "print cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "pred_test = model.predict_generator(generator=batch_generatorp(Xtest, 32, False), val_samples=Xtest.shape[0])\n",
    "pred = pd.DataFrame(pred_test, index = gatest['device_id'], columns=targetencoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kag = KaggleResult(pred.values, pred.index.values, cv_score=cv_score, description='NN Model 1 - random test - new CV set', subdir=sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if kag.validate():\n",
    "    kag.upload()\n",
    "print kag.lb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build convo network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten, Reshape, Embedding\n",
    "# set parameters:\n",
    "\n",
    "\n",
    "max_features = 2000\n",
    "maxlen = X_train.shape[1]\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen,\n",
    "                    dropout=0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(pool_length=model.output_shape[1]))\n",
    "\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(12, init='normal', activation='softmax'))    \n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])  #logloss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
