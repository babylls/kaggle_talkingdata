{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model 0\n",
    "\n",
    "- V0 Based on HyperOpt optimizated model\n",
    "    - including extra features increased CV score, but lowered LB score\n",
    "    - including isChinese features increase LB score slightly\n",
    "\n",
    "Validation scores:\n",
    "- V0: CV: 2.3919 / LB: 2.38810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#import sys\n",
    "#import pymongo\n",
    "#import bson.objectid\n",
    "#pymongo.objectid = bson.objectid\n",
    "#sys.modules[\"pymongo.objectid\"] = bson.objectid\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='tensorflow'\n",
    "#os.environ['THEANO_FLAGS'] = 'device=cpu'\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback\n",
    "import keras\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import log_loss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_in = 'data_ori'\n",
    "dir_feat = 'data'\n",
    "rs = 123\n",
    "fixed_seed_num = 1234\n",
    "np.random.seed(fixed_seed_num)\n",
    "random.seed(fixed_seed_num) # not sure if needed or not\n",
    "\n",
    "feature_files_old = ['features_bag_brand',\n",
    "                 'features_bag_model',\n",
    "                'features_bag_label_installed',\n",
    "                'features_bag_app_installed']\n",
    "feature_files = ['features_brand_bag',\n",
    "                 'features_brand_model_bag',\n",
    "                 'features_brand_model.csv']\n",
    "feature_files=feature_files[0:3]\n",
    "\n",
    "# Function to read feature file\n",
    "def open_feature_file(fname, samples='train'):\n",
    "    if fname[-3:] == 'csv':\n",
    "        if samples=='train':\n",
    "            X = gatrain[['device_id']].merge( pd.read_csv(os.path.join(dir_feat, fname)), on='device_id', how='left')\n",
    "        else:\n",
    "            X = gatest[['device_id']].merge( pd.read_csv(os.path.join(dir_feat, fname)), on='device_id', how='left')\n",
    "            \n",
    "        X.drop('device_id', axis=1, inplace=True)\n",
    "        X.fillna(0, inplace=True)\n",
    "        \n",
    "        for c in X.columns:\n",
    "            if X[c].max()>1:\n",
    "                X[c] = StandardScaler().fit_transform(X)\n",
    "                \n",
    "        return csr_matrix(X.values)\n",
    "    else:\n",
    "        # Assume it is a pickle file\n",
    "        with open(os.path.join(dir_feat, '{}_{}.pickle'.format(fname,samples)), 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate CV score on Xtrain, y\n",
    "def score(X, clf = None, random_state = 123, w = None):\n",
    "    if not clf: clf = LogisticRegression(C=0.13, multi_class='multinomial',solver='lbfgs', n_jobs=2)\n",
    "        \n",
    "    kf = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=random_state)\n",
    "    pred = np.zeros((y.shape[0],nclasses))\n",
    "    for itrain, itest in kf:\n",
    "        Xtr, Xte = X[itrain, :], X[itest, :]\n",
    "        ytr, yte = y[itrain], y[itest]\n",
    "        if np.any(w):\n",
    "            ws = w[itrain]\n",
    "        else:\n",
    "            ws = None\n",
    "            \n",
    "        clf.fit(Xtr, ytr, sample_weight = ws)\n",
    "        pred[itest,:] = clf.predict_proba(Xte)\n",
    "        # Downsize to one fold only for kernels\n",
    "        #print(\"{:.5f}\".format(log_loss(yte, pred[itest,:])))\n",
    "    #print('')\n",
    "    return log_loss(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get basescore on installed bag of apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv(os.path.join(dir_in,'gender_age_train.csv'))\n",
    "gatest = pd.read_csv(os.path.join(dir_in,'gender_age_test.csv'))\n",
    "targetencoder = LabelEncoder().fit(gatrain.group)\n",
    "y = targetencoder.transform(gatrain.group)\n",
    "nclasses = len(targetencoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw = [1, 1, 1, 1]\n",
    "Xtrain = hstack([open_feature_file(f) for f in feature_files],format='csr')\n",
    "Xtest = hstack([open_feature_file(f,'test') for f in feature_files],format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = Xtrain[:, 0:1800]\n",
    "Xtest = Xtest[:, 0:1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1800 features\n",
      "Basescore: 2.39188\n"
     ]
    }
   ],
   "source": [
    "print('Training on {} features'.format(Xtrain.shape[1]))\n",
    "basescore = score(Xtrain, random_state=rs)\n",
    "print('Basescore: {:.5f}'.format(basescore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get score and train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model1(X, params):\n",
    "    \n",
    "    # create model\n",
    "    print ('NN model with following params: %s' % (params))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['layer_1']['units1'], \n",
    "                    input_dim=X.shape[1], \n",
    "                    init='normal', \n",
    "                    activation=params['layer_1']['activation1']))\n",
    "    \n",
    "    model.add(Dropout(params['layer_1']['dropout1']))\n",
    "    \n",
    "    if params['layer_2']['on2']:\n",
    "        model.add(Dense(params['layer_2']['units2'], \n",
    "                        input_dim=X.shape[1], \n",
    "                        init='normal', \n",
    "                        activation=params['layer_2']['activation2']))\n",
    "        #https://www.kaggle.com/poiss0nriot/talkingdata-mobile-user-demographics/bag-of-apps-keras-11-08-16-no-val/run/328610\n",
    "        #model.add(PReLU())\n",
    "        model.add(Dropout(params['layer_2']['dropout2']))\n",
    "    \n",
    "    if params['layer_3']['on3']:\n",
    "        model.add(Dense(params['layer_3']['units3'], \n",
    "                        init='normal', \n",
    "                        activation=params['layer_3']['activation3']))\n",
    "        \n",
    "        model.add(Dropout(params['layer_3']['dropout3']))\n",
    "    \n",
    "    model.add(Dense(12, init='normal', activation='softmax'))    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])  #logloss\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'optimizer': 'adam',\n",
    "          'batch_size': 32,\n",
    "          'n_epoch': 4,\n",
    "         'layer_1': {'on1': True,\n",
    "                     'units1': 150,\n",
    "                     'activation1': 'tanh',\n",
    "                     'dropout1': 0.8},\n",
    "         'layer_2': {'on2': False,\n",
    "                    'units2': 150,\n",
    "                    'activation2': 'tanh',\n",
    "                    'dropout2': 0.2},\n",
    "          'layer_3': {'on3': False,\n",
    "                     'units3': 12,\n",
    "                     'activation3': 'sigmoid',\n",
    "                     'dropout3': 0.2}\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (67176, 1800)\n",
      "Validation set: (7469, 1800)\n",
      "NN model with following params: {'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'sigmoid', 'on3': False, 'units3': 12, 'dropout3': 0.2}, 'layer_2': {'on2': False, 'activation2': 'tanh', 'units2': 150, 'dropout2': 0.2}, 'batch_size': 32, 'n_epoch': 4}\n",
      "Train on 67176 samples, validate on 7469 samples\n",
      "Epoch 1/4\n",
      "11s - loss: 2.4172 - acc: 0.1392 - val_loss: 2.3953 - val_acc: 0.1521\n",
      "Epoch 2/4\n",
      "21s - loss: 2.3972 - acc: 0.1515 - val_loss: 2.3922 - val_acc: 0.1509\n",
      "Epoch 3/4\n",
      "24s - loss: 2.3898 - acc: 0.1561 - val_loss: 2.3906 - val_acc: 0.1540\n",
      "Epoch 4/4\n",
      "26s - loss: 2.3839 - acc: 0.1587 - val_loss: 2.3920 - val_acc: 0.1553\n",
      "('Validation score:', 2.3920245206185271)\n"
     ]
    }
   ],
   "source": [
    "kf = list(StratifiedKFold(y, n_folds=10, shuffle=True, random_state=4242))[0]\n",
    "\n",
    "Xtr, Xte = Xtrain[kf[0], :], Xtrain[kf[1], :]\n",
    "ytr, yte = y[kf[0]], y[kf[1]]\n",
    "\n",
    "print('Training set: ' + str(Xtr.shape))\n",
    "print('Validation set: ' + str(Xte.shape))\n",
    "\n",
    "model=nn_model1(Xtr, params)\n",
    "\n",
    "fit=model.fit(Xtr.todense(), \n",
    "              np_utils.to_categorical(ytr), \n",
    "              nb_epoch = params['n_epoch'], \n",
    "              batch_size=params['batch_size'],\n",
    "              validation_data=(Xte.todense(), np_utils.to_categorical(yte)), \n",
    "              verbose=2)\n",
    "\n",
    "scores_val = model.predict_proba(Xte.todense(), batch_size = 128, verbose = 0)\n",
    "print('Validation score:',log_loss(yte, scores_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111872/112071 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "pred = model.predict_proba(Xtest.todense())\n",
    "pred = pd.DataFrame(pred, index = gatest.device_id, columns=targetencoder.classes_)\n",
    "pred.head()\n",
    "pred.to_csv('model_0_nn.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(Xtrain, dummy_y, stratify=y,\n",
    "                                                  test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch, np.ones(X_batch.shape[0])\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch#, np.ones(X_batch.shape[0])\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0\n",
    "\n",
    "# Best model:\n",
    "# With layer1: 2.38222170278\n",
    "# With layer2: 2.3828 ('activation2': 1, 'units2': 141.67406062812924, 'n_epoch': 6.1304111628793283, 'dropout2': 0.54756289671556369)\n",
    "params = {'optimizer': 'adam',\n",
    "          'batch_size': 32,\n",
    "          'n_epoch': 5,\n",
    "         'layer_1': {'on1': True,\n",
    "                     'units1': 150,\n",
    "                     'activation1': 'tanh',\n",
    "                     'dropout1': 0.8},\n",
    "         'layer_2': {'on2': False,\n",
    "                    'units2': 150,\n",
    "                    'activation2': 'tanh',\n",
    "                    'dropout2': 0.2},\n",
    "          'layer_3': {'on3': False,\n",
    "                     'units3': 12,\n",
    "                     'activation3': 'sigmoid',\n",
    "                     'dropout3': 0.2}\n",
    "          }\n",
    "\n",
    "# define baseline model\n",
    "params = {'optimizer': 'adam',\n",
    "          'batch_size': 32, #hp.uniform('batch_size', 16, 512),\n",
    "          'n_epoch': hp.uniform('n_epoch', 3, 7),\n",
    "         'layer_1': {'on1': True,\n",
    "                     'units1': 150,\n",
    "                     'activation1': 'tanh',\n",
    "                     'dropout1': 0.8},\n",
    "         'layer_2': {'on2': True,\n",
    "                    'units2': hp.uniform('units2', 16, 256),\n",
    "                    'activation2': hp.choice('activation2',['relu','tanh']),\n",
    "                    'dropout2': hp.uniform('dropout2', .2, .9)},\n",
    "          'layer_3': {'on3': False,\n",
    "                     'units3': 16,\n",
    "                     'activation3': 'relu',\n",
    "                     'dropout3': .25}\n",
    "          }\n",
    "\n",
    "\n",
    "def f_nn(params):\n",
    "    global counter\n",
    "    global times\n",
    "    \n",
    "    a = time.time()\n",
    "    \n",
    "    # create model\n",
    "    print ('Params testing %d: %s' % (counter, params))\n",
    "    counter += 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['layer_1']['units1'], \n",
    "                    input_dim=Xtrain.shape[1], \n",
    "                    init='normal', \n",
    "                    activation=params['layer_1']['activation1']))\n",
    "    \n",
    "    model.add(Dropout(params['layer_1']['dropout1']))\n",
    "    \n",
    "    if params['layer_2']['on2']:\n",
    "        model.add(Dense(params['layer_2']['units2'], \n",
    "                        input_dim=Xtrain.shape[1], \n",
    "                        init='normal', \n",
    "                        activation=params['layer_2']['activation2']))\n",
    "        #https://www.kaggle.com/poiss0nriot/talkingdata-mobile-user-demographics/bag-of-apps-keras-11-08-16-no-val/run/328610\n",
    "        #model.add(PReLU())\n",
    "        model.add(Dropout(params['layer_2']['dropout2']))\n",
    "    \n",
    "    if params['layer_3']['on3']:\n",
    "        model.add(Dense(params['layer_3']['units3'], \n",
    "                        init='normal', \n",
    "                        activation=params['layer_3']['activation3']))\n",
    "        \n",
    "        model.add(Dropout(params['layer_3']['dropout3']))\n",
    "    \n",
    "    model.add(Dense(12, init='normal', activation='softmax'))    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])  #logloss\n",
    "    \n",
    "    hist_all = prediction_hist('test')\n",
    "    \n",
    "    fit=model.fit(X_train.todense(), y_train, nb_epoch = int(params['n_epoch']), batch_size=int(params['batch_size']),\n",
    "                             validation_data=(X_val.todense(), y_val), verbose=2,\n",
    "                 callbacks=[hist_all]) \n",
    "    \n",
    "    scores_val = model.predict_proba(X_val.todense(), batch_size = 128, verbose = 0)\n",
    "\n",
    "    logloss = log_loss(y_val, scores_val)\n",
    "    times.append(a-time.time())\n",
    "                        \n",
    "    print('Log_loss:', logloss, 'It took', a-time.time())\n",
    "    print('Average time per eval:', sum(times)/counter)\n",
    "    sys.stdout.flush() \n",
    "    return {'loss': logloss, 'status': STATUS_OK}\n",
    "\n",
    "def plot_loss_progress(pred_hist_obj, ax1=None, ax2=None, c='g'):\n",
    "    \n",
    "    df = pred_hist_obj.predhist\n",
    "    \n",
    "    if not ax1:\n",
    "        fig, (ax1, ax2) = plt.subplot(1, 2, figsize=(16,4))\n",
    "        \n",
    "    df.plot(y='acc', ax=ax1, label=pred_hist_obj.tag, color=c)\n",
    "    df.plot(y='val_acc', ax=ax1, label=pred_hist_obj.tag, linestyle='--', color=c)\n",
    "    ax1.set_title('Accuracy score')\n",
    "    ax1.set_xlabel('Epoch #')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    \n",
    "    df.plot(y='logloss', ax=ax2, label=pred_hist_obj.tag, color=c)\n",
    "    df.plot(y='val_logloss', ax=ax2, label=pred_hist_obj.tag, linestyle='--', color=c)\n",
    "    ax2.set_title('Logloss score')\n",
    "    ax2.set_xlabel('Epoch #')\n",
    "    ax2.set_ylabel('Logloss')\n",
    "\n",
    "class prediction_hist(Callback):\n",
    "    def __init__(self, tag=''):\n",
    "        self.predhist = pd.DataFrame(columns=['acc','logloss','val_acc','val_logloss'])\n",
    "        self.tag = str(tag)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #print logs.values()\n",
    "        self.predhist.loc[epoch] = logs.values()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing 0: {'n_epoch': 4.144557339801517, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 183.1526045434868, 'dropout2': 0.35879601749494217}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 2.4164 - acc: 0.1408 - val_loss: 2.3972 - val_acc: 0.1540\n",
      "Epoch 2/4\n",
      "26s - loss: 2.4046 - acc: 0.1477 - val_loss: 2.3987 - val_acc: 0.1585\n",
      "Epoch 3/4\n",
      "29s - loss: 2.3992 - acc: 0.1499 - val_loss: 2.3933 - val_acc: 0.1543\n",
      "Epoch 4/4\n",
      "32s - loss: 2.3952 - acc: 0.1541 - val_loss: 2.3908 - val_acc: 0.1538\n",
      "('Log_loss:', 2.3908073852472898, 'It took', -114.80944800376892)\n",
      "('Average time per eval:', -114.80944609642029)\n",
      "Params testing 1: {'n_epoch': 5.981885939542452, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 41.45557742913586, 'dropout2': 0.6006194756420492}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "14s - loss: 2.4222 - acc: 0.1376 - val_loss: 2.3988 - val_acc: 0.1417\n",
      "Epoch 2/5\n",
      "25s - loss: 2.4090 - acc: 0.1429 - val_loss: 2.3944 - val_acc: 0.1485\n",
      "Epoch 3/5\n",
      "29s - loss: 2.4047 - acc: 0.1475 - val_loss: 2.3927 - val_acc: 0.1477\n",
      "Epoch 4/5\n",
      "31s - loss: 2.4008 - acc: 0.1491 - val_loss: 2.3934 - val_acc: 0.1548\n",
      "Epoch 5/5\n",
      "35s - loss: 2.3991 - acc: 0.1492 - val_loss: 2.3918 - val_acc: 0.1532\n",
      "('Log_loss:', 2.3917959847879446, 'It took', -142.9733271598816)\n",
      "('Average time per eval:', -128.8913791179657)\n",
      "Params testing 2: {'n_epoch': 3.230122780672359, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 137.6142427598968, 'dropout2': 0.6393059900537531}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "16s - loss: 2.4220 - acc: 0.1346 - val_loss: 2.4058 - val_acc: 0.1530\n",
      "Epoch 2/3\n",
      "25s - loss: 2.4096 - acc: 0.1463 - val_loss: 2.3971 - val_acc: 0.1530\n",
      "Epoch 3/3\n",
      "29s - loss: 2.4047 - acc: 0.1479 - val_loss: 2.3985 - val_acc: 0.1523\n",
      "('Log_loss:', 2.3984505086777967, 'It took', -80.15933609008789)\n",
      "('Average time per eval:', -112.64736374219258)\n",
      "Params testing 3: {'n_epoch': 3.5125762881360014, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 41.24131053224205, 'dropout2': 0.26118441592166486}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "16s - loss: 2.4166 - acc: 0.1391 - val_loss: 2.3930 - val_acc: 0.1528\n",
      "Epoch 2/3\n",
      "27s - loss: 2.4013 - acc: 0.1475 - val_loss: 2.3908 - val_acc: 0.1542\n",
      "Epoch 3/3\n",
      "30s - loss: 2.3961 - acc: 0.1557 - val_loss: 2.3887 - val_acc: 0.1551\n",
      "('Log_loss:', 2.3887329895439526, 'It took', -81.28497910499573)\n",
      "('Average time per eval:', -104.80676734447479)\n",
      "Params testing 4: {'n_epoch': 3.1598518965998967, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 141.7199967891222, 'dropout2': 0.33017571854030536}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "16s - loss: 2.4165 - acc: 0.1425 - val_loss: 2.3948 - val_acc: 0.1496\n",
      "Epoch 2/3\n",
      "26s - loss: 2.4032 - acc: 0.1509 - val_loss: 2.3888 - val_acc: 0.1531\n",
      "Epoch 3/3\n",
      "29s - loss: 2.3971 - acc: 0.1509 - val_loss: 2.3887 - val_acc: 0.1535\n",
      "('Log_loss:', 2.3886857634889771, 'It took', -78.54030418395996)\n",
      "('Average time per eval:', -99.55347428321838)\n",
      "Params testing 5: {'n_epoch': 4.052579398389655, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 223.9928902840955, 'dropout2': 0.2919859347214409}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 2.4150 - acc: 0.1404 - val_loss: 2.4017 - val_acc: 0.1558\n",
      "Epoch 2/4\n",
      "26s - loss: 2.4047 - acc: 0.1486 - val_loss: 2.3983 - val_acc: 0.1546\n",
      "Epoch 3/4\n",
      "30s - loss: 2.3989 - acc: 0.1517 - val_loss: 2.3946 - val_acc: 0.1564\n",
      "Epoch 4/4\n",
      "33s - loss: 2.3945 - acc: 0.1546 - val_loss: 2.3915 - val_acc: 0.1555\n",
      "('Log_loss:', 2.391471742929034, 'It took', -114.01472115516663)\n",
      "('Average time per eval:', -101.96368141969045)\n",
      "Params testing 6: {'n_epoch': 4.073039103255604, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 152.92759003489763, 'dropout2': 0.6237066663378692}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "15s - loss: 2.4212 - acc: 0.1363 - val_loss: 2.4015 - val_acc: 0.1480\n",
      "Epoch 2/4\n",
      "27s - loss: 2.4089 - acc: 0.1441 - val_loss: 2.4034 - val_acc: 0.1484\n",
      "Epoch 3/4\n",
      "30s - loss: 2.4050 - acc: 0.1456 - val_loss: 2.3961 - val_acc: 0.1518\n",
      "Epoch 4/4\n",
      "36s - loss: 2.4036 - acc: 0.1480 - val_loss: 2.3971 - val_acc: 0.1495\n",
      "('Log_loss:', 2.3970723568738515, 'It took', -117.3544909954071)\n",
      "('Average time per eval:', -104.1623683656965)\n",
      "Params testing 7: {'n_epoch': 4.24617744451036, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 49.99126457841777, 'dropout2': 0.588354604095034}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 2.4199 - acc: 0.1392 - val_loss: 2.3922 - val_acc: 0.1522\n",
      "Epoch 2/4\n",
      "28s - loss: 2.4042 - acc: 0.1475 - val_loss: 2.3899 - val_acc: 0.1575\n",
      "Epoch 3/4\n",
      "31s - loss: 2.4001 - acc: 0.1502 - val_loss: 2.3870 - val_acc: 0.1532\n",
      "Epoch 4/4\n",
      "34s - loss: 2.3960 - acc: 0.1532 - val_loss: 2.3848 - val_acc: 0.1574\n",
      "('Log_loss:', 2.3848471174347412, 'It took', -116.98510885238647)\n",
      "('Average time per eval:', -105.76521030068398)\n",
      "Params testing 8: {'n_epoch': 6.792436663361079, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 172.03686531321978, 'dropout2': 0.4716202207488044}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "16s - loss: 2.4179 - acc: 0.1410 - val_loss: 2.4020 - val_acc: 0.1477\n",
      "Epoch 2/6\n",
      "27s - loss: 2.4060 - acc: 0.1491 - val_loss: 2.3964 - val_acc: 0.1535\n",
      "Epoch 3/6\n",
      "31s - loss: 2.4007 - acc: 0.1503 - val_loss: 2.3966 - val_acc: 0.1546\n",
      "Epoch 4/6\n",
      "37s - loss: 2.3988 - acc: 0.1535 - val_loss: 2.3977 - val_acc: 0.1572\n",
      "Epoch 5/6\n",
      "37s - loss: 2.3955 - acc: 0.1533 - val_loss: 2.3953 - val_acc: 0.1532\n",
      "Epoch 6/6\n",
      "37s - loss: 2.3936 - acc: 0.1536 - val_loss: 2.3950 - val_acc: 0.1563\n",
      "('Log_loss:', 2.3949833365490059, 'It took', -196.97156310081482)\n",
      "('Average time per eval:', -115.89924772580464)\n",
      "Params testing 9: {'n_epoch': 4.524578167277366, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 203.41693571019186, 'dropout2': 0.780046349195127}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "17s - loss: 2.4285 - acc: 0.1348 - val_loss: 2.3947 - val_acc: 0.1495\n",
      "Epoch 2/4\n",
      "30s - loss: 2.4149 - acc: 0.1417 - val_loss: 2.3916 - val_acc: 0.1550\n",
      "Epoch 3/4\n",
      "36s - loss: 2.4119 - acc: 0.1450 - val_loss: 2.3896 - val_acc: 0.1528\n",
      "Epoch 4/4\n",
      "35s - loss: 2.4091 - acc: 0.1487 - val_loss: 2.3900 - val_acc: 0.1504\n",
      "('Log_loss:', 2.389981860390241, 'It took', -126.21449613571167)\n",
      "('Average time per eval:', -116.93077235221863)\n",
      "Params testing 10: {'n_epoch': 3.518643437139083, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 116.68206792368935, 'dropout2': 0.2766892861797254}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "16s - loss: 2.4155 - acc: 0.1394 - val_loss: 2.3953 - val_acc: 0.1536\n",
      "Epoch 2/3\n",
      "28s - loss: 2.4014 - acc: 0.1485 - val_loss: 2.3926 - val_acc: 0.1582\n",
      "Epoch 3/3\n",
      "29s - loss: 2.3974 - acc: 0.1516 - val_loss: 2.3917 - val_acc: 0.1567\n",
      "('Log_loss:', 2.3917172891889922, 'It took', -82.29808020591736)\n",
      "('Average time per eval:', -113.78234568509188)\n",
      "Params testing 11: {'n_epoch': 4.79105697938858, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 210.73687031406035, 'dropout2': 0.6394219592423397}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "16s - loss: 2.4216 - acc: 0.1371 - val_loss: 2.4074 - val_acc: 0.1531\n",
      "Epoch 2/4\n",
      "26s - loss: 2.4105 - acc: 0.1451 - val_loss: 2.3981 - val_acc: 0.1546\n",
      "Epoch 3/4\n",
      "31s - loss: 2.4056 - acc: 0.1472 - val_loss: 2.3996 - val_acc: 0.1558\n",
      "Epoch 4/4\n",
      "26s - loss: 2.4031 - acc: 0.1497 - val_loss: 2.4006 - val_acc: 0.1511\n",
      "('Log_loss:', 2.4005944098635585, 'It took', -108.47222208976746)\n",
      "('Average time per eval:', -113.33983530600865)\n",
      "Params testing 12: {'n_epoch': 4.31077880120262, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 174.71894618799152, 'dropout2': 0.33677872292880595}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "11s - loss: 2.4155 - acc: 0.1414 - val_loss: 2.3995 - val_acc: 0.1527\n",
      "Epoch 2/4\n",
      "20s - loss: 2.4032 - acc: 0.1469 - val_loss: 2.3964 - val_acc: 0.1488\n",
      "Epoch 3/4\n",
      "24s - loss: 2.3992 - acc: 0.1515 - val_loss: 2.3953 - val_acc: 0.1540\n",
      "Epoch 4/4\n",
      "25s - loss: 2.3957 - acc: 0.1544 - val_loss: 2.3945 - val_acc: 0.1539\n",
      "('Log_loss:', 2.3944878521184005, 'It took', -86.71661686897278)\n",
      "('Average time per eval:', -111.2918953528771)\n",
      "Params testing 13: {'n_epoch': 3.5926713138566457, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 52.77182485595119, 'dropout2': 0.3494233189607483}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "10s - loss: 2.4155 - acc: 0.1423 - val_loss: 2.3927 - val_acc: 0.1505\n",
      "Epoch 2/3\n",
      "19s - loss: 2.4006 - acc: 0.1479 - val_loss: 2.3877 - val_acc: 0.1515\n",
      "Epoch 3/3\n",
      "23s - loss: 2.3949 - acc: 0.1526 - val_loss: 2.3842 - val_acc: 0.1583\n",
      "('Log_loss:', 2.3842184359829961, 'It took', -57.3203809261322)\n",
      "('Average time per eval:', -107.43678711141858)\n",
      "Params testing 14: {'n_epoch': 3.3541761126539393, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 242.524049948715, 'dropout2': 0.6867596773140348}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "11s - loss: 2.4239 - acc: 0.1372 - val_loss: 2.4017 - val_acc: 0.1459\n",
      "Epoch 2/3\n",
      "20s - loss: 2.4128 - acc: 0.1432 - val_loss: 2.4056 - val_acc: 0.1448\n",
      "Epoch 3/3\n",
      "23s - loss: 2.4085 - acc: 0.1466 - val_loss: 2.4040 - val_acc: 0.1480\n",
      "('Log_loss:', 2.4040355018786377, 'It took', -60.992058992385864)\n",
      "('Average time per eval:', -104.34047183990478)\n",
      "Params testing 15: {'n_epoch': 5.820920446055657, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 195.8421427164853, 'dropout2': 0.42681926942071335}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "11s - loss: 2.4198 - acc: 0.1397 - val_loss: 2.3938 - val_acc: 0.1531\n",
      "Epoch 2/5\n",
      "20s - loss: 2.4056 - acc: 0.1468 - val_loss: 2.3901 - val_acc: 0.1538\n",
      "Epoch 3/5\n",
      "24s - loss: 2.4021 - acc: 0.1507 - val_loss: 2.3869 - val_acc: 0.1539\n",
      "Epoch 4/5\n",
      "26s - loss: 2.3960 - acc: 0.1513 - val_loss: 2.3875 - val_acc: 0.1598\n",
      "Epoch 5/5\n",
      "26s - loss: 2.3940 - acc: 0.1539 - val_loss: 2.3885 - val_acc: 0.1520\n",
      "('Log_loss:', 2.3884813081569911, 'It took', -113.62961912155151)\n",
      "('Average time per eval:', -104.92104348540306)\n",
      "Params testing 16: {'n_epoch': 6.012979363204156, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 133.83097300570367, 'dropout2': 0.853956339329873}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "11s - loss: 2.4316 - acc: 0.1320 - val_loss: 2.3974 - val_acc: 0.1409\n",
      "Epoch 2/6\n",
      "20s - loss: 2.4165 - acc: 0.1407 - val_loss: 2.3944 - val_acc: 0.1444\n",
      "Epoch 3/6\n",
      "24s - loss: 2.4133 - acc: 0.1426 - val_loss: 2.3930 - val_acc: 0.1503\n",
      "Epoch 4/6\n",
      "25s - loss: 2.4108 - acc: 0.1451 - val_loss: 2.3921 - val_acc: 0.1527\n",
      "Epoch 5/6\n",
      "27s - loss: 2.4091 - acc: 0.1440 - val_loss: 2.3905 - val_acc: 0.1526\n",
      "Epoch 6/6\n",
      "28s - loss: 2.4095 - acc: 0.1448 - val_loss: 2.3888 - val_acc: 0.1567\n",
      "('Log_loss:', 2.3888030748344318, 'It took', -141.58068203926086)\n",
      "('Average time per eval:', -107.0774926297805)\n",
      "Params testing 17: {'n_epoch': 3.434929279603609, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 195.49732395912488, 'dropout2': 0.2006893434895934}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "11s - loss: 2.4146 - acc: 0.1428 - val_loss: 2.3989 - val_acc: 0.1547\n",
      "Epoch 2/3\n",
      "20s - loss: 2.4026 - acc: 0.1485 - val_loss: 2.4019 - val_acc: 0.1558\n",
      "Epoch 3/3\n",
      "24s - loss: 2.3972 - acc: 0.1530 - val_loss: 2.3968 - val_acc: 0.1623\n",
      "('Log_loss:', 2.3967694960776074, 'It took', -61.62134003639221)\n",
      "('Average time per eval:', -104.5521507660548)\n",
      "Params testing 18: {'n_epoch': 5.089579590542474, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 236.17251614839574, 'dropout2': 0.23012315738056405}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "11s - loss: 2.4156 - acc: 0.1401 - val_loss: 2.4017 - val_acc: 0.1455\n",
      "Epoch 2/5\n",
      "20s - loss: 2.4039 - acc: 0.1491 - val_loss: 2.3957 - val_acc: 0.1496\n",
      "Epoch 3/5\n",
      "23s - loss: 2.3971 - acc: 0.1518 - val_loss: 2.3951 - val_acc: 0.1554\n",
      "Epoch 4/5\n",
      "25s - loss: 2.3939 - acc: 0.1549 - val_loss: 2.3935 - val_acc: 0.1556\n",
      "Epoch 5/5\n",
      "27s - loss: 2.3893 - acc: 0.1581 - val_loss: 2.3969 - val_acc: 0.1579\n",
      "('Log_loss:', 2.3969328844400377, 'It took', -114.49218916893005)\n",
      "('Average time per eval:', -105.07531068199559)\n",
      "Params testing 19: {'n_epoch': 5.231230143144419, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 232.49476595500198, 'dropout2': 0.659189293854485}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "11s - loss: 2.4247 - acc: 0.1377 - val_loss: 2.3935 - val_acc: 0.1500\n",
      "Epoch 2/5\n",
      "20s - loss: 2.4109 - acc: 0.1461 - val_loss: 2.3921 - val_acc: 0.1526\n",
      "Epoch 3/5\n",
      "24s - loss: 2.4063 - acc: 0.1497 - val_loss: 2.3902 - val_acc: 0.1536\n",
      "Epoch 4/5\n",
      "26s - loss: 2.4034 - acc: 0.1497 - val_loss: 2.3862 - val_acc: 0.1568\n",
      "Epoch 5/5\n",
      "27s - loss: 2.4031 - acc: 0.1497 - val_loss: 2.3852 - val_acc: 0.1548\n",
      "('Log_loss:', 2.3852174915004087, 'It took', -114.47386312484741)\n",
      "('Average time per eval:', -105.54523825645447)\n",
      "Params testing 20: {'n_epoch': 3.815813447894094, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 77.55457445398446, 'dropout2': 0.5144664176845197}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "10s - loss: 2.4176 - acc: 0.1397 - val_loss: 2.3934 - val_acc: 0.1447\n",
      "Epoch 2/3\n",
      "19s - loss: 2.4051 - acc: 0.1482 - val_loss: 2.3910 - val_acc: 0.1542\n",
      "Epoch 3/3\n",
      "23s - loss: 2.3991 - acc: 0.1495 - val_loss: 2.3872 - val_acc: 0.1552\n",
      "('Log_loss:', 2.3872082136349864, 'It took', -58.34907412528992)\n",
      "('Average time per eval:', -103.29780182384309)\n",
      "Params testing 21: {'n_epoch': 4.610125793478536, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 21.53805145092896, 'dropout2': 0.7320054370964078}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "10s - loss: 2.4262 - acc: 0.1363 - val_loss: 2.4005 - val_acc: 0.1448\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4096 - acc: 0.1440 - val_loss: 2.3940 - val_acc: 0.1523\n",
      "Epoch 3/4\n",
      "26s - loss: 2.4055 - acc: 0.1449 - val_loss: 2.3927 - val_acc: 0.1507\n",
      "Epoch 4/4\n",
      "25s - loss: 2.4025 - acc: 0.1496 - val_loss: 2.3902 - val_acc: 0.1495\n",
      "('Log_loss:', 2.3902210856199329, 'It took', -88.42970204353333)\n",
      "('Average time per eval:', -102.62197910655628)\n",
      "Params testing 22: {'n_epoch': 5.512134547883314, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 87.31263004784937, 'dropout2': 0.5470280779082856}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 2.4175 - acc: 0.1385 - val_loss: 2.3934 - val_acc: 0.1499\n",
      "Epoch 2/5\n",
      "19s - loss: 2.4053 - acc: 0.1472 - val_loss: 2.3879 - val_acc: 0.1550\n",
      "Epoch 3/5\n",
      "23s - loss: 2.3989 - acc: 0.1499 - val_loss: 2.3890 - val_acc: 0.1582\n",
      "Epoch 4/5\n",
      "25s - loss: 2.3960 - acc: 0.1530 - val_loss: 2.3862 - val_acc: 0.1587\n",
      "Epoch 5/5\n",
      "26s - loss: 2.3929 - acc: 0.1555 - val_loss: 2.3871 - val_acc: 0.1590\n",
      "('Log_loss:', 2.3871363071855001, 'It took', -110.49767017364502)\n",
      "('Average time per eval:', -102.96440040546915)\n",
      "Params testing 23: {'n_epoch': 3.8938900538406616, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 78.9385447139781, 'dropout2': 0.42122493956294604}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "11s - loss: 2.4166 - acc: 0.1385 - val_loss: 2.3931 - val_acc: 0.1463\n",
      "Epoch 2/3\n",
      "19s - loss: 2.4025 - acc: 0.1487 - val_loss: 2.3882 - val_acc: 0.1540\n",
      "Epoch 3/3\n",
      "23s - loss: 2.3969 - acc: 0.1534 - val_loss: 2.3864 - val_acc: 0.1550\n",
      "('Log_loss:', 2.3863537945761428, 'It took', -59.323671102523804)\n",
      "('Average time per eval:', -101.14603664477666)\n",
      "Params testing 24: {'n_epoch': 3.7358153013400432, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 16.52277576607605, 'dropout2': 0.43395228842625555}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "10s - loss: 2.4192 - acc: 0.1393 - val_loss: 2.3940 - val_acc: 0.1530\n",
      "Epoch 2/3\n",
      "19s - loss: 2.4025 - acc: 0.1487 - val_loss: 2.3913 - val_acc: 0.1540\n",
      "Epoch 3/3\n",
      "23s - loss: 2.3976 - acc: 0.1508 - val_loss: 2.3876 - val_acc: 0.1514\n",
      "('Log_loss:', 2.3876103600260081, 'It took', -58.051608085632324)\n",
      "('Average time per eval:', -99.42225945472717)\n",
      "Params testing 25: {'n_epoch': 3.0327638126764507, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 57.742438403602556, 'dropout2': 0.5054295434807582}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "11s - loss: 2.4171 - acc: 0.1396 - val_loss: 2.3930 - val_acc: 0.1530\n",
      "Epoch 2/3\n",
      "19s - loss: 2.4023 - acc: 0.1497 - val_loss: 2.3869 - val_acc: 0.1546\n",
      "Epoch 3/3\n",
      "23s - loss: 2.3982 - acc: 0.1518 - val_loss: 2.3861 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3860949375595575, 'It took', -59.788981914520264)\n",
      "('Average time per eval:', -97.89790255289812)\n",
      "Params testing 26: {'n_epoch': 4.400524902137286, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 114.37105771712993, 'dropout2': 0.5705570105307929}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4187 - acc: 0.1412 - val_loss: 2.3918 - val_acc: 0.1473\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4056 - acc: 0.1471 - val_loss: 2.3910 - val_acc: 0.1554\n",
      "Epoch 3/4\n",
      "23s - loss: 2.4006 - acc: 0.1506 - val_loss: 2.3880 - val_acc: 0.1585\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3977 - acc: 0.1512 - val_loss: 2.3876 - val_acc: 0.1523\n",
      "('Log_loss:', 2.3876077987826481, 'It took', -93.0677969455719)\n",
      "('Average time per eval:', -97.7190097173055)\n",
      "Params testing 27: {'n_epoch': 4.869573236210128, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 102.54212871395922, 'dropout2': 0.8865455171481116}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4346 - acc: 0.1291 - val_loss: 2.4042 - val_acc: 0.1452\n",
      "Epoch 2/4\n",
      "23s - loss: 2.4190 - acc: 0.1418 - val_loss: 2.3965 - val_acc: 0.1516\n",
      "Epoch 3/4\n",
      "26s - loss: 2.4157 - acc: 0.1401 - val_loss: 2.3952 - val_acc: 0.1476\n",
      "Epoch 4/4\n",
      "28s - loss: 2.4134 - acc: 0.1422 - val_loss: 2.3934 - val_acc: 0.1503\n",
      "('Log_loss:', 2.393433275920215, 'It took', -96.54585194587708)\n",
      "('Average time per eval:', -97.67711119140897)\n",
      "Params testing 28: {'n_epoch': 4.275768516744148, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 59.567919135689266, 'dropout2': 0.36122487477201654}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4155 - acc: 0.1409 - val_loss: 2.3946 - val_acc: 0.1546\n",
      "Epoch 2/4\n",
      "23s - loss: 2.4010 - acc: 0.1506 - val_loss: 2.3876 - val_acc: 0.1599\n",
      "Epoch 3/4\n",
      "27s - loss: 2.3951 - acc: 0.1529 - val_loss: 2.3877 - val_acc: 0.1580\n",
      "Epoch 4/4\n",
      "29s - loss: 2.3906 - acc: 0.1549 - val_loss: 2.3851 - val_acc: 0.1552\n",
      "('Log_loss:', 2.3851163778169653, 'It took', -98.19337701797485)\n",
      "('Average time per eval:', -97.69491342018391)\n",
      "Params testing 29: {'n_epoch': 6.548757493319004, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 31.23359472284135, 'dropout2': 0.37672827680936816}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4158 - acc: 0.1416 - val_loss: 2.3925 - val_acc: 0.1504\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4003 - acc: 0.1499 - val_loss: 2.3881 - val_acc: 0.1597\n",
      "Epoch 3/6\n",
      "26s - loss: 2.3951 - acc: 0.1524 - val_loss: 2.3860 - val_acc: 0.1621\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3918 - acc: 0.1550 - val_loss: 2.3851 - val_acc: 0.1566\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3893 - acc: 0.1566 - val_loss: 2.3847 - val_acc: 0.1605\n",
      "Epoch 6/6\n",
      "26s - loss: 2.3879 - acc: 0.1567 - val_loss: 2.3843 - val_acc: 0.1595\n",
      "('Log_loss:', 2.3842675431688636, 'It took', -151.55329298973083)\n",
      "('Average time per eval:', -99.49019270737966)\n",
      "Params testing 30: {'n_epoch': 6.827629585452351, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 30.71595921200697, 'dropout2': 0.38078954844681706}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "11s - loss: 2.4173 - acc: 0.1402 - val_loss: 2.3908 - val_acc: 0.1527\n",
      "Epoch 2/6\n",
      "20s - loss: 2.4013 - acc: 0.1489 - val_loss: 2.3880 - val_acc: 0.1551\n",
      "Epoch 3/6\n",
      "23s - loss: 2.3957 - acc: 0.1523 - val_loss: 2.3876 - val_acc: 0.1567\n",
      "Epoch 4/6\n",
      "24s - loss: 2.3919 - acc: 0.1540 - val_loss: 2.3851 - val_acc: 0.1590\n",
      "Epoch 5/6\n",
      "26s - loss: 2.3900 - acc: 0.1560 - val_loss: 2.3865 - val_acc: 0.1564\n",
      "Epoch 6/6\n",
      "27s - loss: 2.3871 - acc: 0.1569 - val_loss: 2.3856 - val_acc: 0.1562\n",
      "('Log_loss:', 2.3856160101033459, 'It took', -138.2604570388794)\n",
      "('Average time per eval:', -100.74084635703794)\n",
      "Params testing 31: {'n_epoch': 6.3984997186718005, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 68.50102732498469, 'dropout2': 0.39953763416488874}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "10s - loss: 2.4166 - acc: 0.1407 - val_loss: 2.3917 - val_acc: 0.1492\n",
      "Epoch 2/6\n",
      "19s - loss: 2.4017 - acc: 0.1486 - val_loss: 2.3877 - val_acc: 0.1572\n",
      "Epoch 3/6\n",
      "23s - loss: 2.3958 - acc: 0.1523 - val_loss: 2.3851 - val_acc: 0.1586\n",
      "Epoch 4/6\n",
      "26s - loss: 2.3924 - acc: 0.1566 - val_loss: 2.3852 - val_acc: 0.1575\n",
      "Epoch 5/6\n",
      "26s - loss: 2.3884 - acc: 0.1558 - val_loss: 2.3852 - val_acc: 0.1568\n",
      "Epoch 6/6\n",
      "27s - loss: 2.3862 - acc: 0.1584 - val_loss: 2.3864 - val_acc: 0.1512\n",
      "('Log_loss:', 2.3864087997318242, 'It took', -138.70233702659607)\n",
      "('Average time per eval:', -101.92714291065931)\n",
      "Params testing 32: {'n_epoch': 6.179552330483025, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 35.65521370017227, 'dropout2': 0.3093485424948995}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "10s - loss: 2.4161 - acc: 0.1404 - val_loss: 2.3908 - val_acc: 0.1539\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4007 - acc: 0.1498 - val_loss: 2.3892 - val_acc: 0.1500\n",
      "Epoch 3/6\n",
      "44s - loss: 2.3955 - acc: 0.1526 - val_loss: 2.3852 - val_acc: 0.1586\n",
      "Epoch 4/6\n",
      "26s - loss: 2.3904 - acc: 0.1544 - val_loss: 2.3848 - val_acc: 0.1586\n",
      "Epoch 5/6\n",
      "27s - loss: 2.3863 - acc: 0.1579 - val_loss: 2.3854 - val_acc: 0.1582\n",
      "Epoch 6/6\n",
      "28s - loss: 2.3850 - acc: 0.1580 - val_loss: 2.3846 - val_acc: 0.1583\n",
      "('Log_loss:', 2.3845620439546278, 'It took', -163.28719210624695)\n",
      "('Average time per eval:', -103.78653831192942)\n",
      "Params testing 33: {'n_epoch': 6.544988612367228, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 95.61521363041211, 'dropout2': 0.23060779898142397}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "11s - loss: 2.4148 - acc: 0.1424 - val_loss: 2.3912 - val_acc: 0.1536\n",
      "Epoch 2/6\n",
      "20s - loss: 2.4006 - acc: 0.1501 - val_loss: 2.3876 - val_acc: 0.1550\n",
      "Epoch 3/6\n",
      "23s - loss: 2.3945 - acc: 0.1544 - val_loss: 2.3875 - val_acc: 0.1566\n",
      "Epoch 4/6\n",
      "25s - loss: 2.3901 - acc: 0.1561 - val_loss: 2.3860 - val_acc: 0.1572\n",
      "Epoch 5/6\n",
      "26s - loss: 2.3866 - acc: 0.1591 - val_loss: 2.3857 - val_acc: 0.1556\n",
      "Epoch 6/6\n",
      "27s - loss: 2.3834 - acc: 0.1597 - val_loss: 2.3864 - val_acc: 0.1602\n",
      "('Log_loss:', 2.3864146383045637, 'It took', -138.1051208972931)\n",
      "('Average time per eval:', -104.79590835992028)\n",
      "Params testing 34: {'n_epoch': 5.442294113576054, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 19.527527636027862, 'dropout2': 0.4891346031446277}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 2.4187 - acc: 0.1417 - val_loss: 2.3939 - val_acc: 0.1511\n",
      "Epoch 2/5\n",
      "19s - loss: 2.4040 - acc: 0.1451 - val_loss: 2.3907 - val_acc: 0.1523\n",
      "Epoch 3/5\n",
      "23s - loss: 2.3985 - acc: 0.1488 - val_loss: 2.3881 - val_acc: 0.1546\n",
      "Epoch 4/5\n",
      "25s - loss: 2.3956 - acc: 0.1502 - val_loss: 2.3876 - val_acc: 0.1552\n",
      "Epoch 5/5\n",
      "26s - loss: 2.3935 - acc: 0.1509 - val_loss: 2.3886 - val_acc: 0.1540\n",
      "('Log_loss:', 2.3886445256330804, 'It took', -109.48369812965393)\n",
      "('Average time per eval:', -104.92984518323625)\n",
      "Params testing 35: {'n_epoch': 5.7046187475431065, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 50.5127458463905, 'dropout2': 0.4564131322700927}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 2.4165 - acc: 0.1415 - val_loss: 2.3927 - val_acc: 0.1512\n",
      "Epoch 2/5\n",
      "20s - loss: 2.4029 - acc: 0.1458 - val_loss: 2.3894 - val_acc: 0.1546\n",
      "Epoch 3/5\n",
      "23s - loss: 2.3975 - acc: 0.1500 - val_loss: 2.3882 - val_acc: 0.1522\n",
      "Epoch 4/5\n",
      "25s - loss: 2.3937 - acc: 0.1533 - val_loss: 2.3874 - val_acc: 0.1536\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3913 - acc: 0.1535 - val_loss: 2.3857 - val_acc: 0.1574\n",
      "('Log_loss:', 2.3856639218432574, 'It took', -111.9825279712677)\n",
      "('Average time per eval:', -105.1257529589865)\n",
      "Params testing 36: {'n_epoch': 6.944964098222628, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 31.361936909760317, 'dropout2': 0.24524968917522624}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4148 - acc: 0.1402 - val_loss: 2.3923 - val_acc: 0.1473\n",
      "Epoch 2/6\n",
      "23s - loss: 2.3994 - acc: 0.1482 - val_loss: 2.3876 - val_acc: 0.1568\n",
      "Epoch 3/6\n",
      "26s - loss: 2.3942 - acc: 0.1527 - val_loss: 2.3853 - val_acc: 0.1540\n",
      "Epoch 4/6\n",
      "29s - loss: 2.3906 - acc: 0.1552 - val_loss: 2.3843 - val_acc: 0.1554\n",
      "Epoch 5/6\n",
      "30s - loss: 2.3870 - acc: 0.1557 - val_loss: 2.3846 - val_acc: 0.1552\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3851 - acc: 0.1553 - val_loss: 2.3852 - val_acc: 0.1597\n",
      "('Log_loss:', 2.3852039150696096, 'It took', -159.10080695152283)\n",
      "('Average time per eval:', -106.58453817625303)\n",
      "Params testing 37: {'n_epoch': 6.52814592258986, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 111.96750204945536, 'dropout2': 0.327531243676938}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4159 - acc: 0.1424 - val_loss: 2.3938 - val_acc: 0.1460\n",
      "Epoch 2/6\n",
      "23s - loss: 2.4016 - acc: 0.1490 - val_loss: 2.3890 - val_acc: 0.1579\n",
      "Epoch 3/6\n",
      "27s - loss: 2.3960 - acc: 0.1554 - val_loss: 2.3878 - val_acc: 0.1586\n",
      "Epoch 4/6\n",
      "29s - loss: 2.3922 - acc: 0.1561 - val_loss: 2.3858 - val_acc: 0.1590\n",
      "Epoch 5/6\n",
      "30s - loss: 2.3888 - acc: 0.1562 - val_loss: 2.3862 - val_acc: 0.1586\n",
      "Epoch 6/6\n",
      "31s - loss: 2.3877 - acc: 0.1574 - val_loss: 2.3889 - val_acc: 0.1562\n",
      "('Log_loss:', 2.3889104628767304, 'It took', -162.48323798179626)\n",
      "('Average time per eval:', -108.05555656709168)\n",
      "Params testing 38: {'n_epoch': 3.0182636546461366, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 45.11860807631363, 'dropout2': 0.5435561581186727}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4193 - acc: 0.1383 - val_loss: 2.3946 - val_acc: 0.1480\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4042 - acc: 0.1485 - val_loss: 2.3910 - val_acc: 0.1523\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3989 - acc: 0.1497 - val_loss: 2.3894 - val_acc: 0.1531\n",
      "('Log_loss:', 2.3894196078281058, 'It took', -66.66478204727173)\n",
      "('Average time per eval:', -106.99425463187389)\n",
      "Params testing 39: {'n_epoch': 6.222854969169354, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 151.54516462751906, 'dropout2': 0.28064193490407197}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "11s - loss: 2.4158 - acc: 0.1414 - val_loss: 2.3945 - val_acc: 0.1514\n",
      "Epoch 2/6\n",
      "20s - loss: 2.4028 - acc: 0.1487 - val_loss: 2.3912 - val_acc: 0.1566\n",
      "Epoch 3/6\n",
      "23s - loss: 2.3970 - acc: 0.1529 - val_loss: 2.3877 - val_acc: 0.1560\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3931 - acc: 0.1527 - val_loss: 2.3867 - val_acc: 0.1547\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3887 - acc: 0.1572 - val_loss: 2.3885 - val_acc: 0.1473\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3867 - acc: 0.1578 - val_loss: 2.3868 - val_acc: 0.1539\n",
      "('Log_loss:', 2.3867889893236987, 'It took', -147.80032014846802)\n",
      "('Average time per eval:', -108.01440623998641)\n",
      "Params testing 40: {'n_epoch': 5.309578183306288, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 70.76829014272565, 'dropout2': 0.3632152813285819}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4161 - acc: 0.1407 - val_loss: 2.3936 - val_acc: 0.1530\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4006 - acc: 0.1502 - val_loss: 2.3877 - val_acc: 0.1538\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3963 - acc: 0.1507 - val_loss: 2.3868 - val_acc: 0.1539\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3912 - acc: 0.1565 - val_loss: 2.3857 - val_acc: 0.1597\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3888 - acc: 0.1564 - val_loss: 2.3861 - val_acc: 0.1610\n",
      "('Log_loss:', 2.3860844700386914, 'It took', -123.06113314628601)\n",
      "('Average time per eval:', -108.38139952682867)\n",
      "Params testing 41: {'n_epoch': 4.061635574676585, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 127.91944727944406, 'dropout2': 0.4049188945984329}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4174 - acc: 0.1414 - val_loss: 2.3925 - val_acc: 0.1523\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4035 - acc: 0.1478 - val_loss: 2.3876 - val_acc: 0.1524\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3984 - acc: 0.1521 - val_loss: 2.3869 - val_acc: 0.1556\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3956 - acc: 0.1537 - val_loss: 2.3865 - val_acc: 0.1622\n",
      "('Log_loss:', 2.3864611116772583, 'It took', -95.27499389648438)\n",
      "('Average time per eval:', -108.06934220450265)\n",
      "Params testing 42: {'n_epoch': 3.6277603672674035, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 254.2794295574273, 'dropout2': 0.29683935899799335}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "14s - loss: 2.4156 - acc: 0.1426 - val_loss: 2.3993 - val_acc: 0.1491\n",
      "Epoch 2/3\n",
      "23s - loss: 2.4037 - acc: 0.1482 - val_loss: 2.3967 - val_acc: 0.1559\n",
      "Epoch 3/3\n",
      "27s - loss: 2.3988 - acc: 0.1512 - val_loss: 2.3962 - val_acc: 0.1527\n",
      "('Log_loss:', 2.3961723940863608, 'It took', -70.45652294158936)\n",
      "('Average time per eval:', -107.19462545527968)\n",
      "Params testing 43: {'n_epoch': 3.2636308219763803, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 151.3892706552878, 'dropout2': 0.4499976512365789}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4183 - acc: 0.1389 - val_loss: 2.3916 - val_acc: 0.1547\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4042 - acc: 0.1485 - val_loss: 2.3878 - val_acc: 0.1626\n",
      "Epoch 3/3\n",
      "26s - loss: 2.3984 - acc: 0.1507 - val_loss: 2.3876 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3875531759557154, 'It took', -67.39249897003174)\n",
      "('Average time per eval:', -106.29003164984964)\n",
      "Params testing 44: {'n_epoch': 4.67228725269335, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 91.08476254886504, 'dropout2': 0.34019464700512597}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4164 - acc: 0.1404 - val_loss: 2.3958 - val_acc: 0.1511\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4027 - acc: 0.1502 - val_loss: 2.3952 - val_acc: 0.1479\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3979 - acc: 0.1508 - val_loss: 2.3916 - val_acc: 0.1571\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3941 - acc: 0.1527 - val_loss: 2.3898 - val_acc: 0.1591\n",
      "('Log_loss:', 2.3898302869361814, 'It took', -94.84273290634155)\n",
      "('Average time per eval:', -106.0356471909417)\n",
      "Params testing 45: {'n_epoch': 5.013561886049083, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 165.44558582519875, 'dropout2': 0.2611459548365584}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4175 - acc: 0.1396 - val_loss: 2.3953 - val_acc: 0.1493\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4037 - acc: 0.1464 - val_loss: 2.3892 - val_acc: 0.1507\n",
      "Epoch 3/5\n",
      "26s - loss: 2.3971 - acc: 0.1514 - val_loss: 2.3894 - val_acc: 0.1560\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3936 - acc: 0.1541 - val_loss: 2.3861 - val_acc: 0.1575\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3901 - acc: 0.1551 - val_loss: 2.3869 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3868589398530733, 'It took', -126.52915501594543)\n",
      "('Average time per eval:', -106.4811581891516)\n",
      "Params testing 46: {'n_epoch': 5.691000696221543, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 60.19850198358657, 'dropout2': 0.2013611841141948}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4144 - acc: 0.1431 - val_loss: 2.3941 - val_acc: 0.1512\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4003 - acc: 0.1487 - val_loss: 2.3892 - val_acc: 0.1535\n",
      "Epoch 3/5\n",
      "27s - loss: 2.3959 - acc: 0.1524 - val_loss: 2.3908 - val_acc: 0.1534\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3913 - acc: 0.1556 - val_loss: 2.3869 - val_acc: 0.1558\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3883 - acc: 0.1555 - val_loss: 2.3866 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3865904757429184, 'It took', -125.90621280670166)\n",
      "('Average time per eval:', -106.89445716269474)\n",
      "Params testing 47: {'n_epoch': 6.0472623786831505, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 123.1058338240993, 'dropout2': 0.6088916164530072}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4208 - acc: 0.1380 - val_loss: 2.3942 - val_acc: 0.1542\n",
      "Epoch 2/6\n",
      "24s - loss: 2.4065 - acc: 0.1476 - val_loss: 2.3901 - val_acc: 0.1514\n",
      "Epoch 3/6\n",
      "27s - loss: 2.4020 - acc: 0.1503 - val_loss: 2.3875 - val_acc: 0.1610\n",
      "Epoch 4/6\n",
      "30s - loss: 2.3982 - acc: 0.1519 - val_loss: 2.3861 - val_acc: 0.1558\n",
      "Epoch 5/6\n",
      "30s - loss: 2.3976 - acc: 0.1512 - val_loss: 2.3863 - val_acc: 0.1552\n",
      "Epoch 6/6\n",
      "32s - loss: 2.3922 - acc: 0.1550 - val_loss: 2.3857 - val_acc: 0.1591\n",
      "('Log_loss:', 2.3857368870296267, 'It took', -164.82882380485535)\n",
      "('Average time per eval:', -108.10142311453819)\n",
      "Params testing 48: {'n_epoch': 6.685642791315607, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 26.97123317374903, 'dropout2': 0.5205670265663271}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "14s - loss: 2.4218 - acc: 0.1391 - val_loss: 2.3991 - val_acc: 0.1413\n",
      "Epoch 2/6\n",
      "23s - loss: 2.4071 - acc: 0.1457 - val_loss: 2.3957 - val_acc: 0.1445\n",
      "Epoch 3/6\n",
      "27s - loss: 2.4029 - acc: 0.1467 - val_loss: 2.3948 - val_acc: 0.1485\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3996 - acc: 0.1484 - val_loss: 2.3934 - val_acc: 0.1554\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3977 - acc: 0.1490 - val_loss: 2.3905 - val_acc: 0.1528\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3969 - acc: 0.1510 - val_loss: 2.3910 - val_acc: 0.1530\n",
      "('Log_loss:', 2.3910064412894148, 'It took', -160.82259583473206)\n",
      "('Average time per eval:', -109.17736539548757)\n",
      "Params testing 49: {'n_epoch': 4.453961985997321, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 41.44273888850603, 'dropout2': 0.20529104326612602}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4147 - acc: 0.1397 - val_loss: 2.3935 - val_acc: 0.1563\n",
      "Epoch 2/4\n",
      "22s - loss: 2.3986 - acc: 0.1496 - val_loss: 2.3891 - val_acc: 0.1552\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3931 - acc: 0.1540 - val_loss: 2.3861 - val_acc: 0.1587\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3895 - acc: 0.1558 - val_loss: 2.3855 - val_acc: 0.1564\n",
      "('Log_loss:', 2.3855207037772437, 'It took', -94.77774405479431)\n",
      "('Average time per eval:', -108.88937293052673)\n",
      "Params testing 50: {'n_epoch': 3.95166863743265, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 104.7779755507419, 'dropout2': 0.472942361729623}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4174 - acc: 0.1390 - val_loss: 2.3955 - val_acc: 0.1475\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4030 - acc: 0.1482 - val_loss: 2.3910 - val_acc: 0.1484\n",
      "Epoch 3/3\n",
      "26s - loss: 2.3976 - acc: 0.1522 - val_loss: 2.3880 - val_acc: 0.1580\n",
      "('Log_loss:', 2.388001585961538, 'It took', -67.54398703575134)\n",
      "('Average time per eval:', -108.07867905205372)\n",
      "Params testing 51: {'n_epoch': 3.1331182533131865, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 215.063600514789, 'dropout2': 0.757464398858047}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "14s - loss: 2.4257 - acc: 0.1350 - val_loss: 2.4056 - val_acc: 0.1475\n",
      "Epoch 2/3\n",
      "23s - loss: 2.4150 - acc: 0.1405 - val_loss: 2.4057 - val_acc: 0.1483\n",
      "Epoch 3/3\n",
      "27s - loss: 2.4113 - acc: 0.1446 - val_loss: 2.4046 - val_acc: 0.1480\n",
      "('Log_loss:', 2.4045713072707051, 'It took', -71.24149703979492)\n",
      "('Average time per eval:', -107.37027168732423)\n",
      "Params testing 52: {'n_epoch': 4.190065668963173, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 83.7388957718639, 'dropout2': 0.6818306990975816}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4221 - acc: 0.1387 - val_loss: 2.3946 - val_acc: 0.1519\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4074 - acc: 0.1458 - val_loss: 2.3887 - val_acc: 0.1548\n",
      "Epoch 3/4\n",
      "26s - loss: 2.4023 - acc: 0.1482 - val_loss: 2.3891 - val_acc: 0.1574\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3991 - acc: 0.1514 - val_loss: 2.3854 - val_acc: 0.1554\n",
      "('Log_loss:', 2.385426179352951, 'It took', -96.229562997818)\n",
      "('Average time per eval:', -107.16006961408651)\n",
      "Params testing 53: {'n_epoch': 5.188701237870577, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 139.56361536245643, 'dropout2': 0.39308770110621694}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "14s - loss: 2.4182 - acc: 0.1395 - val_loss: 2.3939 - val_acc: 0.1451\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4033 - acc: 0.1466 - val_loss: 2.3899 - val_acc: 0.1556\n",
      "Epoch 3/5\n",
      "26s - loss: 2.3982 - acc: 0.1500 - val_loss: 2.3887 - val_acc: 0.1562\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3942 - acc: 0.1539 - val_loss: 2.3881 - val_acc: 0.1532\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3921 - acc: 0.1559 - val_loss: 2.3866 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3865872490287883, 'It took', -126.41516995429993)\n",
      "('Average time per eval:', -107.51664550657625)\n",
      "Params testing 54: {'n_epoch': 4.850720765166041, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 71.60575455519708, 'dropout2': 0.5772882762246774}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4211 - acc: 0.1391 - val_loss: 2.3966 - val_acc: 0.1492\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4084 - acc: 0.1450 - val_loss: 2.3927 - val_acc: 0.1566\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4027 - acc: 0.1472 - val_loss: 2.3943 - val_acc: 0.1554\n",
      "Epoch 4/4\n",
      "28s - loss: 2.4006 - acc: 0.1482 - val_loss: 2.3928 - val_acc: 0.1511\n",
      "('Log_loss:', 2.3928248599678588, 'It took', -95.4607949256897)\n",
      "('Average time per eval:', -107.29744820594787)\n",
      "Params testing 55: {'n_epoch': 6.989272751241532, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 49.44999799678861, 'dropout2': 0.33537693612706304}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4153 - acc: 0.1408 - val_loss: 2.3921 - val_acc: 0.1522\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4006 - acc: 0.1485 - val_loss: 2.3881 - val_acc: 0.1536\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3944 - acc: 0.1525 - val_loss: 2.3871 - val_acc: 0.1572\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3916 - acc: 0.1545 - val_loss: 2.3863 - val_acc: 0.1590\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3880 - acc: 0.1571 - val_loss: 2.3848 - val_acc: 0.1563\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3849 - acc: 0.1578 - val_loss: 2.3845 - val_acc: 0.1570\n",
      "('Log_loss:', 2.3845287375880604, 'It took', -152.89034509658813)\n",
      "('Average time per eval:', -108.11160706196513)\n",
      "Params testing 56: {'n_epoch': 5.841548515412242, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 183.75394735160572, 'dropout2': 0.264488541997058}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4176 - acc: 0.1416 - val_loss: 2.3919 - val_acc: 0.1515\n",
      "Epoch 2/5\n",
      "23s - loss: 2.4043 - acc: 0.1487 - val_loss: 2.3885 - val_acc: 0.1548\n",
      "Epoch 3/5\n",
      "26s - loss: 2.3983 - acc: 0.1508 - val_loss: 2.3886 - val_acc: 0.1516\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3938 - acc: 0.1521 - val_loss: 2.3898 - val_acc: 0.1518\n",
      "Epoch 5/5\n",
      "30s - loss: 2.3905 - acc: 0.1552 - val_loss: 2.3893 - val_acc: 0.1516\n",
      "('Log_loss:', 2.3893228647793836, 'It took', -127.24210906028748)\n",
      "('Average time per eval:', -108.44722988730983)\n",
      "Params testing 57: {'n_epoch': 3.5866454968711845, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 99.4443081616354, 'dropout2': 0.8338272713530863}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4290 - acc: 0.1336 - val_loss: 2.3994 - val_acc: 0.1483\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4146 - acc: 0.1411 - val_loss: 2.3937 - val_acc: 0.1527\n",
      "Epoch 3/3\n",
      "25s - loss: 2.4102 - acc: 0.1451 - val_loss: 2.3926 - val_acc: 0.1530\n",
      "('Log_loss:', 2.3925669641460261, 'It took', -66.57143020629883)\n",
      "('Average time per eval:', -107.72523332053217)\n",
      "Params testing 58: {'n_epoch': 3.35001623498705, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 16.948236774580266, 'dropout2': 0.311768716922541}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4192 - acc: 0.1372 - val_loss: 2.3964 - val_acc: 0.1467\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4037 - acc: 0.1450 - val_loss: 2.3924 - val_acc: 0.1510\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3998 - acc: 0.1479 - val_loss: 2.3903 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3903011815528146, 'It took', -66.41406011581421)\n",
      "('Average time per eval:', -107.02504390781209)\n",
      "Params testing 59: {'n_epoch': 5.570327500152006, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 55.3655249820703, 'dropout2': 0.3683480469549059}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4149 - acc: 0.1420 - val_loss: 2.3925 - val_acc: 0.1491\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4010 - acc: 0.1498 - val_loss: 2.3882 - val_acc: 0.1563\n",
      "Epoch 3/5\n",
      "26s - loss: 2.3957 - acc: 0.1527 - val_loss: 2.3871 - val_acc: 0.1570\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3915 - acc: 0.1557 - val_loss: 2.3868 - val_acc: 0.1611\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3886 - acc: 0.1568 - val_loss: 2.3855 - val_acc: 0.1536\n",
      "('Log_loss:', 2.3854552657968902, 'It took', -123.38498497009277)\n",
      "('Average time per eval:', -107.29770957628885)\n",
      "Params testing 60: {'n_epoch': 4.66646288502047, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 38.380594276117534, 'dropout2': 0.6436387468976907}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4217 - acc: 0.1373 - val_loss: 2.3971 - val_acc: 0.1507\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4069 - acc: 0.1449 - val_loss: 2.3913 - val_acc: 0.1531\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4020 - acc: 0.1484 - val_loss: 2.3899 - val_acc: 0.1531\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3982 - acc: 0.1516 - val_loss: 2.3892 - val_acc: 0.1582\n",
      "('Log_loss:', 2.3892152872731915, 'It took', -93.3986668586731)\n",
      "('Average time per eval:', -107.06985640134968)\n",
      "Params testing 61: {'n_epoch': 3.7293756445702404, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 65.77209623029987, 'dropout2': 0.42151721400762543}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4169 - acc: 0.1395 - val_loss: 2.3932 - val_acc: 0.1491\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4020 - acc: 0.1478 - val_loss: 2.3890 - val_acc: 0.1571\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3963 - acc: 0.1505 - val_loss: 2.3878 - val_acc: 0.1548\n",
      "('Log_loss:', 2.3878284337499522, 'It took', -66.52184200286865)\n",
      "('Average time per eval:', -106.41585615373427)\n",
      "Params testing 62: {'n_epoch': 6.342884309387896, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 162.2463112637173, 'dropout2': 0.5287976789346347}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4198 - acc: 0.1382 - val_loss: 2.3945 - val_acc: 0.1548\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4067 - acc: 0.1476 - val_loss: 2.3879 - val_acc: 0.1575\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4016 - acc: 0.1500 - val_loss: 2.3874 - val_acc: 0.1582\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3966 - acc: 0.1517 - val_loss: 2.3877 - val_acc: 0.1597\n",
      "Epoch 5/6\n",
      "31s - loss: 2.3948 - acc: 0.1552 - val_loss: 2.3869 - val_acc: 0.1547\n",
      "Epoch 6/6\n",
      "31s - loss: 2.3930 - acc: 0.1560 - val_loss: 2.3870 - val_acc: 0.1538\n",
      "('Log_loss:', 2.3870036343515255, 'It took', -159.7368369102478)\n",
      "('Average time per eval:', -107.2622208973718)\n",
      "Params testing 63: {'n_epoch': 5.377420775125618, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 108.4293359280266, 'dropout2': 0.49594766455642914}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4181 - acc: 0.1395 - val_loss: 2.4024 - val_acc: 0.1439\n",
      "Epoch 2/5\n",
      "23s - loss: 2.4061 - acc: 0.1476 - val_loss: 2.3951 - val_acc: 0.1511\n",
      "Epoch 3/5\n",
      "27s - loss: 2.4008 - acc: 0.1494 - val_loss: 2.3923 - val_acc: 0.1554\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3977 - acc: 0.1501 - val_loss: 2.3949 - val_acc: 0.1520\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3954 - acc: 0.1528 - val_loss: 2.3916 - val_acc: 0.1493\n",
      "('Log_loss:', 2.3915855186157371, 'It took', -129.83012795448303)\n",
      "('Average time per eval:', -107.61484441161156)\n",
      "Params testing 64: {'n_epoch': 5.105404461961343, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 25.588073825954, 'dropout2': 0.23186582454470892}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4156 - acc: 0.1413 - val_loss: 2.3913 - val_acc: 0.1530\n",
      "Epoch 2/5\n",
      "19s - loss: 2.3989 - acc: 0.1490 - val_loss: 2.3892 - val_acc: 0.1500\n",
      "Epoch 3/5\n",
      "23s - loss: 2.3935 - acc: 0.1524 - val_loss: 2.3857 - val_acc: 0.1560\n",
      "Epoch 4/5\n",
      "24s - loss: 2.3902 - acc: 0.1556 - val_loss: 2.3860 - val_acc: 0.1566\n",
      "Epoch 5/5\n",
      "26s - loss: 2.3864 - acc: 0.1559 - val_loss: 2.3839 - val_acc: 0.1575\n",
      "('Log_loss:', 2.3838982270417315, 'It took', -111.10344409942627)\n",
      "('Average time per eval:', -107.66851514302768)\n",
      "Params testing 65: {'n_epoch': 4.777458955484898, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 25.382540281650826, 'dropout2': 0.22287399758174395}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "10s - loss: 2.4146 - acc: 0.1423 - val_loss: 2.3934 - val_acc: 0.1495\n",
      "Epoch 2/4\n",
      "19s - loss: 2.3985 - acc: 0.1480 - val_loss: 2.3887 - val_acc: 0.1570\n",
      "Epoch 3/4\n",
      "24s - loss: 2.3942 - acc: 0.1511 - val_loss: 2.3847 - val_acc: 0.1583\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3890 - acc: 0.1539 - val_loss: 2.3848 - val_acc: 0.1580\n",
      "('Log_loss:', 2.3848468877181528, 'It took', -87.545725107193)\n",
      "('Average time per eval:', -107.36362436684695)\n",
      "Params testing 66: {'n_epoch': 5.889081742073414, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 33.15876649814976, 'dropout2': 0.24268789918197758}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4139 - acc: 0.1417 - val_loss: 2.3922 - val_acc: 0.1530\n",
      "Epoch 2/5\n",
      "21s - loss: 2.3997 - acc: 0.1503 - val_loss: 2.3912 - val_acc: 0.1559\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3939 - acc: 0.1507 - val_loss: 2.3868 - val_acc: 0.1555\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3895 - acc: 0.1551 - val_loss: 2.3871 - val_acc: 0.1590\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3873 - acc: 0.1562 - val_loss: 2.3857 - val_acc: 0.1532\n",
      "('Log_loss:', 2.3857261059656558, 'It took', -122.26714992523193)\n",
      "('Average time per eval:', -107.5860650325889)\n",
      "Params testing 67: {'n_epoch': 5.012869071884211, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 81.94497916175919, 'dropout2': 0.29213256197386067}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4151 - acc: 0.1403 - val_loss: 2.3951 - val_acc: 0.1405\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4015 - acc: 0.1486 - val_loss: 2.3895 - val_acc: 0.1495\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3952 - acc: 0.1533 - val_loss: 2.3873 - val_acc: 0.1551\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3909 - acc: 0.1544 - val_loss: 2.3858 - val_acc: 0.1576\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3879 - acc: 0.1579 - val_loss: 2.3868 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3867772456119822, 'It took', -122.53885197639465)\n",
      "('Average time per eval:', -107.80595894420848)\n",
      "Params testing 68: {'n_epoch': 4.342140732651917, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 48.921394669849334, 'dropout2': 0.44540233184943406}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4170 - acc: 0.1416 - val_loss: 2.3926 - val_acc: 0.1479\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4018 - acc: 0.1484 - val_loss: 2.3879 - val_acc: 0.1544\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3955 - acc: 0.1538 - val_loss: 2.3870 - val_acc: 0.1585\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3923 - acc: 0.1543 - val_loss: 2.3861 - val_acc: 0.1578\n",
      "('Log_loss:', 2.3860567270224515, 'It took', -93.475172996521)\n",
      "('Average time per eval:', -107.59826637696528)\n",
      "Params testing 69: {'n_epoch': 5.16707308389614, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 17.14658098630895, 'dropout2': 0.35054504961883587}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4177 - acc: 0.1392 - val_loss: 2.3943 - val_acc: 0.1511\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4015 - acc: 0.1500 - val_loss: 2.3904 - val_acc: 0.1543\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3966 - acc: 0.1499 - val_loss: 2.3867 - val_acc: 0.1540\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3928 - acc: 0.1559 - val_loss: 2.3859 - val_acc: 0.1580\n",
      "Epoch 5/5\n",
      "26s - loss: 2.3904 - acc: 0.1562 - val_loss: 2.3864 - val_acc: 0.1543\n",
      "('Log_loss:', 2.3863594114157713, 'It took', -120.30567193031311)\n",
      "('Average time per eval:', -107.77980072838919)\n",
      "Params testing 70: {'n_epoch': 6.819766618531678, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 24.273242315055292, 'dropout2': 0.4780429988371979}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "10s - loss: 2.4183 - acc: 0.1392 - val_loss: 2.3950 - val_acc: 0.1507\n",
      "Epoch 2/6\n",
      "19s - loss: 2.4029 - acc: 0.1504 - val_loss: 2.3914 - val_acc: 0.1519\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3979 - acc: 0.1510 - val_loss: 2.3876 - val_acc: 0.1538\n",
      "Epoch 4/6\n",
      "26s - loss: 2.3930 - acc: 0.1552 - val_loss: 2.3856 - val_acc: 0.1566\n",
      "Epoch 5/6\n",
      "26s - loss: 2.3923 - acc: 0.1542 - val_loss: 2.3859 - val_acc: 0.1583\n",
      "Epoch 6/6\n",
      "28s - loss: 2.3898 - acc: 0.1562 - val_loss: 2.3860 - val_acc: 0.1586\n",
      "('Log_loss:', 2.3860179936633554, 'It took', -141.57667589187622)\n",
      "('Average time per eval:', -108.25581304120345)\n",
      "Params testing 71: {'n_epoch': 5.636390548578854, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 75.72344336343644, 'dropout2': 0.3211723790901566}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 2.4162 - acc: 0.1413 - val_loss: 2.3934 - val_acc: 0.1558\n",
      "Epoch 2/5\n",
      "20s - loss: 2.4013 - acc: 0.1484 - val_loss: 2.3880 - val_acc: 0.1576\n",
      "Epoch 3/5\n",
      "24s - loss: 2.3955 - acc: 0.1539 - val_loss: 2.3864 - val_acc: 0.1572\n",
      "Epoch 4/5\n",
      "25s - loss: 2.3912 - acc: 0.1541 - val_loss: 2.3858 - val_acc: 0.1550\n",
      "Epoch 5/5\n",
      "27s - loss: 2.3880 - acc: 0.1595 - val_loss: 2.3869 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3869259085056131, 'It took', -113.4808030128479)\n",
      "('Average time per eval:', -108.3283823331197)\n",
      "Params testing 72: {'n_epoch': 6.158311115420674, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 64.16926943637523, 'dropout2': 0.3850386103548552}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "11s - loss: 2.4162 - acc: 0.1404 - val_loss: 2.3928 - val_acc: 0.1551\n",
      "Epoch 2/6\n",
      "20s - loss: 2.4016 - acc: 0.1497 - val_loss: 2.3879 - val_acc: 0.1556\n",
      "Epoch 3/6\n",
      "23s - loss: 2.3960 - acc: 0.1540 - val_loss: 2.3894 - val_acc: 0.1550\n",
      "Epoch 4/6\n",
      "25s - loss: 2.3926 - acc: 0.1538 - val_loss: 2.3852 - val_acc: 0.1594\n",
      "Epoch 5/6\n",
      "26s - loss: 2.3887 - acc: 0.1568 - val_loss: 2.3860 - val_acc: 0.1524\n",
      "Epoch 6/6\n",
      "28s - loss: 2.3864 - acc: 0.1583 - val_loss: 2.3851 - val_acc: 0.1599\n",
      "('Log_loss:', 2.3851460776794, 'It took', -140.01294207572937)\n",
      "('Average time per eval:', -108.76241738175693)\n",
      "Params testing 73: {'n_epoch': 3.939466848258963, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 37.320113763827166, 'dropout2': 0.2707038053315752}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "10s - loss: 2.4151 - acc: 0.1416 - val_loss: 2.3917 - val_acc: 0.1520\n",
      "Epoch 2/3\n",
      "19s - loss: 2.4002 - acc: 0.1504 - val_loss: 2.3880 - val_acc: 0.1580\n",
      "Epoch 3/3\n",
      "23s - loss: 2.3939 - acc: 0.1520 - val_loss: 2.3861 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3860531724747913, 'It took', -58.68426299095154)\n",
      "('Average time per eval:', -108.08568555277748)\n",
      "Params testing 74: {'n_epoch': 4.493403013637311, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 121.46158267686891, 'dropout2': 0.4111531375694364}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "11s - loss: 2.4170 - acc: 0.1418 - val_loss: 2.3930 - val_acc: 0.1543\n",
      "Epoch 2/4\n",
      "19s - loss: 2.4038 - acc: 0.1478 - val_loss: 2.3880 - val_acc: 0.1567\n",
      "Epoch 3/4\n",
      "24s - loss: 2.3982 - acc: 0.1509 - val_loss: 2.3869 - val_acc: 0.1564\n",
      "Epoch 4/4\n",
      "26s - loss: 2.3941 - acc: 0.1544 - val_loss: 2.3846 - val_acc: 0.1590\n",
      "('Log_loss:', 2.3845936142181601, 'It took', -86.50393199920654)\n",
      "('Average time per eval:', -107.7979288260142)\n",
      "Params testing 75: {'n_epoch': 4.162788909812213, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 44.703396295557496, 'dropout2': 0.21461614225785486}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "10s - loss: 2.4144 - acc: 0.1428 - val_loss: 2.3916 - val_acc: 0.1483\n",
      "Epoch 2/4\n",
      "19s - loss: 2.3995 - acc: 0.1515 - val_loss: 2.3896 - val_acc: 0.1516\n",
      "Epoch 3/4\n",
      "23s - loss: 2.3931 - acc: 0.1547 - val_loss: 2.3869 - val_acc: 0.1572\n",
      "Epoch 4/4\n",
      "25s - loss: 2.3891 - acc: 0.1552 - val_loss: 2.3859 - val_acc: 0.1568\n",
      "('Log_loss:', 2.3858924863678888, 'It took', -82.81105303764343)\n",
      "('Average time per eval:', -107.46915412890284)\n",
      "Params testing 76: {'n_epoch': 6.5695721600172154, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 92.17988247380416, 'dropout2': 0.3744118365286875}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "11s - loss: 2.4165 - acc: 0.1397 - val_loss: 2.3921 - val_acc: 0.1512\n",
      "Epoch 2/6\n",
      "19s - loss: 2.4030 - acc: 0.1481 - val_loss: 2.3895 - val_acc: 0.1520\n",
      "Epoch 3/6\n",
      "23s - loss: 2.3961 - acc: 0.1522 - val_loss: 2.3867 - val_acc: 0.1548\n",
      "Epoch 4/6\n",
      "25s - loss: 2.3930 - acc: 0.1528 - val_loss: 2.3883 - val_acc: 0.1548\n",
      "Epoch 5/6\n",
      "26s - loss: 2.3892 - acc: 0.1565 - val_loss: 2.3850 - val_acc: 0.1613\n",
      "Epoch 6/6\n",
      "27s - loss: 2.3859 - acc: 0.1590 - val_loss: 2.3856 - val_acc: 0.1594\n",
      "('Log_loss:', 2.3855785674011614, 'It took', -138.84530997276306)\n",
      "('Average time per eval:', -107.87663665994421)\n",
      "Params testing 77: {'n_epoch': 5.948874373048408, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 56.956202550764004, 'dropout2': 0.24335078733796472}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "11s - loss: 2.4156 - acc: 0.1419 - val_loss: 2.3940 - val_acc: 0.1489\n",
      "Epoch 2/5\n",
      "19s - loss: 2.4001 - acc: 0.1489 - val_loss: 2.3882 - val_acc: 0.1510\n",
      "Epoch 3/5\n",
      "23s - loss: 2.3933 - acc: 0.1523 - val_loss: 2.3850 - val_acc: 0.1554\n",
      "Epoch 4/5\n",
      "26s - loss: 2.3895 - acc: 0.1555 - val_loss: 2.3881 - val_acc: 0.1566\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3857 - acc: 0.1565 - val_loss: 2.3861 - val_acc: 0.1539\n",
      "('Log_loss:', 2.3860968955873072, 'It took', -114.48309588432312)\n",
      "('Average time per eval:', -107.96133484290196)\n",
      "Params testing 78: {'n_epoch': 6.384000990028398, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 133.47726922599932, 'dropout2': 0.29161657675332237}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4163 - acc: 0.1421 - val_loss: 2.3929 - val_acc: 0.1507\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4027 - acc: 0.1482 - val_loss: 2.3905 - val_acc: 0.1550\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3964 - acc: 0.1514 - val_loss: 2.3881 - val_acc: 0.1530\n",
      "Epoch 4/6\n",
      "25s - loss: 2.3923 - acc: 0.1533 - val_loss: 2.3866 - val_acc: 0.1508\n",
      "Epoch 5/6\n",
      "27s - loss: 2.3888 - acc: 0.1563 - val_loss: 2.3860 - val_acc: 0.1567\n",
      "Epoch 6/6\n",
      "28s - loss: 2.3849 - acc: 0.1594 - val_loss: 2.3877 - val_acc: 0.1551\n",
      "('Log_loss:', 2.3877121942981523, 'It took', -146.95005416870117)\n",
      "('Average time per eval:', -108.45486292054382)\n",
      "Params testing 79: {'n_epoch': 5.30127665276948, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 30.705452717505146, 'dropout2': 0.5608983213697604}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 2.4195 - acc: 0.1393 - val_loss: 2.3964 - val_acc: 0.1501\n",
      "Epoch 2/5\n",
      "20s - loss: 2.4050 - acc: 0.1485 - val_loss: 2.3900 - val_acc: 0.1511\n",
      "Epoch 3/5\n",
      "22s - loss: 2.3989 - acc: 0.1492 - val_loss: 2.3897 - val_acc: 0.1564\n",
      "Epoch 4/5\n",
      "25s - loss: 2.3956 - acc: 0.1514 - val_loss: 2.3870 - val_acc: 0.1535\n",
      "Epoch 5/5\n",
      "27s - loss: 2.3930 - acc: 0.1540 - val_loss: 2.3871 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3870831516738749, 'It took', -111.70743107795715)\n",
      "('Average time per eval:', -108.49551999568939)\n",
      "Params testing 80: {'n_epoch': 4.567799461458276, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 74.67774145391411, 'dropout2': 0.3526196428646914}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4167 - acc: 0.1423 - val_loss: 2.3952 - val_acc: 0.1534\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4029 - acc: 0.1493 - val_loss: 2.3927 - val_acc: 0.1544\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3982 - acc: 0.1523 - val_loss: 2.3898 - val_acc: 0.1543\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3949 - acc: 0.1540 - val_loss: 2.3901 - val_acc: 0.1564\n",
      "('Log_loss:', 2.3901304343958305, 'It took', -94.14212703704834)\n",
      "('Average time per eval:', -108.31831760171019)\n",
      "Params testing 81: {'n_epoch': 5.485168867685422, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 53.72135291761722, 'dropout2': 0.5954719564995231}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 2.4205 - acc: 0.1379 - val_loss: 2.3933 - val_acc: 0.1520\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4048 - acc: 0.1469 - val_loss: 2.3897 - val_acc: 0.1538\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4004 - acc: 0.1490 - val_loss: 2.3879 - val_acc: 0.1540\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3967 - acc: 0.1519 - val_loss: 2.3872 - val_acc: 0.1566\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3945 - acc: 0.1522 - val_loss: 2.3858 - val_acc: 0.1554\n",
      "('Log_loss:', 2.3857545111634728, 'It took', -119.0662910938263)\n",
      "('Average time per eval:', -108.44939043754485)\n",
      "Params testing 82: {'n_epoch': 3.12457452396348, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 86.11880634759416, 'dropout2': 0.4390060033032841}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4170 - acc: 0.1412 - val_loss: 2.3914 - val_acc: 0.1505\n",
      "Epoch 2/3\n",
      "19s - loss: 2.4030 - acc: 0.1461 - val_loss: 2.3883 - val_acc: 0.1564\n",
      "Epoch 3/3\n",
      "23s - loss: 2.3974 - acc: 0.1517 - val_loss: 2.3890 - val_acc: 0.1536\n",
      "('Log_loss:', 2.3889717633558796, 'It took', -60.523417949676514)\n",
      "('Average time per eval:', -107.8719690564167)\n",
      "Params testing 83: {'n_epoch': 4.921192018070649, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 43.268256217937875, 'dropout2': 0.3084430039771664}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "11s - loss: 2.4162 - acc: 0.1407 - val_loss: 2.3928 - val_acc: 0.1572\n",
      "Epoch 2/4\n",
      "19s - loss: 2.3996 - acc: 0.1497 - val_loss: 2.3877 - val_acc: 0.1568\n",
      "Epoch 3/4\n",
      "22s - loss: 2.3942 - acc: 0.1532 - val_loss: 2.3849 - val_acc: 0.1574\n",
      "Epoch 4/4\n",
      "24s - loss: 2.3900 - acc: 0.1559 - val_loss: 2.3847 - val_acc: 0.1576\n",
      "('Log_loss:', 2.3847400937104655, 'It took', -82.70043778419495)\n",
      "('Average time per eval:', -107.57230795848938)\n",
      "Params testing 84: {'n_epoch': 6.084449355801793, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 63.34616098161403, 'dropout2': 0.25496736157978017}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "10s - loss: 2.4154 - acc: 0.1416 - val_loss: 2.3948 - val_acc: 0.1481\n",
      "Epoch 2/6\n",
      "20s - loss: 2.4019 - acc: 0.1496 - val_loss: 2.3907 - val_acc: 0.1563\n",
      "Epoch 3/6\n",
      "23s - loss: 2.3959 - acc: 0.1520 - val_loss: 2.3897 - val_acc: 0.1576\n",
      "Epoch 4/6\n",
      "25s - loss: 2.3932 - acc: 0.1544 - val_loss: 2.3890 - val_acc: 0.1576\n",
      "Epoch 5/6\n",
      "26s - loss: 2.3894 - acc: 0.1573 - val_loss: 2.3901 - val_acc: 0.1570\n",
      "Epoch 6/6\n",
      "27s - loss: 2.3873 - acc: 0.1573 - val_loss: 2.3873 - val_acc: 0.1566\n",
      "('Log_loss:', 2.3873103429205833, 'It took', -139.74272203445435)\n",
      "('Average time per eval:', -107.95078340418199)\n",
      "Params testing 85: {'n_epoch': 3.42324426600313, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 182.856249935528, 'dropout2': 0.4658170734541201}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "11s - loss: 2.4200 - acc: 0.1400 - val_loss: 2.3950 - val_acc: 0.1514\n",
      "Epoch 2/3\n",
      "20s - loss: 2.4064 - acc: 0.1477 - val_loss: 2.3915 - val_acc: 0.1500\n",
      "Epoch 3/3\n",
      "23s - loss: 2.4002 - acc: 0.1493 - val_loss: 2.3895 - val_acc: 0.1526\n",
      "('Log_loss:', 2.3895348143775568, 'It took', -60.73050093650818)\n",
      "('Average time per eval:', -107.40171033005382)\n",
      "Params testing 86: {'n_epoch': 5.097854583992107, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 16.66064706159398, 'dropout2': 0.27943870848886737}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "11s - loss: 2.4167 - acc: 0.1400 - val_loss: 2.3936 - val_acc: 0.1445\n",
      "Epoch 2/5\n",
      "21s - loss: 2.3999 - acc: 0.1475 - val_loss: 2.3891 - val_acc: 0.1558\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3955 - acc: 0.1518 - val_loss: 2.3861 - val_acc: 0.1547\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3916 - acc: 0.1534 - val_loss: 2.3868 - val_acc: 0.1571\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3894 - acc: 0.1543 - val_loss: 2.3848 - val_acc: 0.1605\n",
      "('Log_loss:', 2.3847979474987384, 'It took', -120.27422904968262)\n",
      "('Average time per eval:', -107.54967030437513)\n",
      "Params testing 87: {'n_epoch': 5.748999593957829, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 22.838291138201093, 'dropout2': 0.7178805140188629}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4258 - acc: 0.1348 - val_loss: 2.3987 - val_acc: 0.1426\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4085 - acc: 0.1443 - val_loss: 2.3935 - val_acc: 0.1488\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4035 - acc: 0.1475 - val_loss: 2.3907 - val_acc: 0.1539\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4026 - acc: 0.1484 - val_loss: 2.3912 - val_acc: 0.1523\n",
      "Epoch 5/5\n",
      "28s - loss: 2.4000 - acc: 0.1477 - val_loss: 2.3908 - val_acc: 0.1572\n",
      "('Log_loss:', 2.3908353439611307, 'It took', -120.5327820777893)\n",
      "('Average time per eval:', -107.69720565459944)\n",
      "Params testing 88: {'n_epoch': 3.8375066325587954, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 31.434934046732554, 'dropout2': 0.34267829072547035}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4159 - acc: 0.1394 - val_loss: 2.3918 - val_acc: 0.1536\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4009 - acc: 0.1490 - val_loss: 2.3895 - val_acc: 0.1531\n",
      "Epoch 3/3\n",
      "26s - loss: 2.3949 - acc: 0.1517 - val_loss: 2.3886 - val_acc: 0.1522\n",
      "('Log_loss:', 2.3886325976147971, 'It took', -65.40738105773926)\n",
      "('Average time per eval:', -107.22203905127022)\n",
      "Params testing 89: {'n_epoch': 6.698390555294239, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 147.26535446129088, 'dropout2': 0.2015253337226831}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4145 - acc: 0.1415 - val_loss: 2.3962 - val_acc: 0.1562\n",
      "Epoch 2/6\n",
      "23s - loss: 2.4019 - acc: 0.1491 - val_loss: 2.3908 - val_acc: 0.1595\n",
      "Epoch 3/6\n",
      "26s - loss: 2.3959 - acc: 0.1544 - val_loss: 2.3909 - val_acc: 0.1591\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3928 - acc: 0.1549 - val_loss: 2.3880 - val_acc: 0.1591\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3885 - acc: 0.1576 - val_loss: 2.3873 - val_acc: 0.1571\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3853 - acc: 0.1614 - val_loss: 2.3907 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3906825615770209, 'It took', -156.53684496879578)\n",
      "('Average time per eval:', -107.76998132864634)\n",
      "Params testing 90: {'n_epoch': 4.7311053312327624, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 38.080139931051626, 'dropout2': 0.23114635241503814}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4141 - acc: 0.1422 - val_loss: 2.3905 - val_acc: 0.1530\n",
      "Epoch 2/4\n",
      "21s - loss: 2.3992 - acc: 0.1517 - val_loss: 2.3878 - val_acc: 0.1530\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3930 - acc: 0.1524 - val_loss: 2.3858 - val_acc: 0.1602\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3887 - acc: 0.1564 - val_loss: 2.3836 - val_acc: 0.1610\n",
      "('Log_loss:', 2.383624030107983, 'It took', -91.84876298904419)\n",
      "('Average time per eval:', -107.59502287487408)\n",
      "Params testing 91: {'n_epoch': 4.717011767158845, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 97.92885403669177, 'dropout2': 0.23199589226037182}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4148 - acc: 0.1427 - val_loss: 2.3911 - val_acc: 0.1489\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4010 - acc: 0.1516 - val_loss: 2.3886 - val_acc: 0.1532\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3953 - acc: 0.1522 - val_loss: 2.3876 - val_acc: 0.1564\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3915 - acc: 0.1542 - val_loss: 2.3856 - val_acc: 0.1543\n",
      "('Log_loss:', 2.3856316787481884, 'It took', -94.03068900108337)\n",
      "('Average time per eval:', -107.44758445283641)\n",
      "Params testing 92: {'n_epoch': 4.350325090290061, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 91.11657175625587, 'dropout2': 0.21018619874320743}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4143 - acc: 0.1426 - val_loss: 2.3910 - val_acc: 0.1455\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4005 - acc: 0.1494 - val_loss: 2.3889 - val_acc: 0.1601\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3939 - acc: 0.1545 - val_loss: 2.3863 - val_acc: 0.1558\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3895 - acc: 0.1580 - val_loss: 2.3872 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3872303627082117, 'It took', -96.77868819236755)\n",
      "('Average time per eval:', -107.3328651253895)\n",
      "Params testing 93: {'n_epoch': 4.92158824162344, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 248.69561208726037, 'dropout2': 0.8294322229787083}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "14s - loss: 2.4343 - acc: 0.1324 - val_loss: 2.3969 - val_acc: 0.1448\n",
      "Epoch 2/4\n",
      "23s - loss: 2.4203 - acc: 0.1418 - val_loss: 2.3934 - val_acc: 0.1499\n",
      "Epoch 3/4\n",
      "26s - loss: 2.4174 - acc: 0.1427 - val_loss: 2.3911 - val_acc: 0.1505\n",
      "Epoch 4/4\n",
      "29s - loss: 2.4152 - acc: 0.1438 - val_loss: 2.3909 - val_acc: 0.1530\n",
      "('Log_loss:', 2.390945292464409, 'It took', -98.28659296035767)\n",
      "('Average time per eval:', -107.23662816463633)\n",
      "Params testing 94: {'n_epoch': 4.2271034477880685, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 69.13493668601127, 'dropout2': 0.31741837197219847}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4159 - acc: 0.1416 - val_loss: 2.3932 - val_acc: 0.1542\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4006 - acc: 0.1496 - val_loss: 2.3926 - val_acc: 0.1560\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3967 - acc: 0.1523 - val_loss: 2.3909 - val_acc: 0.1562\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3932 - acc: 0.1529 - val_loss: 2.3875 - val_acc: 0.1599\n",
      "('Log_loss:', 2.3875013446954756, 'It took', -95.4270749092102)\n",
      "('Average time per eval:', -107.11231706769843)\n",
      "Params testing 95: {'n_epoch': 4.0345581507269825, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 79.99195315418962, 'dropout2': 0.2507606254257738}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4140 - acc: 0.1441 - val_loss: 2.3934 - val_acc: 0.1452\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4000 - acc: 0.1491 - val_loss: 2.3878 - val_acc: 0.1570\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3954 - acc: 0.1547 - val_loss: 2.3869 - val_acc: 0.1566\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3903 - acc: 0.1571 - val_loss: 2.3857 - val_acc: 0.1556\n",
      "('Log_loss:', 2.3856664706643262, 'It took', -93.9412591457367)\n",
      "('Average time per eval:', -106.975118547678)\n",
      "Params testing 96: {'n_epoch': 3.7612235499957323, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 48.43824069783821, 'dropout2': 0.22579287925544947}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4142 - acc: 0.1418 - val_loss: 2.3915 - val_acc: 0.1481\n",
      "Epoch 2/3\n",
      "22s - loss: 2.3990 - acc: 0.1496 - val_loss: 2.3885 - val_acc: 0.1559\n",
      "Epoch 3/3\n",
      "26s - loss: 2.3936 - acc: 0.1524 - val_loss: 2.3855 - val_acc: 0.1567\n",
      "('Log_loss:', 2.385482680740096, 'It took', -66.2097179889679)\n",
      "('Average time per eval:', -106.55485667641629)\n",
      "Params testing 97: {'n_epoch': 3.588978391222871, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 228.68066528114917, 'dropout2': 0.39378900671475325}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "14s - loss: 2.4204 - acc: 0.1380 - val_loss: 2.3937 - val_acc: 0.1550\n",
      "Epoch 2/3\n",
      "23s - loss: 2.4061 - acc: 0.1461 - val_loss: 2.3907 - val_acc: 0.1532\n",
      "Epoch 3/3\n",
      "26s - loss: 2.4001 - acc: 0.1493 - val_loss: 2.3901 - val_acc: 0.1526\n",
      "('Log_loss:', 2.3901221336706566, 'It took', -69.06192302703857)\n",
      "('Average time per eval:', -106.17227571107904)\n",
      "Params testing 98: {'n_epoch': 5.390832105063427, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 117.1486385668276, 'dropout2': 0.6136711068446467}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4217 - acc: 0.1380 - val_loss: 2.4015 - val_acc: 0.1491\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4084 - acc: 0.1460 - val_loss: 2.3969 - val_acc: 0.1507\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4037 - acc: 0.1483 - val_loss: 2.3975 - val_acc: 0.1567\n",
      "Epoch 4/5\n",
      "28s - loss: 2.4013 - acc: 0.1505 - val_loss: 2.3947 - val_acc: 0.1532\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4002 - acc: 0.1499 - val_loss: 2.3964 - val_acc: 0.1563\n",
      "('Log_loss:', 2.3964159131976195, 'It took', -125.84014296531677)\n",
      "('Average time per eval:', -106.37094102724635)\n",
      "Params testing 99: {'n_epoch': 5.563646311004042, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 39.026686918346975, 'dropout2': 0.8991239006345384}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4342 - acc: 0.1312 - val_loss: 2.4075 - val_acc: 0.1422\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4182 - acc: 0.1409 - val_loss: 2.4021 - val_acc: 0.1439\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4142 - acc: 0.1410 - val_loss: 2.3991 - val_acc: 0.1497\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4139 - acc: 0.1401 - val_loss: 2.3989 - val_acc: 0.1500\n",
      "Epoch 5/5\n",
      "28s - loss: 2.4121 - acc: 0.1430 - val_loss: 2.3965 - val_acc: 0.1485\n",
      "('Log_loss:', 2.3964522692207479, 'It took', -121.74834489822388)\n",
      "('Average time per eval:', -106.52471505641937)\n",
      "Params testing 100: {'n_epoch': 4.447551657381469, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 108.77966366282132, 'dropout2': 0.2981150630391312}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4163 - acc: 0.1429 - val_loss: 2.3940 - val_acc: 0.1505\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4020 - acc: 0.1481 - val_loss: 2.3922 - val_acc: 0.1487\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3956 - acc: 0.1512 - val_loss: 2.3871 - val_acc: 0.1524\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3917 - acc: 0.1545 - val_loss: 2.3858 - val_acc: 0.1572\n",
      "('Log_loss:', 2.3857544243192019, 'It took', -96.39753794670105)\n",
      "('Average time per eval:', -106.42444596668281)\n",
      "Params testing 101: {'n_epoch': 3.2450451458881795, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 200.003736486653, 'dropout2': 0.26928718354219255}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4176 - acc: 0.1408 - val_loss: 2.3923 - val_acc: 0.1487\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4042 - acc: 0.1485 - val_loss: 2.3885 - val_acc: 0.1572\n",
      "Epoch 3/3\n",
      "26s - loss: 2.3996 - acc: 0.1506 - val_loss: 2.3873 - val_acc: 0.1542\n",
      "('Log_loss:', 2.3872543718998149, 'It took', -67.89170908927917)\n",
      "('Average time per eval:', -106.04667402716244)\n",
      "Params testing 102: {'n_epoch': 4.57997534158653, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 27.17424305283631, 'dropout2': 0.4128063318287756}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4175 - acc: 0.1399 - val_loss: 2.3925 - val_acc: 0.1503\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4013 - acc: 0.1504 - val_loss: 2.3890 - val_acc: 0.1532\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3956 - acc: 0.1496 - val_loss: 2.3849 - val_acc: 0.1554\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3933 - acc: 0.1523 - val_loss: 2.3866 - val_acc: 0.1599\n",
      "('Log_loss:', 2.3865718018136839, 'It took', -91.14896988868713)\n",
      "('Average time per eval:', -105.90203611364642)\n",
      "Params testing 103: {'n_epoch': 4.093342135466444, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 60.086092026869224, 'dropout2': 0.33369739098877826}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4166 - acc: 0.1399 - val_loss: 2.3932 - val_acc: 0.1452\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4027 - acc: 0.1484 - val_loss: 2.3887 - val_acc: 0.1618\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3986 - acc: 0.1516 - val_loss: 2.3899 - val_acc: 0.1546\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3937 - acc: 0.1514 - val_loss: 2.3912 - val_acc: 0.1547\n",
      "('Log_loss:', 2.3911780645372782, 'It took', -92.54290819168091)\n",
      "('Average time per eval:', -105.77358294908817)\n",
      "Params testing 104: {'n_epoch': 5.093718508068903, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 103.40591627064, 'dropout2': 0.5299830061458012}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4185 - acc: 0.1383 - val_loss: 2.3957 - val_acc: 0.1500\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4054 - acc: 0.1454 - val_loss: 2.3890 - val_acc: 0.1520\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3993 - acc: 0.1497 - val_loss: 2.3863 - val_acc: 0.1535\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3961 - acc: 0.1506 - val_loss: 2.3847 - val_acc: 0.1556\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3934 - acc: 0.1550 - val_loss: 2.3855 - val_acc: 0.1586\n",
      "('Log_loss:', 2.3855332580514546, 'It took', -122.05308699607849)\n",
      "('Average time per eval:', -105.92862583569118)\n",
      "Params testing 105: {'n_epoch': 4.811804716702233, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 216.6495603554114, 'dropout2': 0.2839497351792533}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4179 - acc: 0.1415 - val_loss: 2.3944 - val_acc: 0.1428\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4054 - acc: 0.1483 - val_loss: 2.3905 - val_acc: 0.1522\n",
      "Epoch 3/4\n",
      "26s - loss: 2.3993 - acc: 0.1507 - val_loss: 2.3892 - val_acc: 0.1576\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3952 - acc: 0.1554 - val_loss: 2.3881 - val_acc: 0.1507\n",
      "('Log_loss:', 2.3881053116792268, 'It took', -95.87806105613708)\n",
      "('Average time per eval:', -105.83380918682747)\n",
      "Params testing 106: {'n_epoch': 5.22019035994952, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 129.74368979717565, 'dropout2': 0.23609568622724597}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4150 - acc: 0.1400 - val_loss: 2.3926 - val_acc: 0.1480\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4025 - acc: 0.1488 - val_loss: 2.3880 - val_acc: 0.1580\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3963 - acc: 0.1518 - val_loss: 2.3871 - val_acc: 0.1536\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3913 - acc: 0.1547 - val_loss: 2.3880 - val_acc: 0.1536\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3893 - acc: 0.1561 - val_loss: 2.3875 - val_acc: 0.1589\n",
      "('Log_loss:', 2.3874736602939297, 'It took', -123.57307314872742)\n",
      "('Average time per eval:', -105.99959667152334)\n",
      "Params testing 107: {'n_epoch': 3.478131357659702, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 51.95227279513087, 'dropout2': 0.20006747382852927}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4143 - acc: 0.1412 - val_loss: 2.3932 - val_acc: 0.1488\n",
      "Epoch 2/3\n",
      "21s - loss: 2.3995 - acc: 0.1506 - val_loss: 2.3887 - val_acc: 0.1582\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3933 - acc: 0.1538 - val_loss: 2.3865 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3864831975213394, 'It took', -64.56177997589111)\n",
      "('Average time per eval:', -105.61591317256291)\n",
      "Params testing 108: {'n_epoch': 5.287383759038576, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 164.2964285921205, 'dropout2': 0.3052934033856055}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4180 - acc: 0.1408 - val_loss: 2.3936 - val_acc: 0.1539\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4038 - acc: 0.1481 - val_loss: 2.3879 - val_acc: 0.1548\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3966 - acc: 0.1532 - val_loss: 2.3884 - val_acc: 0.1491\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3946 - acc: 0.1530 - val_loss: 2.3884 - val_acc: 0.1493\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3904 - acc: 0.1546 - val_loss: 2.3888 - val_acc: 0.1532\n",
      "('Log_loss:', 2.3888087725706404, 'It took', -123.96646189689636)\n",
      "('Average time per eval:', -105.78426681308571)\n",
      "Params testing 109: {'n_epoch': 3.6965192911571925, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 144.49357405859354, 'dropout2': 0.4277705965534651}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4173 - acc: 0.1407 - val_loss: 2.3984 - val_acc: 0.1550\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4050 - acc: 0.1486 - val_loss: 2.3964 - val_acc: 0.1550\n",
      "Epoch 3/3\n",
      "25s - loss: 2.4006 - acc: 0.1497 - val_loss: 2.3932 - val_acc: 0.1552\n",
      "('Log_loss:', 2.3931868724263516, 'It took', -67.2409520149231)\n",
      "('Average time per eval:', -105.43387303352355)\n",
      "Params testing 110: {'n_epoch': 3.9963640682678614, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 22.166262342461245, 'dropout2': 0.5016830190078033}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4201 - acc: 0.1372 - val_loss: 2.3955 - val_acc: 0.1463\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4030 - acc: 0.1501 - val_loss: 2.3904 - val_acc: 0.1520\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3990 - acc: 0.1508 - val_loss: 2.3893 - val_acc: 0.1518\n",
      "('Log_loss:', 2.3893283342759473, 'It took', -64.13810396194458)\n",
      "('Average time per eval:', -105.0618390693321)\n",
      "Params testing 111: {'n_epoch': 4.7052854562714, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 35.3091203046647, 'dropout2': 0.3542896820143153}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4151 - acc: 0.1421 - val_loss: 2.3939 - val_acc: 0.1483\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4010 - acc: 0.1493 - val_loss: 2.3877 - val_acc: 0.1587\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3951 - acc: 0.1535 - val_loss: 2.3851 - val_acc: 0.1583\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3918 - acc: 0.1542 - val_loss: 2.3849 - val_acc: 0.1607\n",
      "('Log_loss:', 2.3848586147603674, 'It took', -91.87179398536682)\n",
      "('Average time per eval:', -104.94407079262393)\n",
      "Params testing 112: {'n_epoch': 4.272212272478007, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 72.45198774886381, 'dropout2': 0.21614082973298}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4143 - acc: 0.1419 - val_loss: 2.3916 - val_acc: 0.1507\n",
      "Epoch 2/4\n",
      "21s - loss: 2.3989 - acc: 0.1516 - val_loss: 2.3886 - val_acc: 0.1560\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3943 - acc: 0.1542 - val_loss: 2.3864 - val_acc: 0.1576\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3892 - acc: 0.1562 - val_loss: 2.3857 - val_acc: 0.1572\n",
      "('Log_loss:', 2.3857289069049519, 'It took', -92.9338710308075)\n",
      "('Average time per eval:', -104.83778583053994)\n",
      "Params testing 113: {'n_epoch': 6.296399861173292, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 66.95517815128765, 'dropout2': 0.45927942175105124}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4172 - acc: 0.1438 - val_loss: 2.3937 - val_acc: 0.1504\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4021 - acc: 0.1489 - val_loss: 2.3869 - val_acc: 0.1621\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3973 - acc: 0.1510 - val_loss: 2.3856 - val_acc: 0.1626\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3937 - acc: 0.1541 - val_loss: 2.3855 - val_acc: 0.1586\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3904 - acc: 0.1546 - val_loss: 2.3858 - val_acc: 0.1597\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3892 - acc: 0.1556 - val_loss: 2.3836 - val_acc: 0.1566\n",
      "('Log_loss:', 2.3836350702038724, 'It took', -151.27041101455688)\n",
      "('Average time per eval:', -105.24508955185874)\n",
      "Params testing 114: {'n_epoch': 6.333465901777485, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 86.19219041932209, 'dropout2': 0.791302372769989}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4280 - acc: 0.1359 - val_loss: 2.4064 - val_acc: 0.1463\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4152 - acc: 0.1408 - val_loss: 2.4049 - val_acc: 0.1468\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4110 - acc: 0.1436 - val_loss: 2.4020 - val_acc: 0.1405\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4097 - acc: 0.1435 - val_loss: 2.4011 - val_acc: 0.1485\n",
      "Epoch 5/6\n",
      "28s - loss: 2.4093 - acc: 0.1442 - val_loss: 2.3998 - val_acc: 0.1488\n",
      "Epoch 6/6\n",
      "29s - loss: 2.4084 - acc: 0.1461 - val_loss: 2.4002 - val_acc: 0.1477\n",
      "('Log_loss:', 2.4001952629928578, 'It took', -153.31950402259827)\n",
      "('Average time per eval:', -105.6631279281948)\n",
      "Params testing 115: {'n_epoch': 5.983357979724018, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 66.1139638466545, 'dropout2': 0.6664389716001177}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4213 - acc: 0.1392 - val_loss: 2.3957 - val_acc: 0.1515\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4073 - acc: 0.1451 - val_loss: 2.3905 - val_acc: 0.1551\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4008 - acc: 0.1481 - val_loss: 2.3878 - val_acc: 0.1567\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3981 - acc: 0.1522 - val_loss: 2.3861 - val_acc: 0.1582\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3964 - acc: 0.1513 - val_loss: 2.3861 - val_acc: 0.1556\n",
      "('Log_loss:', 2.3860668166693495, 'It took', -121.62538123130798)\n",
      "('Average time per eval:', -105.80073354984152)\n",
      "Params testing 116: {'n_epoch': 6.242325570628079, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 159.30497463666475, 'dropout2': 0.7021397872351502}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4242 - acc: 0.1390 - val_loss: 2.3928 - val_acc: 0.1523\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4102 - acc: 0.1453 - val_loss: 2.3886 - val_acc: 0.1576\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4055 - acc: 0.1487 - val_loss: 2.3872 - val_acc: 0.1560\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4042 - acc: 0.1488 - val_loss: 2.3860 - val_acc: 0.1601\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3991 - acc: 0.1519 - val_loss: 2.3855 - val_acc: 0.1594\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3993 - acc: 0.1519 - val_loss: 2.3849 - val_acc: 0.1606\n",
      "('Log_loss:', 2.3848751657391691, 'It took', -154.18028283119202)\n",
      "('Average time per eval:', -106.21423396289858)\n",
      "Params testing 117: {'n_epoch': 6.91506134430519, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 176.56309024005014, 'dropout2': 0.585903429662022}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4211 - acc: 0.1395 - val_loss: 2.3928 - val_acc: 0.1539\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4075 - acc: 0.1476 - val_loss: 2.3894 - val_acc: 0.1544\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4038 - acc: 0.1504 - val_loss: 2.3887 - val_acc: 0.1579\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3989 - acc: 0.1494 - val_loss: 2.3869 - val_acc: 0.1531\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3971 - acc: 0.1540 - val_loss: 2.3850 - val_acc: 0.1550\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3960 - acc: 0.1552 - val_loss: 2.3877 - val_acc: 0.1571\n",
      "('Log_loss:', 2.387676823739548, 'It took', -155.94951915740967)\n",
      "('Average time per eval:', -106.63571941246421)\n",
      "Params testing 118: {'n_epoch': 4.934068903810083, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 78.28660046760058, 'dropout2': 0.7591719839411392}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4247 - acc: 0.1365 - val_loss: 2.3962 - val_acc: 0.1425\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4108 - acc: 0.1438 - val_loss: 2.3945 - val_acc: 0.1516\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4054 - acc: 0.1466 - val_loss: 2.3899 - val_acc: 0.1542\n",
      "Epoch 4/4\n",
      "27s - loss: 2.4025 - acc: 0.1475 - val_loss: 2.3889 - val_acc: 0.1504\n",
      "('Log_loss:', 2.3888792288613132, 'It took', -93.22764205932617)\n",
      "('Average time per eval:', -106.52304647750213)\n",
      "Params testing 119: {'n_epoch': 5.7596263891972, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 44.9303563606094, 'dropout2': 0.632142221646902}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4208 - acc: 0.1392 - val_loss: 2.3936 - val_acc: 0.1495\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4056 - acc: 0.1470 - val_loss: 2.3902 - val_acc: 0.1568\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3997 - acc: 0.1504 - val_loss: 2.3883 - val_acc: 0.1552\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3978 - acc: 0.1504 - val_loss: 2.3861 - val_acc: 0.1597\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3954 - acc: 0.1505 - val_loss: 2.3868 - val_acc: 0.1556\n",
      "('Log_loss:', 2.3867909920429562, 'It took', -120.13539600372314)\n",
      "('Average time per eval:', -106.63648271560669)\n",
      "Params testing 120: {'n_epoch': 6.454914051843367, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 20.14254030056573, 'dropout2': 0.5545713557652919}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4233 - acc: 0.1384 - val_loss: 2.3997 - val_acc: 0.1452\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4089 - acc: 0.1437 - val_loss: 2.3972 - val_acc: 0.1435\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4049 - acc: 0.1444 - val_loss: 2.3938 - val_acc: 0.1449\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4021 - acc: 0.1480 - val_loss: 2.3923 - val_acc: 0.1515\n",
      "Epoch 5/6\n",
      "28s - loss: 2.4009 - acc: 0.1471 - val_loss: 2.3943 - val_acc: 0.1495\n",
      "Epoch 6/6\n",
      "29s - loss: 2.4003 - acc: 0.1489 - val_loss: 2.3940 - val_acc: 0.1492\n",
      "('Log_loss:', 2.3939805431593335, 'It took', -150.7922170162201)\n",
      "('Average time per eval:', -107.0014061316971)\n",
      "Params testing 121: {'n_epoch': 5.904429939146219, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 61.34677591712726, 'dropout2': 0.45859662793124095}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4165 - acc: 0.1414 - val_loss: 2.3926 - val_acc: 0.1524\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4016 - acc: 0.1486 - val_loss: 2.3888 - val_acc: 0.1501\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3975 - acc: 0.1519 - val_loss: 2.3865 - val_acc: 0.1625\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3931 - acc: 0.1551 - val_loss: 2.3869 - val_acc: 0.1546\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3918 - acc: 0.1543 - val_loss: 2.3887 - val_acc: 0.1551\n",
      "('Log_loss:', 2.388720750425509, 'It took', -121.04930210113525)\n",
      "('Average time per eval:', -107.11655281215418)\n",
      "Params testing 122: {'n_epoch': 6.658809761310827, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 93.31841339125252, 'dropout2': 0.48420081479005717}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4176 - acc: 0.1411 - val_loss: 2.3926 - val_acc: 0.1516\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4034 - acc: 0.1490 - val_loss: 2.3885 - val_acc: 0.1598\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3967 - acc: 0.1522 - val_loss: 2.3872 - val_acc: 0.1551\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3946 - acc: 0.1524 - val_loss: 2.3855 - val_acc: 0.1572\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3914 - acc: 0.1552 - val_loss: 2.3847 - val_acc: 0.1578\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3892 - acc: 0.1565 - val_loss: 2.3860 - val_acc: 0.1564\n",
      "('Log_loss:', 2.3860109944845567, 'It took', -150.783362865448)\n",
      "('Average time per eval:', -107.47156752028117)\n",
      "Params testing 123: {'n_epoch': 5.560118518799731, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 121.68332571879854, 'dropout2': 0.3811728637669958}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4167 - acc: 0.1412 - val_loss: 2.3942 - val_acc: 0.1481\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4027 - acc: 0.1481 - val_loss: 2.3894 - val_acc: 0.1535\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3979 - acc: 0.1524 - val_loss: 2.3890 - val_acc: 0.1556\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3940 - acc: 0.1522 - val_loss: 2.3862 - val_acc: 0.1560\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3896 - acc: 0.1566 - val_loss: 2.3847 - val_acc: 0.1599\n",
      "('Log_loss:', 2.3846689580018867, 'It took', -122.26521897315979)\n",
      "('Average time per eval:', -107.59087115333926)\n",
      "Params testing 124: {'n_epoch': 6.099391350071076, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 16.163879987153173, 'dropout2': 0.8742860905567207}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4332 - acc: 0.1297 - val_loss: 2.4098 - val_acc: 0.1416\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4181 - acc: 0.1378 - val_loss: 2.4043 - val_acc: 0.1463\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4161 - acc: 0.1401 - val_loss: 2.4046 - val_acc: 0.1425\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4139 - acc: 0.1407 - val_loss: 2.4043 - val_acc: 0.1437\n",
      "Epoch 5/6\n",
      "28s - loss: 2.4136 - acc: 0.1400 - val_loss: 2.4017 - val_acc: 0.1503\n",
      "Epoch 6/6\n",
      "29s - loss: 2.4120 - acc: 0.1431 - val_loss: 2.4019 - val_acc: 0.1451\n",
      "('Log_loss:', 2.4019160678806299, 'It took', -149.49536991119385)\n",
      "('Average time per eval:', -107.92610713577271)\n",
      "Params testing 125: {'n_epoch': 4.641337717042699, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 27.640973181732427, 'dropout2': 0.5155760028862546}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4213 - acc: 0.1368 - val_loss: 2.3972 - val_acc: 0.1493\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4077 - acc: 0.1448 - val_loss: 2.3936 - val_acc: 0.1511\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4016 - acc: 0.1488 - val_loss: 2.3930 - val_acc: 0.1492\n",
      "Epoch 4/4\n",
      "27s - loss: 2.4000 - acc: 0.1491 - val_loss: 2.3906 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3905974508225998, 'It took', -92.33195805549622)\n",
      "('Average time per eval:', -107.80234403837295)\n",
      "Params testing 126: {'n_epoch': 5.658461445223719, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 41.712434347801945, 'dropout2': 0.43900363199453857}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4175 - acc: 0.1393 - val_loss: 2.3937 - val_acc: 0.1508\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4019 - acc: 0.1478 - val_loss: 2.3886 - val_acc: 0.1560\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3961 - acc: 0.1524 - val_loss: 2.3881 - val_acc: 0.1601\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3926 - acc: 0.1509 - val_loss: 2.3862 - val_acc: 0.1593\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3913 - acc: 0.1545 - val_loss: 2.3855 - val_acc: 0.1550\n",
      "('Log_loss:', 2.3855171964917719, 'It took', -121.27720808982849)\n",
      "('Average time per eval:', -107.9084453150982)\n",
      "Params testing 127: {'n_epoch': 5.136161127954811, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 137.02218764434858, 'dropout2': 0.6564084005694458}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4221 - acc: 0.1366 - val_loss: 2.3934 - val_acc: 0.1507\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4084 - acc: 0.1457 - val_loss: 2.3909 - val_acc: 0.1507\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4025 - acc: 0.1491 - val_loss: 2.3890 - val_acc: 0.1546\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4013 - acc: 0.1515 - val_loss: 2.3900 - val_acc: 0.1508\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3979 - acc: 0.1509 - val_loss: 2.3875 - val_acc: 0.1548\n",
      "('Log_loss:', 2.3874544511204139, 'It took', -122.8091070652008)\n",
      "('Average time per eval:', -108.02485672570765)\n",
      "Params testing 128: {'n_epoch': 5.0249994215828595, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 56.60342789907641, 'dropout2': 0.2570946697911357}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4147 - acc: 0.1436 - val_loss: 2.3933 - val_acc: 0.1479\n",
      "Epoch 2/5\n",
      "21s - loss: 2.3994 - acc: 0.1516 - val_loss: 2.3888 - val_acc: 0.1543\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3942 - acc: 0.1531 - val_loss: 2.3877 - val_acc: 0.1571\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3901 - acc: 0.1553 - val_loss: 2.3855 - val_acc: 0.1572\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3872 - acc: 0.1580 - val_loss: 2.3858 - val_acc: 0.1572\n",
      "('Log_loss:', 2.3857545943513108, 'It took', -120.49765205383301)\n",
      "('Average time per eval:', -108.12154504495074)\n",
      "Params testing 129: {'n_epoch': 5.414428300684558, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 97.5749021544781, 'dropout2': 0.7866254096474845}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4269 - acc: 0.1372 - val_loss: 2.3965 - val_acc: 0.1463\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4118 - acc: 0.1455 - val_loss: 2.3944 - val_acc: 0.1484\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4070 - acc: 0.1450 - val_loss: 2.3904 - val_acc: 0.1518\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4054 - acc: 0.1464 - val_loss: 2.3892 - val_acc: 0.1554\n",
      "Epoch 5/5\n",
      "28s - loss: 2.4028 - acc: 0.1491 - val_loss: 2.3893 - val_acc: 0.1554\n",
      "('Log_loss:', 2.3892869312259295, 'It took', -121.25273394584656)\n",
      "('Average time per eval:', -108.22255418300628)\n",
      "Params testing 130: {'n_epoch': 6.2507018678493775, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 191.08198876435307, 'dropout2': 0.7417441463407164}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4256 - acc: 0.1347 - val_loss: 2.4079 - val_acc: 0.1516\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4152 - acc: 0.1411 - val_loss: 2.4058 - val_acc: 0.1489\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4115 - acc: 0.1422 - val_loss: 2.4041 - val_acc: 0.1530\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4077 - acc: 0.1465 - val_loss: 2.4035 - val_acc: 0.1480\n",
      "Epoch 5/6\n",
      "29s - loss: 2.4070 - acc: 0.1460 - val_loss: 2.4056 - val_acc: 0.1510\n",
      "Epoch 6/6\n",
      "30s - loss: 2.4070 - acc: 0.1468 - val_loss: 2.4040 - val_acc: 0.1507\n",
      "('Log_loss:', 2.4039639058746669, 'It took', -156.0544011592865)\n",
      "('Average time per eval:', -108.5876827785987)\n",
      "Params testing 131: {'n_epoch': 4.542400316723239, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 35.75497239818536, 'dropout2': 0.4015976765580948}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4158 - acc: 0.1401 - val_loss: 2.3941 - val_acc: 0.1465\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4016 - acc: 0.1479 - val_loss: 2.3895 - val_acc: 0.1492\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3963 - acc: 0.1519 - val_loss: 2.3875 - val_acc: 0.1556\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3921 - acc: 0.1534 - val_loss: 2.3840 - val_acc: 0.1567\n",
      "('Log_loss:', 2.3840390594849277, 'It took', -91.59755992889404)\n",
      "('Average time per eval:', -108.45896971948218)\n",
      "Params testing 132: {'n_epoch': 5.831754220884589, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 116.28261314870295, 'dropout2': 0.5694552379799044}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4196 - acc: 0.1390 - val_loss: 2.3959 - val_acc: 0.1542\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4051 - acc: 0.1468 - val_loss: 2.3893 - val_acc: 0.1531\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4000 - acc: 0.1494 - val_loss: 2.3880 - val_acc: 0.1539\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3981 - acc: 0.1515 - val_loss: 2.3885 - val_acc: 0.1544\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3945 - acc: 0.1538 - val_loss: 2.3863 - val_acc: 0.1571\n",
      "('Log_loss:', 2.386258952537172, 'It took', -122.3897111415863)\n",
      "('Average time per eval:', -108.56371212901925)\n",
      "Params testing 133: {'n_epoch': 4.789821655827403, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 47.605904450210566, 'dropout2': 0.32571962784732217}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4154 - acc: 0.1401 - val_loss: 2.3922 - val_acc: 0.1481\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4007 - acc: 0.1491 - val_loss: 2.3896 - val_acc: 0.1468\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3949 - acc: 0.1520 - val_loss: 2.3853 - val_acc: 0.1574\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3919 - acc: 0.1544 - val_loss: 2.3846 - val_acc: 0.1571\n",
      "('Log_loss:', 2.3846413603168757, 'It took', -91.93031907081604)\n",
      "('Average time per eval:', -108.43958232296048)\n",
      "Params testing 134: {'n_epoch': 6.880921149245501, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 107.9451909205881, 'dropout2': 0.5363234617189452}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4184 - acc: 0.1376 - val_loss: 2.3920 - val_acc: 0.1536\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4035 - acc: 0.1484 - val_loss: 2.3895 - val_acc: 0.1582\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3996 - acc: 0.1501 - val_loss: 2.3873 - val_acc: 0.1568\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3966 - acc: 0.1530 - val_loss: 2.3864 - val_acc: 0.1582\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3928 - acc: 0.1535 - val_loss: 2.3846 - val_acc: 0.1613\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3912 - acc: 0.1545 - val_loss: 2.3849 - val_acc: 0.1554\n",
      "('Log_loss:', 2.3848769469807878, 'It took', -151.8585798740387)\n",
      "('Average time per eval:', -108.76120451997828)\n",
      "Params testing 135: {'n_epoch': 4.415186461820321, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 81.45539297449739, 'dropout2': 0.8205705154273806}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4301 - acc: 0.1331 - val_loss: 2.4072 - val_acc: 0.1464\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4174 - acc: 0.1386 - val_loss: 2.4063 - val_acc: 0.1443\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4134 - acc: 0.1405 - val_loss: 2.4037 - val_acc: 0.1461\n",
      "Epoch 4/4\n",
      "27s - loss: 2.4129 - acc: 0.1418 - val_loss: 2.4034 - val_acc: 0.1463\n",
      "('Log_loss:', 2.4034240237196935, 'It took', -93.73256182670593)\n",
      "('Average time per eval:', -108.65069979429245)\n",
      "Params testing 136: {'n_epoch': 6.992895956079934, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 127.81823415920485, 'dropout2': 0.37002426929525556}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4168 - acc: 0.1413 - val_loss: 2.3921 - val_acc: 0.1574\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4029 - acc: 0.1471 - val_loss: 2.3883 - val_acc: 0.1532\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3979 - acc: 0.1532 - val_loss: 2.3880 - val_acc: 0.1579\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3943 - acc: 0.1535 - val_loss: 2.3893 - val_acc: 0.1501\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3900 - acc: 0.1561 - val_loss: 2.3868 - val_acc: 0.1526\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3879 - acc: 0.1572 - val_loss: 2.3863 - val_acc: 0.1508\n",
      "('Log_loss:', 2.3862749079485344, 'It took', -152.30778813362122)\n",
      "('Average time per eval:', -108.9693646639803)\n",
      "Params testing 137: {'n_epoch': 5.462977304214742, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 74.60909784123467, 'dropout2': 0.6872118259846175}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4220 - acc: 0.1376 - val_loss: 2.3963 - val_acc: 0.1459\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4074 - acc: 0.1461 - val_loss: 2.3908 - val_acc: 0.1542\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4022 - acc: 0.1470 - val_loss: 2.3882 - val_acc: 0.1544\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3991 - acc: 0.1509 - val_loss: 2.3890 - val_acc: 0.1593\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3969 - acc: 0.1513 - val_loss: 2.3871 - val_acc: 0.1606\n",
      "('Log_loss:', 2.387052456191932, 'It took', -120.87354898452759)\n",
      "('Average time per eval:', -109.05562686229098)\n",
      "Params testing 138: {'n_epoch': 5.310049791179713, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 88.34124425175659, 'dropout2': 0.27081648635693145}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4145 - acc: 0.1426 - val_loss: 2.3911 - val_acc: 0.1467\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4010 - acc: 0.1486 - val_loss: 2.3893 - val_acc: 0.1547\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3957 - acc: 0.1525 - val_loss: 2.3881 - val_acc: 0.1546\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3906 - acc: 0.1562 - val_loss: 2.3895 - val_acc: 0.1587\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3875 - acc: 0.1575 - val_loss: 2.3880 - val_acc: 0.1582\n",
      "('Log_loss:', 2.3879765704206317, 'It took', -121.5812451839447)\n",
      "('Average time per eval:', -109.14573921574106)\n",
      "Params testing 139: {'n_epoch': 4.133645330156336, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 66.3415199388639, 'dropout2': 0.46927340595235023}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4167 - acc: 0.1400 - val_loss: 2.3934 - val_acc: 0.1496\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4026 - acc: 0.1494 - val_loss: 2.3880 - val_acc: 0.1544\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3960 - acc: 0.1516 - val_loss: 2.3878 - val_acc: 0.1598\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3930 - acc: 0.1530 - val_loss: 2.3861 - val_acc: 0.1539\n",
      "('Log_loss:', 2.3860814547270515, 'It took', -91.59002995491028)\n",
      "('Average time per eval:', -109.02034128563744)\n",
      "Params testing 140: {'n_epoch': 3.875878085109068, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 51.46541481111237, 'dropout2': 0.28753606700970946}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4155 - acc: 0.1410 - val_loss: 2.3958 - val_acc: 0.1551\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4012 - acc: 0.1491 - val_loss: 2.3873 - val_acc: 0.1559\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3963 - acc: 0.1500 - val_loss: 2.3903 - val_acc: 0.1532\n",
      "('Log_loss:', 2.3903113540362777, 'It took', -65.47593092918396)\n",
      "('Average time per eval:', -108.71151567350888)\n",
      "Params testing 141: {'n_epoch': 4.985298853272585, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 29.730066392401895, 'dropout2': 0.4189316158184213}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4170 - acc: 0.1417 - val_loss: 2.3931 - val_acc: 0.1492\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4023 - acc: 0.1483 - val_loss: 2.3890 - val_acc: 0.1540\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3963 - acc: 0.1509 - val_loss: 2.3861 - val_acc: 0.1568\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3930 - acc: 0.1520 - val_loss: 2.3854 - val_acc: 0.1598\n",
      "('Log_loss:', 2.3854274158818836, 'It took', -91.91761708259583)\n",
      "('Average time per eval:', -108.59324877362856)\n",
      "Params testing 142: {'n_epoch': 6.742936396004181, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 40.72472830206781, 'dropout2': 0.6161107444088243}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4216 - acc: 0.1386 - val_loss: 2.3965 - val_acc: 0.1429\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4058 - acc: 0.1471 - val_loss: 2.3901 - val_acc: 0.1547\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4006 - acc: 0.1492 - val_loss: 2.3869 - val_acc: 0.1539\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3967 - acc: 0.1510 - val_loss: 2.3875 - val_acc: 0.1574\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3956 - acc: 0.1527 - val_loss: 2.3857 - val_acc: 0.1595\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3940 - acc: 0.1520 - val_loss: 2.3856 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3856339613841424, 'It took', -149.63225293159485)\n",
      "('Average time per eval:', -108.8802348100222)\n",
      "Params testing 143: {'n_epoch': 4.86385193045002, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 102.30545335123321, 'dropout2': 0.5040673148974597}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4184 - acc: 0.1389 - val_loss: 2.3933 - val_acc: 0.1500\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4047 - acc: 0.1463 - val_loss: 2.3897 - val_acc: 0.1568\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3989 - acc: 0.1505 - val_loss: 2.3880 - val_acc: 0.1555\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3948 - acc: 0.1540 - val_loss: 2.3862 - val_acc: 0.1590\n",
      "('Log_loss:', 2.3861637834470519, 'It took', -92.68789291381836)\n",
      "('Average time per eval:', -108.76778798467583)\n",
      "Params testing 144: {'n_epoch': 4.375736817322133, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 54.668473489785534, 'dropout2': 0.3443293147353237}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4159 - acc: 0.1427 - val_loss: 2.3928 - val_acc: 0.1526\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4005 - acc: 0.1487 - val_loss: 2.3876 - val_acc: 0.1558\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3954 - acc: 0.1521 - val_loss: 2.3881 - val_acc: 0.1552\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3918 - acc: 0.1553 - val_loss: 2.3877 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3877344602973922, 'It took', -92.37536001205444)\n",
      "('Average time per eval:', -108.65473675070137)\n",
      "Params testing 145: {'n_epoch': 4.553066100907769, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 19.045923427587166, 'dropout2': 0.41140522279395486}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4189 - acc: 0.1386 - val_loss: 2.3950 - val_acc: 0.1528\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4027 - acc: 0.1476 - val_loss: 2.3915 - val_acc: 0.1538\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3968 - acc: 0.1488 - val_loss: 2.3873 - val_acc: 0.1536\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3935 - acc: 0.1523 - val_loss: 2.3876 - val_acc: 0.1571\n",
      "('Log_loss:', 2.3876293523044008, 'It took', -91.26607394218445)\n",
      "('Average time per eval:', -108.53563631397404)\n",
      "Params testing 146: {'n_epoch': 4.526722237131457, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 32.51433338844505, 'dropout2': 0.4911336693689573}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4178 - acc: 0.1387 - val_loss: 2.3933 - val_acc: 0.1507\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4012 - acc: 0.1484 - val_loss: 2.3892 - val_acc: 0.1485\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3987 - acc: 0.1502 - val_loss: 2.3877 - val_acc: 0.1516\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3949 - acc: 0.1532 - val_loss: 2.3883 - val_acc: 0.1570\n",
      "('Log_loss:', 2.3882628132886294, 'It took', -91.9324700832367)\n",
      "('Average time per eval:', -108.42268959843382)\n",
      "Params testing 147: {'n_epoch': 3.9552333337876195, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 36.37309538918705, 'dropout2': 0.4528400747957127}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4181 - acc: 0.1403 - val_loss: 2.3938 - val_acc: 0.1479\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4020 - acc: 0.1495 - val_loss: 2.3885 - val_acc: 0.1552\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3959 - acc: 0.1523 - val_loss: 2.3861 - val_acc: 0.1574\n",
      "('Log_loss:', 2.3860646250769757, 'It took', -64.31072497367859)\n",
      "('Average time per eval:', -108.12463577534702)\n",
      "Params testing 148: {'n_epoch': 4.731696398619908, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 26.828627600752604, 'dropout2': 0.3967426802582509}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4174 - acc: 0.1387 - val_loss: 2.3915 - val_acc: 0.1459\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4019 - acc: 0.1496 - val_loss: 2.3905 - val_acc: 0.1538\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3963 - acc: 0.1504 - val_loss: 2.3868 - val_acc: 0.1576\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3928 - acc: 0.1544 - val_loss: 2.3862 - val_acc: 0.1566\n",
      "('Log_loss:', 2.386223287164702, 'It took', -91.82431817054749)\n",
      "('Average time per eval:', -108.01523766421631)\n",
      "Params testing 149: {'n_epoch': 4.620696259888528, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 44.78749810839024, 'dropout2': 0.35782464477588705}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4163 - acc: 0.1405 - val_loss: 2.3937 - val_acc: 0.1491\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4013 - acc: 0.1488 - val_loss: 2.3893 - val_acc: 0.1564\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3954 - acc: 0.1534 - val_loss: 2.3860 - val_acc: 0.1559\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3920 - acc: 0.1536 - val_loss: 2.3854 - val_acc: 0.1571\n",
      "('Log_loss:', 2.3853701885890426, 'It took', -92.35750007629395)\n",
      "('Average time per eval:', -107.91085273901622)\n",
      "Params testing 150: {'n_epoch': 4.284514690270492, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 23.640795226212088, 'dropout2': 0.5474958244136654}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4193 - acc: 0.1398 - val_loss: 2.3950 - val_acc: 0.1511\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4045 - acc: 0.1486 - val_loss: 2.3907 - val_acc: 0.1559\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3989 - acc: 0.1494 - val_loss: 2.3878 - val_acc: 0.1531\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3971 - acc: 0.1505 - val_loss: 2.3873 - val_acc: 0.1564\n",
      "('Log_loss:', 2.3872866231380314, 'It took', -91.47525191307068)\n",
      "('Average time per eval:', -107.80200769411807)\n",
      "Params testing 151: {'n_epoch': 4.199789569870321, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 16.03310135188823, 'dropout2': 0.38444512263617125}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4180 - acc: 0.1401 - val_loss: 2.3946 - val_acc: 0.1540\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4024 - acc: 0.1476 - val_loss: 2.3883 - val_acc: 0.1546\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3970 - acc: 0.1500 - val_loss: 2.3871 - val_acc: 0.1522\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3940 - acc: 0.1531 - val_loss: 2.3870 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3870055864715014, 'It took', -91.40870404243469)\n",
      "('Average time per eval:', -107.6941570121991)\n",
      "Params testing 152: {'n_epoch': 5.064156830289064, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 67.84320401306267, 'dropout2': 0.32034811678765807}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4149 - acc: 0.1426 - val_loss: 2.3921 - val_acc: 0.1487\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4011 - acc: 0.1492 - val_loss: 2.3889 - val_acc: 0.1562\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3958 - acc: 0.1521 - val_loss: 2.3852 - val_acc: 0.1519\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3921 - acc: 0.1547 - val_loss: 2.3863 - val_acc: 0.1560\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3873 - acc: 0.1566 - val_loss: 2.3862 - val_acc: 0.1593\n",
      "('Log_loss:', 2.3862452584618445, 'It took', -120.41777610778809)\n",
      "('Average time per eval:', -107.77731791352915)\n",
      "Params testing 153: {'n_epoch': 5.1775016400154605, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 37.21909044727625, 'dropout2': 0.5188146261084822}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4191 - acc: 0.1389 - val_loss: 2.3941 - val_acc: 0.1471\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4037 - acc: 0.1478 - val_loss: 2.3885 - val_acc: 0.1547\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3980 - acc: 0.1482 - val_loss: 2.3869 - val_acc: 0.1532\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3943 - acc: 0.1534 - val_loss: 2.3856 - val_acc: 0.1551\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3916 - acc: 0.1530 - val_loss: 2.3860 - val_acc: 0.1566\n",
      "('Log_loss:', 2.3859718618398054, 'It took', -120.77262616157532)\n",
      "('Average time per eval:', -107.86170303202294)\n",
      "Params testing 154: {'n_epoch': 4.443527930880994, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 51.300127349479496, 'dropout2': 0.30668629323738456}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4149 - acc: 0.1419 - val_loss: 2.3927 - val_acc: 0.1505\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4003 - acc: 0.1478 - val_loss: 2.3901 - val_acc: 0.1523\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3945 - acc: 0.1520 - val_loss: 2.3866 - val_acc: 0.1552\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3908 - acc: 0.1552 - val_loss: 2.3871 - val_acc: 0.1538\n",
      "('Log_loss:', 2.3871198227410018, 'It took', -93.05456209182739)\n",
      "('Average time per eval:', -107.76617308432056)\n",
      "Params testing 155: {'n_epoch': 3.655798871457421, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 34.75309686737357, 'dropout2': 0.43252488638766534}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4176 - acc: 0.1421 - val_loss: 2.3940 - val_acc: 0.1510\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4015 - acc: 0.1486 - val_loss: 2.3881 - val_acc: 0.1576\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3971 - acc: 0.1512 - val_loss: 2.3874 - val_acc: 0.1566\n",
      "('Log_loss:', 2.3874212060669553, 'It took', -64.38892602920532)\n",
      "('Average time per eval:', -107.48811380221294)\n",
      "Params testing 156: {'n_epoch': 6.6157483924582134, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 57.22833449203574, 'dropout2': 0.5956169029558165}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4189 - acc: 0.1408 - val_loss: 2.3926 - val_acc: 0.1544\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4044 - acc: 0.1486 - val_loss: 2.3898 - val_acc: 0.1556\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4002 - acc: 0.1494 - val_loss: 2.3881 - val_acc: 0.1538\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3971 - acc: 0.1505 - val_loss: 2.3864 - val_acc: 0.1574\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3936 - acc: 0.1528 - val_loss: 2.3867 - val_acc: 0.1546\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3930 - acc: 0.1542 - val_loss: 2.3855 - val_acc: 0.1578\n",
      "('Log_loss:', 2.3855085249979249, 'It took', -150.79736304283142)\n",
      "('Average time per eval:', -107.76396888684316)\n",
      "Params testing 157: {'n_epoch': 6.488859905462347, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 62.01594917826618, 'dropout2': 0.24233892255305822}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4142 - acc: 0.1411 - val_loss: 2.3933 - val_acc: 0.1493\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4002 - acc: 0.1488 - val_loss: 2.3872 - val_acc: 0.1546\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3933 - acc: 0.1531 - val_loss: 2.3873 - val_acc: 0.1571\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3903 - acc: 0.1548 - val_loss: 2.3860 - val_acc: 0.1574\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3857 - acc: 0.1559 - val_loss: 2.3862 - val_acc: 0.1559\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3845 - acc: 0.1593 - val_loss: 2.3862 - val_acc: 0.1540\n",
      "('Log_loss:', 2.3861524917119441, 'It took', -151.24909377098083)\n",
      "('Average time per eval:', -108.03919119020051)\n",
      "Params testing 158: {'n_epoch': 3.351218354148639, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 156.73765817827797, 'dropout2': 0.4728599956065591}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 2.4195 - acc: 0.1382 - val_loss: 2.3939 - val_acc: 0.1461\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4045 - acc: 0.1479 - val_loss: 2.3886 - val_acc: 0.1515\n",
      "Epoch 3/3\n",
      "26s - loss: 2.4003 - acc: 0.1514 - val_loss: 2.3849 - val_acc: 0.1548\n",
      "('Log_loss:', 2.3849425056713245, 'It took', -67.4336199760437)\n",
      "('Average time per eval:', -107.78381023317013)\n",
      "Params testing 159: {'n_epoch': 4.06523220889993, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 70.93784802167022, 'dropout2': 0.444746487590554}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4167 - acc: 0.1408 - val_loss: 2.3924 - val_acc: 0.1485\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4036 - acc: 0.1483 - val_loss: 2.3877 - val_acc: 0.1535\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3966 - acc: 0.1512 - val_loss: 2.3863 - val_acc: 0.1555\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3934 - acc: 0.1549 - val_loss: 2.3886 - val_acc: 0.1507\n",
      "('Log_loss:', 2.3885759467929164, 'It took', -93.0445499420166)\n",
      "('Average time per eval:', -107.69168985038996)\n",
      "Params testing 160: {'n_epoch': 3.81472225311013, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 21.271216603418452, 'dropout2': 0.40268862475412553}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4167 - acc: 0.1411 - val_loss: 2.3931 - val_acc: 0.1514\n",
      "Epoch 2/3\n",
      "22s - loss: 2.4022 - acc: 0.1488 - val_loss: 2.3880 - val_acc: 0.1548\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3966 - acc: 0.1503 - val_loss: 2.3867 - val_acc: 0.1550\n",
      "('Log_loss:', 2.3867237979210385, 'It took', -64.45019197463989)\n",
      "('Average time per eval:', -107.423109112319)\n",
      "Params testing 161: {'n_epoch': 5.296636361685837, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 40.487876136879564, 'dropout2': 0.3325172238423102}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4147 - acc: 0.1417 - val_loss: 2.3919 - val_acc: 0.1485\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4005 - acc: 0.1509 - val_loss: 2.3860 - val_acc: 0.1542\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3946 - acc: 0.1527 - val_loss: 2.3856 - val_acc: 0.1566\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3903 - acc: 0.1543 - val_loss: 2.3861 - val_acc: 0.1591\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3887 - acc: 0.1564 - val_loss: 2.3859 - val_acc: 0.1567\n",
      "('Log_loss:', 2.3858589108303856, 'It took', -120.5241630077362)\n",
      "('Average time per eval:', -107.50397980213165)\n",
      "Params testing 162: {'n_epoch': 4.854411570424223, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 77.01405005048261, 'dropout2': 0.3691858942004779}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4169 - acc: 0.1398 - val_loss: 2.3948 - val_acc: 0.1547\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4028 - acc: 0.1488 - val_loss: 2.3959 - val_acc: 0.1479\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3985 - acc: 0.1494 - val_loss: 2.3916 - val_acc: 0.1520\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3943 - acc: 0.1519 - val_loss: 2.3903 - val_acc: 0.1532\n",
      "('Log_loss:', 2.3903133932400285, 'It took', -93.7135488986969)\n",
      "('Average time per eval:', -107.41937592570767)\n",
      "Params testing 163: {'n_epoch': 4.936933456073427, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 47.48592477237979, 'dropout2': 0.22248195365574938}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4140 - acc: 0.1412 - val_loss: 2.3920 - val_acc: 0.1496\n",
      "Epoch 2/4\n",
      "21s - loss: 2.3987 - acc: 0.1497 - val_loss: 2.3874 - val_acc: 0.1551\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3930 - acc: 0.1534 - val_loss: 2.3861 - val_acc: 0.1564\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3892 - acc: 0.1561 - val_loss: 2.3845 - val_acc: 0.1613\n",
      "('Log_loss:', 2.3844946130108267, 'It took', -92.8830840587616)\n",
      "('Average time per eval:', -107.33073999387462)\n",
      "Params testing 164: {'n_epoch': 4.492893395585066, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 87.20319052929496, 'dropout2': 0.5732055137957771}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4199 - acc: 0.1378 - val_loss: 2.3928 - val_acc: 0.1503\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4039 - acc: 0.1484 - val_loss: 2.3902 - val_acc: 0.1524\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3996 - acc: 0.1514 - val_loss: 2.3871 - val_acc: 0.1538\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3961 - acc: 0.1539 - val_loss: 2.3861 - val_acc: 0.1536\n",
      "('Log_loss:', 2.3860608716608849, 'It took', -93.85486698150635)\n",
      "('Average time per eval:', -107.24906803044406)\n",
      "Params testing 165: {'n_epoch': 5.593075824762007, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 237.42218894691197, 'dropout2': 0.2589563677416554}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4197 - acc: 0.1389 - val_loss: 2.3940 - val_acc: 0.1526\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4054 - acc: 0.1470 - val_loss: 2.3899 - val_acc: 0.1505\n",
      "Epoch 3/5\n",
      "26s - loss: 2.3991 - acc: 0.1487 - val_loss: 2.3919 - val_acc: 0.1507\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3943 - acc: 0.1533 - val_loss: 2.3863 - val_acc: 0.1540\n",
      "Epoch 5/5\n",
      "30s - loss: 2.3916 - acc: 0.1534 - val_loss: 2.3863 - val_acc: 0.1531\n",
      "('Log_loss:', 2.3863188280932808, 'It took', -127.33474898338318)\n",
      "('Average time per eval:', -107.37006610847381)\n",
      "Params testing 166: {'n_epoch': 4.319321934289456, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 59.43877617556389, 'dropout2': 0.48519989330070795}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4177 - acc: 0.1411 - val_loss: 2.3937 - val_acc: 0.1503\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4039 - acc: 0.1470 - val_loss: 2.3869 - val_acc: 0.1574\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3972 - acc: 0.1516 - val_loss: 2.3860 - val_acc: 0.1586\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3940 - acc: 0.1529 - val_loss: 2.3853 - val_acc: 0.1618\n",
      "('Log_loss:', 2.3853491957628981, 'It took', -92.50200009346008)\n",
      "('Average time per eval:', -107.28103576020567)\n",
      "Params testing 167: {'n_epoch': 4.759440926759814, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 16.044209294651917, 'dropout2': 0.20556480855502313}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4166 - acc: 0.1403 - val_loss: 2.3952 - val_acc: 0.1472\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4012 - acc: 0.1486 - val_loss: 2.3912 - val_acc: 0.1530\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3965 - acc: 0.1525 - val_loss: 2.3895 - val_acc: 0.1547\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3931 - acc: 0.1526 - val_loss: 2.3877 - val_acc: 0.1567\n",
      "('Log_loss:', 2.387679526721572, 'It took', -92.0951738357544)\n",
      "('Average time per eval:', -107.1906437135878)\n",
      "Params testing 168: {'n_epoch': 5.73148888167324, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 29.242284127979985, 'dropout2': 0.279384117510392}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4151 - acc: 0.1422 - val_loss: 2.3937 - val_acc: 0.1468\n",
      "Epoch 2/5\n",
      "21s - loss: 2.3995 - acc: 0.1493 - val_loss: 2.3872 - val_acc: 0.1559\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3942 - acc: 0.1525 - val_loss: 2.3872 - val_acc: 0.1597\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3893 - acc: 0.1559 - val_loss: 2.3859 - val_acc: 0.1574\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3876 - acc: 0.1556 - val_loss: 2.3859 - val_acc: 0.1582\n",
      "('Log_loss:', 2.385949267991069, 'It took', -120.8684720993042)\n",
      "('Average time per eval:', -107.27157760371823)\n",
      "Params testing 169: {'n_epoch': 4.669579802576633, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 24.60688885807181, 'dropout2': 0.4548363542915925}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4180 - acc: 0.1389 - val_loss: 2.3942 - val_acc: 0.1508\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4026 - acc: 0.1477 - val_loss: 2.3900 - val_acc: 0.1539\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3989 - acc: 0.1529 - val_loss: 2.3878 - val_acc: 0.1560\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3938 - acc: 0.1522 - val_loss: 2.3879 - val_acc: 0.1538\n",
      "('Log_loss:', 2.387861061281408, 'It took', -91.71187210083008)\n",
      "('Average time per eval:', -107.18004991867963)\n",
      "Params testing 170: {'n_epoch': 5.355174929678906, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 81.26036372955802, 'dropout2': 0.42249691893471053}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4154 - acc: 0.1419 - val_loss: 2.3924 - val_acc: 0.1488\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4031 - acc: 0.1480 - val_loss: 2.3874 - val_acc: 0.1547\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3969 - acc: 0.1513 - val_loss: 2.3884 - val_acc: 0.1473\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3929 - acc: 0.1531 - val_loss: 2.3883 - val_acc: 0.1543\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3887 - acc: 0.1595 - val_loss: 2.3847 - val_acc: 0.1589\n",
      "('Log_loss:', 2.3846609894053534, 'It took', -121.9895691871643)\n",
      "('Average time per eval:', -107.26665528894168)\n",
      "Params testing 171: {'n_epoch': 5.507335040922, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 53.53005773546954, 'dropout2': 0.3003568376885782}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4156 - acc: 0.1407 - val_loss: 2.3940 - val_acc: 0.1455\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4009 - acc: 0.1478 - val_loss: 2.3884 - val_acc: 0.1546\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3952 - acc: 0.1512 - val_loss: 2.3872 - val_acc: 0.1540\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3907 - acc: 0.1561 - val_loss: 2.3845 - val_acc: 0.1566\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3877 - acc: 0.1577 - val_loss: 2.3863 - val_acc: 0.1575\n",
      "('Log_loss:', 2.3862632546736542, 'It took', -121.28507804870605)\n",
      "('Average time per eval:', -107.34815773991652)\n",
      "Params testing 172: {'n_epoch': 4.145187138367454, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 112.15977415126427, 'dropout2': 0.5368151390167372}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4187 - acc: 0.1411 - val_loss: 2.3950 - val_acc: 0.1570\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4057 - acc: 0.1458 - val_loss: 2.3898 - val_acc: 0.1547\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3996 - acc: 0.1510 - val_loss: 2.3874 - val_acc: 0.1579\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3962 - acc: 0.1518 - val_loss: 2.3873 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3872804818637499, 'It took', -92.85132193565369)\n",
      "('Average time per eval:', -107.26436099565097)\n",
      "Params testing 173: {'n_epoch': 5.9543429936422045, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 93.92528416555245, 'dropout2': 0.3850192831832635}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4166 - acc: 0.1411 - val_loss: 2.4000 - val_acc: 0.1496\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4034 - acc: 0.1482 - val_loss: 2.3915 - val_acc: 0.1538\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3981 - acc: 0.1521 - val_loss: 2.3908 - val_acc: 0.1570\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3954 - acc: 0.1536 - val_loss: 2.3898 - val_acc: 0.1572\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3924 - acc: 0.1543 - val_loss: 2.3907 - val_acc: 0.1526\n",
      "('Log_loss:', 2.3907360781494109, 'It took', -122.6097400188446)\n",
      "('Average time per eval:', -107.35255281267494)\n",
      "Params testing 174: {'n_epoch': 5.250046768399113, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 43.638827201507226, 'dropout2': 0.641878824346189}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4219 - acc: 0.1380 - val_loss: 2.3952 - val_acc: 0.1505\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4058 - acc: 0.1470 - val_loss: 2.3912 - val_acc: 0.1504\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4019 - acc: 0.1488 - val_loss: 2.3895 - val_acc: 0.1563\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3985 - acc: 0.1500 - val_loss: 2.3905 - val_acc: 0.1568\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3957 - acc: 0.1538 - val_loss: 2.3875 - val_acc: 0.1576\n",
      "('Log_loss:', 2.3875411852192312, 'It took', -120.93078398704529)\n",
      "('Average time per eval:', -107.43014269965036)\n",
      "Params testing 175: {'n_epoch': 6.12063946777806, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 64.10295352578201, 'dropout2': 0.5102698370791837}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4185 - acc: 0.1408 - val_loss: 2.3943 - val_acc: 0.1532\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4029 - acc: 0.1501 - val_loss: 2.3894 - val_acc: 0.1543\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3989 - acc: 0.1506 - val_loss: 2.3873 - val_acc: 0.1552\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3943 - acc: 0.1531 - val_loss: 2.3858 - val_acc: 0.1563\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3931 - acc: 0.1528 - val_loss: 2.3850 - val_acc: 0.1571\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3902 - acc: 0.1563 - val_loss: 2.3834 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3834482788432867, 'It took', -151.28464102745056)\n",
      "('Average time per eval:', -107.67931598018517)\n",
      "Params testing 176: {'n_epoch': 6.180416594836241, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 98.4780739994947, 'dropout2': 0.6598329841158826}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4218 - acc: 0.1385 - val_loss: 2.3939 - val_acc: 0.1511\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4069 - acc: 0.1455 - val_loss: 2.3896 - val_acc: 0.1547\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4027 - acc: 0.1491 - val_loss: 2.3897 - val_acc: 0.1572\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3998 - acc: 0.1497 - val_loss: 2.3867 - val_acc: 0.1497\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3960 - acc: 0.1513 - val_loss: 2.3860 - val_acc: 0.1597\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3953 - acc: 0.1548 - val_loss: 2.3869 - val_acc: 0.1564\n",
      "('Log_loss:', 2.3869448880264086, 'It took', -151.9278860092163)\n",
      "('Average time per eval:', -107.92930789047715)\n",
      "Params testing 177: {'n_epoch': 6.313664879322425, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 65.18997930196389, 'dropout2': 0.5904555137727632}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4207 - acc: 0.1385 - val_loss: 2.3942 - val_acc: 0.1456\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4042 - acc: 0.1480 - val_loss: 2.3891 - val_acc: 0.1547\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3996 - acc: 0.1505 - val_loss: 2.3881 - val_acc: 0.1534\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3955 - acc: 0.1514 - val_loss: 2.3874 - val_acc: 0.1585\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3951 - acc: 0.1538 - val_loss: 2.3869 - val_acc: 0.1563\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3915 - acc: 0.1536 - val_loss: 2.3845 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3845337152864285, 'It took', -150.4530210494995)\n",
      "('Average time per eval:', -108.16820514336061)\n",
      "Params testing 178: {'n_epoch': 6.062095052076792, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 75.42418620917408, 'dropout2': 0.7133500121488949}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4235 - acc: 0.1381 - val_loss: 2.3950 - val_acc: 0.1472\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4076 - acc: 0.1467 - val_loss: 2.3907 - val_acc: 0.1527\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4044 - acc: 0.1501 - val_loss: 2.3887 - val_acc: 0.1548\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3993 - acc: 0.1519 - val_loss: 2.3898 - val_acc: 0.1542\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3993 - acc: 0.1507 - val_loss: 2.3875 - val_acc: 0.1595\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3960 - acc: 0.1505 - val_loss: 2.3860 - val_acc: 0.1571\n",
      "('Log_loss:', 2.3860026002089905, 'It took', -150.95712208747864)\n",
      "('Average time per eval:', -108.40724936677091)\n",
      "Params testing 179: {'n_epoch': 6.4235417095694025, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 84.82843755435607, 'dropout2': 0.6270127224087247}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4212 - acc: 0.1372 - val_loss: 2.4005 - val_acc: 0.1460\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4096 - acc: 0.1449 - val_loss: 2.3991 - val_acc: 0.1495\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4046 - acc: 0.1439 - val_loss: 2.3968 - val_acc: 0.1510\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4019 - acc: 0.1458 - val_loss: 2.3972 - val_acc: 0.1528\n",
      "Epoch 5/6\n",
      "28s - loss: 2.4000 - acc: 0.1489 - val_loss: 2.3941 - val_acc: 0.1540\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3999 - acc: 0.1486 - val_loss: 2.3933 - val_acc: 0.1510\n",
      "('Log_loss:', 2.3933230990415728, 'It took', -151.55580878257751)\n",
      "('Average time per eval:', -108.64696358044942)\n",
      "Params testing 180: {'n_epoch': 5.799400581071268, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 70.94366155130786, 'dropout2': 0.6782302248569295}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4225 - acc: 0.1369 - val_loss: 2.3948 - val_acc: 0.1477\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4081 - acc: 0.1448 - val_loss: 2.3906 - val_acc: 0.1563\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4017 - acc: 0.1497 - val_loss: 2.3875 - val_acc: 0.1527\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3997 - acc: 0.1488 - val_loss: 2.3862 - val_acc: 0.1560\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3968 - acc: 0.1517 - val_loss: 2.3854 - val_acc: 0.1587\n",
      "('Log_loss:', 2.3853659718278712, 'It took', -120.93397998809814)\n",
      "('Average time per eval:', -108.71484764373105)\n",
      "Params testing 181: {'n_epoch': 6.758839335104549, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 168.70342343498044, 'dropout2': 0.5173016081924069}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4192 - acc: 0.1394 - val_loss: 2.3908 - val_acc: 0.1504\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4070 - acc: 0.1477 - val_loss: 2.3922 - val_acc: 0.1552\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4018 - acc: 0.1496 - val_loss: 2.3875 - val_acc: 0.1575\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3970 - acc: 0.1542 - val_loss: 2.3871 - val_acc: 0.1547\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3942 - acc: 0.1529 - val_loss: 2.3855 - val_acc: 0.1571\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3928 - acc: 0.1546 - val_loss: 2.3884 - val_acc: 0.1551\n",
      "('Log_loss:', 2.38844676704979, 'It took', -155.2602779865265)\n",
      "('Average time per eval:', -108.97059176125369)\n",
      "Params testing 182: {'n_epoch': 6.130411162879328, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 141.67406062812924, 'dropout2': 0.5475628967155637}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4196 - acc: 0.1395 - val_loss: 2.3933 - val_acc: 0.1552\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4062 - acc: 0.1472 - val_loss: 2.3898 - val_acc: 0.1514\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4007 - acc: 0.1486 - val_loss: 2.3879 - val_acc: 0.1594\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3980 - acc: 0.1527 - val_loss: 2.3867 - val_acc: 0.1563\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3953 - acc: 0.1553 - val_loss: 2.3854 - val_acc: 0.1591\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3914 - acc: 0.1546 - val_loss: 2.3829 - val_acc: 0.1587\n",
      "('Log_loss:', 2.3828570247654488, 'It took', -153.20782995224)\n",
      "('Average time per eval:', -109.2123252980696)\n",
      "Params testing 183: {'n_epoch': 6.546547262632855, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 175.36008320017118, 'dropout2': 0.6078811966137605}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4218 - acc: 0.1372 - val_loss: 2.3937 - val_acc: 0.1504\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4078 - acc: 0.1465 - val_loss: 2.3881 - val_acc: 0.1559\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4032 - acc: 0.1502 - val_loss: 2.3877 - val_acc: 0.1543\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4000 - acc: 0.1529 - val_loss: 2.3867 - val_acc: 0.1586\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3976 - acc: 0.1515 - val_loss: 2.3871 - val_acc: 0.1574\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3943 - acc: 0.1547 - val_loss: 2.3863 - val_acc: 0.1552\n",
      "('Log_loss:', 2.3862687805814713, 'It took', -155.51942491531372)\n",
      "('Average time per eval:', -109.46399431254552)\n",
      "Params testing 184: {'n_epoch': 6.133044247881735, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 140.06528432583917, 'dropout2': 0.575504435822154}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4209 - acc: 0.1381 - val_loss: 2.3948 - val_acc: 0.1492\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4063 - acc: 0.1480 - val_loss: 2.3917 - val_acc: 0.1510\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4012 - acc: 0.1490 - val_loss: 2.3867 - val_acc: 0.1586\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3985 - acc: 0.1516 - val_loss: 2.3854 - val_acc: 0.1589\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3940 - acc: 0.1521 - val_loss: 2.3862 - val_acc: 0.1542\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3936 - acc: 0.1559 - val_loss: 2.3851 - val_acc: 0.1567\n",
      "('Log_loss:', 2.3850585276212573, 'It took', -154.08138513565063)\n",
      "('Average time per eval:', -109.70516938647708)\n",
      "Params testing 185: {'n_epoch': 6.831231127316289, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 145.31996854370698, 'dropout2': 0.5575185142460642}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4193 - acc: 0.1388 - val_loss: 2.3949 - val_acc: 0.1493\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4060 - acc: 0.1476 - val_loss: 2.3896 - val_acc: 0.1551\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4009 - acc: 0.1498 - val_loss: 2.3880 - val_acc: 0.1556\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3977 - acc: 0.1508 - val_loss: 2.3888 - val_acc: 0.1578\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3950 - acc: 0.1523 - val_loss: 2.3860 - val_acc: 0.1567\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3926 - acc: 0.1544 - val_loss: 2.3859 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3858619222939894, 'It took', -153.62330889701843)\n",
      "('Average time per eval:', -109.94128841097637)\n",
      "Params testing 186: {'n_epoch': 5.881047483980036, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 186.3078074690097, 'dropout2': 0.8037718820648352}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4285 - acc: 0.1359 - val_loss: 2.4113 - val_acc: 0.1426\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4158 - acc: 0.1399 - val_loss: 2.4072 - val_acc: 0.1508\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4126 - acc: 0.1428 - val_loss: 2.4060 - val_acc: 0.1467\n",
      "Epoch 4/5\n",
      "28s - loss: 2.4116 - acc: 0.1433 - val_loss: 2.4062 - val_acc: 0.1463\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4119 - acc: 0.1435 - val_loss: 2.4100 - val_acc: 0.1457\n",
      "('Log_loss:', 2.410018859026485, 'It took', -125.52652406692505)\n",
      "('Average time per eval:', -110.02463191715792)\n",
      "Params testing 187: {'n_epoch': 6.016341668743548, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 208.66048996883507, 'dropout2': 0.4997543767252614}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4199 - acc: 0.1401 - val_loss: 2.3928 - val_acc: 0.1485\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4077 - acc: 0.1464 - val_loss: 2.3896 - val_acc: 0.1532\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4011 - acc: 0.1498 - val_loss: 2.3868 - val_acc: 0.1571\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3984 - acc: 0.1517 - val_loss: 2.3888 - val_acc: 0.1538\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3959 - acc: 0.1526 - val_loss: 2.3847 - val_acc: 0.1544\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3924 - acc: 0.1561 - val_loss: 2.3853 - val_acc: 0.1554\n",
      "('Log_loss:', 2.3853023694885533, 'It took', -155.70577311515808)\n",
      "('Average time per eval:', -110.26761670442338)\n",
      "Params testing 188: {'n_epoch': 6.292858153518788, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 132.71113593320658, 'dropout2': 0.5296004415763214}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4193 - acc: 0.1392 - val_loss: 2.3953 - val_acc: 0.1491\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4057 - acc: 0.1478 - val_loss: 2.3908 - val_acc: 0.1551\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4005 - acc: 0.1498 - val_loss: 2.3869 - val_acc: 0.1572\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3979 - acc: 0.1516 - val_loss: 2.3873 - val_acc: 0.1519\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3939 - acc: 0.1557 - val_loss: 2.3849 - val_acc: 0.1617\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3920 - acc: 0.1560 - val_loss: 2.3856 - val_acc: 0.1536\n",
      "('Log_loss:', 2.3855730699145248, 'It took', -152.15763401985168)\n",
      "('Average time per eval:', -110.48925700767961)\n",
      "Params testing 189: {'n_epoch': 6.5829144995743025, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 151.36360481372716, 'dropout2': 0.5457849412328661}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4207 - acc: 0.1381 - val_loss: 2.3918 - val_acc: 0.1453\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4063 - acc: 0.1461 - val_loss: 2.3888 - val_acc: 0.1496\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3999 - acc: 0.1511 - val_loss: 2.3867 - val_acc: 0.1556\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3978 - acc: 0.1511 - val_loss: 2.3865 - val_acc: 0.1575\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3943 - acc: 0.1522 - val_loss: 2.3863 - val_acc: 0.1594\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3931 - acc: 0.1536 - val_loss: 2.3887 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3887081035908571, 'It took', -153.51680397987366)\n",
      "('Average time per eval:', -110.7157177711788)\n",
      "Params testing 190: {'n_epoch': 6.41534112103164, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 121.95475432079718, 'dropout2': 0.6501290283497632}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4225 - acc: 0.1379 - val_loss: 2.3925 - val_acc: 0.1518\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4078 - acc: 0.1473 - val_loss: 2.3901 - val_acc: 0.1571\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4033 - acc: 0.1480 - val_loss: 2.3884 - val_acc: 0.1552\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3989 - acc: 0.1512 - val_loss: 2.3881 - val_acc: 0.1550\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3975 - acc: 0.1536 - val_loss: 2.3865 - val_acc: 0.1556\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3960 - acc: 0.1529 - val_loss: 2.3863 - val_acc: 0.1547\n",
      "('Log_loss:', 2.3863340939237707, 'It took', -152.16214609146118)\n",
      "('Average time per eval:', -110.93271477309821)\n",
      "Params testing 191: {'n_epoch': 6.677444881043633, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 126.74943509105232, 'dropout2': 0.762934816680993}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4267 - acc: 0.1337 - val_loss: 2.4085 - val_acc: 0.1437\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4151 - acc: 0.1426 - val_loss: 2.4028 - val_acc: 0.1473\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4108 - acc: 0.1436 - val_loss: 2.4036 - val_acc: 0.1475\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4089 - acc: 0.1444 - val_loss: 2.4018 - val_acc: 0.1453\n",
      "Epoch 5/6\n",
      "29s - loss: 2.4078 - acc: 0.1446 - val_loss: 2.4026 - val_acc: 0.1453\n",
      "Epoch 6/6\n",
      "29s - loss: 2.4064 - acc: 0.1473 - val_loss: 2.3995 - val_acc: 0.1492\n",
      "('Log_loss:', 2.3994764331803067, 'It took', -152.89698481559753)\n",
      "('Average time per eval:', -111.15127867460251)\n",
      "Params testing 192: {'n_epoch': 5.6659951241073445, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 118.31543573385156, 'dropout2': 0.7346324904573431}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4238 - acc: 0.1391 - val_loss: 2.3948 - val_acc: 0.1492\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4100 - acc: 0.1440 - val_loss: 2.3901 - val_acc: 0.1511\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4045 - acc: 0.1476 - val_loss: 2.3897 - val_acc: 0.1530\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4028 - acc: 0.1497 - val_loss: 2.3897 - val_acc: 0.1546\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4008 - acc: 0.1511 - val_loss: 2.3872 - val_acc: 0.1572\n",
      "('Log_loss:', 2.3872365737740551, 'It took', -122.62330198287964)\n",
      "('Average time per eval:', -111.21071920493723)\n",
      "Params testing 193: {'n_epoch': 6.960550388334417, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 156.38215338568122, 'dropout2': 0.5617443961882157}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4204 - acc: 0.1397 - val_loss: 2.3934 - val_acc: 0.1484\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4059 - acc: 0.1475 - val_loss: 2.3874 - val_acc: 0.1590\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4021 - acc: 0.1502 - val_loss: 2.3885 - val_acc: 0.1590\n",
      "Epoch 4/6\n",
      "29s - loss: 2.3964 - acc: 0.1542 - val_loss: 2.3867 - val_acc: 0.1558\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3957 - acc: 0.1542 - val_loss: 2.3872 - val_acc: 0.1547\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3934 - acc: 0.1543 - val_loss: 2.3845 - val_acc: 0.1576\n",
      "('Log_loss:', 2.3844629194983651, 'It took', -155.54962611198425)\n",
      "('Average time per eval:', -111.43927026655255)\n",
      "Params testing 194: {'n_epoch': 6.202758171737766, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 104.41799476784949, 'dropout2': 0.622701777358985}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4213 - acc: 0.1374 - val_loss: 2.3951 - val_acc: 0.1560\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4065 - acc: 0.1463 - val_loss: 2.3903 - val_acc: 0.1539\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4020 - acc: 0.1492 - val_loss: 2.3857 - val_acc: 0.1574\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3989 - acc: 0.1511 - val_loss: 2.3883 - val_acc: 0.1563\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3948 - acc: 0.1528 - val_loss: 2.3850 - val_acc: 0.1579\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3937 - acc: 0.1543 - val_loss: 2.3854 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3854364381378086, 'It took', -151.53280210494995)\n",
      "('Average time per eval:', -111.64487811724345)\n",
      "Params testing 195: {'n_epoch': 5.928992283065895, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 109.23959923313382, 'dropout2': 0.6714049389824539}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4219 - acc: 0.1396 - val_loss: 2.3938 - val_acc: 0.1540\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4077 - acc: 0.1465 - val_loss: 2.3921 - val_acc: 0.1543\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4030 - acc: 0.1489 - val_loss: 2.3885 - val_acc: 0.1558\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4006 - acc: 0.1504 - val_loss: 2.3864 - val_acc: 0.1595\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3977 - acc: 0.1521 - val_loss: 2.3891 - val_acc: 0.1526\n",
      "('Log_loss:', 2.3890555308901718, 'It took', -121.87857103347778)\n",
      "('Average time per eval:', -111.69709083012172)\n",
      "Params testing 196: {'n_epoch': 6.114698403757248, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 167.8756872274676, 'dropout2': 0.5066306759093246}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4189 - acc: 0.1399 - val_loss: 2.3918 - val_acc: 0.1508\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4056 - acc: 0.1482 - val_loss: 2.3892 - val_acc: 0.1530\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4001 - acc: 0.1508 - val_loss: 2.3855 - val_acc: 0.1576\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3965 - acc: 0.1524 - val_loss: 2.3857 - val_acc: 0.1576\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3959 - acc: 0.1534 - val_loss: 2.3858 - val_acc: 0.1558\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3920 - acc: 0.1565 - val_loss: 2.3879 - val_acc: 0.1538\n",
      "('Log_loss:', 2.3878549494182586, 'It took', -154.58247423171997)\n",
      "('Average time per eval:', -111.91478312560145)\n",
      "Params testing 197: {'n_epoch': 5.834890149129533, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 179.8776887061607, 'dropout2': 0.6886416279053111}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4229 - acc: 0.1380 - val_loss: 2.4059 - val_acc: 0.1472\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4124 - acc: 0.1425 - val_loss: 2.3977 - val_acc: 0.1512\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4084 - acc: 0.1450 - val_loss: 2.4010 - val_acc: 0.1491\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4053 - acc: 0.1457 - val_loss: 2.4030 - val_acc: 0.1487\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4036 - acc: 0.1465 - val_loss: 2.3980 - val_acc: 0.1472\n",
      "('Log_loss:', 2.3979712584096626, 'It took', -124.86047291755676)\n",
      "('Average time per eval:', -111.98016539727799)\n",
      "Params testing 198: {'n_epoch': 6.366262060202477, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 133.69231152459776, 'dropout2': 0.5989598882108146}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4199 - acc: 0.1386 - val_loss: 2.3951 - val_acc: 0.1455\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4068 - acc: 0.1469 - val_loss: 2.3907 - val_acc: 0.1522\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4010 - acc: 0.1511 - val_loss: 2.3867 - val_acc: 0.1527\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3989 - acc: 0.1516 - val_loss: 2.3861 - val_acc: 0.1579\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3957 - acc: 0.1529 - val_loss: 2.3858 - val_acc: 0.1585\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3934 - acc: 0.1539 - val_loss: 2.3840 - val_acc: 0.1591\n",
      "('Log_loss:', 2.38402009529241, 'It took', -152.95888495445251)\n",
      "('Average time per eval:', -112.18608861113313)\n",
      "Params testing 199: {'n_epoch': 6.500436925617696, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 91.29253234694721, 'dropout2': 0.6387089828898471}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4211 - acc: 0.1380 - val_loss: 2.3928 - val_acc: 0.1520\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4073 - acc: 0.1462 - val_loss: 2.3905 - val_acc: 0.1544\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4015 - acc: 0.1487 - val_loss: 2.3888 - val_acc: 0.1559\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3980 - acc: 0.1512 - val_loss: 2.3881 - val_acc: 0.1587\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3968 - acc: 0.1512 - val_loss: 2.3861 - val_acc: 0.1568\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3941 - acc: 0.1534 - val_loss: 2.3854 - val_acc: 0.1547\n",
      "('Log_loss:', 2.3853678355228478, 'It took', -151.40603399276733)\n",
      "('Average time per eval:', -112.38218833327294)\n",
      "Params testing 200: {'n_epoch': 6.8195052643088, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 195.63068159055558, 'dropout2': 0.58306699052569}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4205 - acc: 0.1376 - val_loss: 2.3937 - val_acc: 0.1481\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4077 - acc: 0.1438 - val_loss: 2.3910 - val_acc: 0.1550\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4033 - acc: 0.1477 - val_loss: 2.3890 - val_acc: 0.1524\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4002 - acc: 0.1518 - val_loss: 2.3894 - val_acc: 0.1519\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3973 - acc: 0.1525 - val_loss: 2.3873 - val_acc: 0.1572\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3944 - acc: 0.1523 - val_loss: 2.3878 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3877731797812038, 'It took', -156.4499590396881)\n",
      "('Average time per eval:', -112.60143096767254)\n",
      "Params testing 201: {'n_epoch': 6.0196619855486215, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 152.22693937839907, 'dropout2': 0.47652246923185315}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4177 - acc: 0.1414 - val_loss: 2.3900 - val_acc: 0.1532\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4052 - acc: 0.1474 - val_loss: 2.3898 - val_acc: 0.1547\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3998 - acc: 0.1521 - val_loss: 2.3883 - val_acc: 0.1487\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3959 - acc: 0.1522 - val_loss: 2.3857 - val_acc: 0.1590\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3913 - acc: 0.1564 - val_loss: 2.3865 - val_acc: 0.1587\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3900 - acc: 0.1561 - val_loss: 2.3839 - val_acc: 0.1539\n",
      "('Log_loss:', 2.3838554506898406, 'It took', -152.9287359714508)\n",
      "('Average time per eval:', -112.80107108673246)\n",
      "Params testing 202: {'n_epoch': 5.632299204624358, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 124.87681050555187, 'dropout2': 0.46484865022872024}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4183 - acc: 0.1378 - val_loss: 2.3940 - val_acc: 0.1546\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4034 - acc: 0.1493 - val_loss: 2.3874 - val_acc: 0.1558\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3986 - acc: 0.1504 - val_loss: 2.3886 - val_acc: 0.1536\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3950 - acc: 0.1530 - val_loss: 2.3875 - val_acc: 0.1546\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3927 - acc: 0.1540 - val_loss: 2.3875 - val_acc: 0.1550\n",
      "('Log_loss:', 2.3875018088069706, 'It took', -122.9149010181427)\n",
      "('Average time per eval:', -112.85089290905468)\n",
      "Params testing 203: {'n_epoch': 5.494924018419484, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 143.07418456452012, 'dropout2': 0.7033646707934719}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4230 - acc: 0.1372 - val_loss: 2.4042 - val_acc: 0.1459\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4123 - acc: 0.1410 - val_loss: 2.4024 - val_acc: 0.1501\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4089 - acc: 0.1436 - val_loss: 2.4007 - val_acc: 0.1527\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4058 - acc: 0.1468 - val_loss: 2.4017 - val_acc: 0.1491\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4041 - acc: 0.1490 - val_loss: 2.3961 - val_acc: 0.1493\n",
      "('Log_loss:', 2.3961240114649658, 'It took', -123.5650429725647)\n",
      "('Average time per eval:', -112.9034132478284)\n",
      "Params testing 204: {'n_epoch': 6.27629468813435, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 113.9747285096775, 'dropout2': 0.4899099272717734}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4176 - acc: 0.1396 - val_loss: 2.3943 - val_acc: 0.1527\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4045 - acc: 0.1456 - val_loss: 2.3881 - val_acc: 0.1575\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3979 - acc: 0.1506 - val_loss: 2.3862 - val_acc: 0.1562\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3955 - acc: 0.1523 - val_loss: 2.3859 - val_acc: 0.1528\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3928 - acc: 0.1540 - val_loss: 2.3861 - val_acc: 0.1551\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3897 - acc: 0.1555 - val_loss: 2.3849 - val_acc: 0.1599\n",
      "('Log_loss:', 2.3848751566380049, 'It took', -152.75265502929688)\n",
      "('Average time per eval:', -113.09779977798462)\n",
      "Params testing 205: {'n_epoch': 5.736082974419521, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 161.31052698029993, 'dropout2': 0.43484571733136274}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4185 - acc: 0.1413 - val_loss: 2.3934 - val_acc: 0.1544\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4044 - acc: 0.1470 - val_loss: 2.3908 - val_acc: 0.1519\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3999 - acc: 0.1500 - val_loss: 2.3890 - val_acc: 0.1532\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3958 - acc: 0.1528 - val_loss: 2.3856 - val_acc: 0.1531\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3922 - acc: 0.1545 - val_loss: 2.3857 - val_acc: 0.1571\n",
      "('Log_loss:', 2.3856740872054583, 'It took', -124.52259612083435)\n",
      "('Average time per eval:', -113.15325994977674)\n",
      "Params testing 206: {'n_epoch': 6.9189747944016835, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 171.51200439286413, 'dropout2': 0.6040712381659088}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4213 - acc: 0.1386 - val_loss: 2.3934 - val_acc: 0.1507\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4087 - acc: 0.1446 - val_loss: 2.3899 - val_acc: 0.1536\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4029 - acc: 0.1485 - val_loss: 2.3876 - val_acc: 0.1534\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4003 - acc: 0.1508 - val_loss: 2.3885 - val_acc: 0.1563\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3977 - acc: 0.1511 - val_loss: 2.3873 - val_acc: 0.1567\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3955 - acc: 0.1546 - val_loss: 2.3864 - val_acc: 0.1587\n",
      "('Log_loss:', 2.3863984886232537, 'It took', -154.55872797966003)\n",
      "('Average time per eval:', -113.3532863561658)\n",
      "Params testing 207: {'n_epoch': 5.396782667083219, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 148.27669698885032, 'dropout2': 0.5229768626311223}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4198 - acc: 0.1394 - val_loss: 2.3906 - val_acc: 0.1587\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4054 - acc: 0.1482 - val_loss: 2.3883 - val_acc: 0.1546\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4006 - acc: 0.1491 - val_loss: 2.3874 - val_acc: 0.1556\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3974 - acc: 0.1530 - val_loss: 2.3861 - val_acc: 0.1542\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3954 - acc: 0.1533 - val_loss: 2.3851 - val_acc: 0.1543\n",
      "('Log_loss:', 2.3851288159475419, 'It took', -123.46782493591309)\n",
      "('Average time per eval:', -113.40191394090652)\n",
      "Params testing 208: {'n_epoch': 6.608432137971272, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 99.88871445074273, 'dropout2': 0.8726412953291046}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4336 - acc: 0.1303 - val_loss: 2.4015 - val_acc: 0.1426\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4161 - acc: 0.1414 - val_loss: 2.3953 - val_acc: 0.1520\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4119 - acc: 0.1453 - val_loss: 2.3932 - val_acc: 0.1538\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4108 - acc: 0.1440 - val_loss: 2.3951 - val_acc: 0.1499\n",
      "Epoch 5/6\n",
      "28s - loss: 2.4091 - acc: 0.1439 - val_loss: 2.3912 - val_acc: 0.1505\n",
      "Epoch 6/6\n",
      "29s - loss: 2.4101 - acc: 0.1450 - val_loss: 2.3924 - val_acc: 0.1524\n",
      "('Log_loss:', 2.3924458634747983, 'It took', -151.5103669166565)\n",
      "('Average time per eval:', -113.58425103192124)\n",
      "Params testing 209: {'n_epoch': 6.202123407841909, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 107.06002257590406, 'dropout2': 0.5493384879265532}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4189 - acc: 0.1384 - val_loss: 2.3922 - val_acc: 0.1508\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4059 - acc: 0.1473 - val_loss: 2.3896 - val_acc: 0.1479\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3999 - acc: 0.1533 - val_loss: 2.3857 - val_acc: 0.1554\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3964 - acc: 0.1522 - val_loss: 2.3863 - val_acc: 0.1546\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3930 - acc: 0.1549 - val_loss: 2.3845 - val_acc: 0.1560\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3914 - acc: 0.1548 - val_loss: 2.3854 - val_acc: 0.1571\n",
      "('Log_loss:', 2.3854449109707958, 'It took', -151.54930996894836)\n",
      "('Average time per eval:', -113.76503702685946)\n",
      "Params testing 210: {'n_epoch': 6.998687520877126, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 200.87511717678234, 'dropout2': 0.5674898761844476}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4191 - acc: 0.1396 - val_loss: 2.3989 - val_acc: 0.1522\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4102 - acc: 0.1446 - val_loss: 2.4004 - val_acc: 0.1510\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4048 - acc: 0.1485 - val_loss: 2.3977 - val_acc: 0.1556\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4008 - acc: 0.1504 - val_loss: 2.3954 - val_acc: 0.1615\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3992 - acc: 0.1521 - val_loss: 2.3968 - val_acc: 0.1554\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3974 - acc: 0.1531 - val_loss: 2.3990 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3989598236373304, 'It took', -156.90846395492554)\n",
      "('Average time per eval:', -113.96950823887829)\n",
      "Params testing 211: {'n_epoch': 6.726124916206199, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 220.45603707081747, 'dropout2': 0.5040039179865428}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4210 - acc: 0.1403 - val_loss: 2.3937 - val_acc: 0.1406\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4073 - acc: 0.1482 - val_loss: 2.3909 - val_acc: 0.1492\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4031 - acc: 0.1490 - val_loss: 2.3896 - val_acc: 0.1510\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3996 - acc: 0.1496 - val_loss: 2.3876 - val_acc: 0.1562\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3969 - acc: 0.1536 - val_loss: 2.3862 - val_acc: 0.1578\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3939 - acc: 0.1542 - val_loss: 2.3874 - val_acc: 0.1570\n",
      "('Log_loss:', 2.3874244720757631, 'It took', -157.04333305358887)\n",
      "('Average time per eval:', -114.17268665331714)\n",
      "Params testing 212: {'n_epoch': 6.487547151984959, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 188.96527730353245, 'dropout2': 0.6185763022682458}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4225 - acc: 0.1375 - val_loss: 2.3925 - val_acc: 0.1481\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4083 - acc: 0.1476 - val_loss: 2.3901 - val_acc: 0.1555\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4049 - acc: 0.1482 - val_loss: 2.3878 - val_acc: 0.1563\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4011 - acc: 0.1512 - val_loss: 2.3868 - val_acc: 0.1574\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3981 - acc: 0.1512 - val_loss: 2.3850 - val_acc: 0.1583\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3972 - acc: 0.1526 - val_loss: 2.3864 - val_acc: 0.1598\n",
      "('Log_loss:', 2.3864015844562498, 'It took', -156.8056709766388)\n",
      "('Average time per eval:', -114.37284150481784)\n",
      "Params testing 213: {'n_epoch': 6.06855998683075, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 129.68339722768488, 'dropout2': 0.7489516743343934}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4251 - acc: 0.1372 - val_loss: 2.3950 - val_acc: 0.1511\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4119 - acc: 0.1436 - val_loss: 2.3888 - val_acc: 0.1518\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4057 - acc: 0.1472 - val_loss: 2.3881 - val_acc: 0.1510\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4042 - acc: 0.1492 - val_loss: 2.3874 - val_acc: 0.1574\n",
      "Epoch 5/6\n",
      "28s - loss: 2.4016 - acc: 0.1488 - val_loss: 2.3875 - val_acc: 0.1591\n",
      "Epoch 6/6\n",
      "29s - loss: 2.4009 - acc: 0.1495 - val_loss: 2.3875 - val_acc: 0.1594\n",
      "('Log_loss:', 2.3874785208746796, 'It took', -152.50985288619995)\n",
      "('Average time per eval:', -114.55105183391927)\n",
      "Params testing 214: {'n_epoch': 5.916688806618908, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 119.19155089400255, 'dropout2': 0.5384756031948825}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4176 - acc: 0.1393 - val_loss: 2.3936 - val_acc: 0.1483\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4059 - acc: 0.1467 - val_loss: 2.3899 - val_acc: 0.1571\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3997 - acc: 0.1507 - val_loss: 2.3866 - val_acc: 0.1603\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3969 - acc: 0.1523 - val_loss: 2.3865 - val_acc: 0.1544\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3927 - acc: 0.1558 - val_loss: 2.3867 - val_acc: 0.1568\n",
      "('Log_loss:', 2.3866851184025162, 'It took', -122.56444096565247)\n",
      "('Average time per eval:', -114.58832341127618)\n",
      "Params testing 215: {'n_epoch': 6.361910405008863, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 78.7909435736076, 'dropout2': 0.5880283292941599}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4191 - acc: 0.1370 - val_loss: 2.3931 - val_acc: 0.1479\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4045 - acc: 0.1490 - val_loss: 2.3905 - val_acc: 0.1542\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3995 - acc: 0.1502 - val_loss: 2.3872 - val_acc: 0.1554\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3970 - acc: 0.1489 - val_loss: 2.3864 - val_acc: 0.1535\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3932 - acc: 0.1538 - val_loss: 2.3852 - val_acc: 0.1568\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3938 - acc: 0.1535 - val_loss: 2.3859 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3858506986100458, 'It took', -151.1918249130249)\n",
      "('Average time per eval:', -114.7577840619617)\n",
      "Params testing 216: {'n_epoch': 5.826223365337926, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 252.52666067655173, 'dropout2': 0.44817860640189006}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4171 - acc: 0.1389 - val_loss: 2.4042 - val_acc: 0.1548\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4076 - acc: 0.1474 - val_loss: 2.3983 - val_acc: 0.1547\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4024 - acc: 0.1484 - val_loss: 2.3981 - val_acc: 0.1579\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3976 - acc: 0.1513 - val_loss: 2.3969 - val_acc: 0.1507\n",
      "Epoch 5/5\n",
      "30s - loss: 2.3971 - acc: 0.1522 - val_loss: 2.3977 - val_acc: 0.1523\n",
      "('Log_loss:', 2.3977239514256241, 'It took', -128.16273713111877)\n",
      "('Average time per eval:', -114.8195580348441)\n",
      "Params testing 217: {'n_epoch': 5.185678923784651, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 140.27544336329123, 'dropout2': 0.46459962741678223}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4180 - acc: 0.1410 - val_loss: 2.3942 - val_acc: 0.1501\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4046 - acc: 0.1482 - val_loss: 2.3875 - val_acc: 0.1510\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4000 - acc: 0.1512 - val_loss: 2.3859 - val_acc: 0.1589\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3957 - acc: 0.1521 - val_loss: 2.3874 - val_acc: 0.1560\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3929 - acc: 0.1546 - val_loss: 2.3858 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3857873076572513, 'It took', -124.28388404846191)\n",
      "('Average time per eval:', -114.86297237435612)\n",
      "Params testing 218: {'n_epoch': 5.583895890487303, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 210.2131391744603, 'dropout2': 0.4833939803180087}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4199 - acc: 0.1392 - val_loss: 2.3923 - val_acc: 0.1484\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4077 - acc: 0.1446 - val_loss: 2.3895 - val_acc: 0.1566\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4011 - acc: 0.1494 - val_loss: 2.3921 - val_acc: 0.1505\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3981 - acc: 0.1510 - val_loss: 2.3855 - val_acc: 0.1576\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3957 - acc: 0.1519 - val_loss: 2.3880 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3879516242561474, 'It took', -126.66496205329895)\n",
      "('Average time per eval:', -114.91686272947756)\n",
      "Params testing 219: {'n_epoch': 5.417883284899946, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 95.63504716743444, 'dropout2': 0.7263775545954947}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4245 - acc: 0.1366 - val_loss: 2.3963 - val_acc: 0.1455\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4096 - acc: 0.1435 - val_loss: 2.3909 - val_acc: 0.1510\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4046 - acc: 0.1462 - val_loss: 2.3890 - val_acc: 0.1547\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4012 - acc: 0.1495 - val_loss: 2.3866 - val_acc: 0.1593\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3990 - acc: 0.1513 - val_loss: 2.3882 - val_acc: 0.1547\n",
      "('Log_loss:', 2.3881542134661795, 'It took', -122.62400388717651)\n",
      "('Average time per eval:', -114.95189518061551)\n",
      "Params testing 220: {'n_epoch': 5.708851514206592, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 89.01311219544917, 'dropout2': 0.7702413223492806}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4252 - acc: 0.1356 - val_loss: 2.3946 - val_acc: 0.1524\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4110 - acc: 0.1436 - val_loss: 2.3918 - val_acc: 0.1511\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4057 - acc: 0.1468 - val_loss: 2.3889 - val_acc: 0.1531\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4024 - acc: 0.1478 - val_loss: 2.3888 - val_acc: 0.1528\n",
      "Epoch 5/5\n",
      "28s - loss: 2.4026 - acc: 0.1493 - val_loss: 2.3881 - val_acc: 0.1613\n",
      "('Log_loss:', 2.3880838817576766, 'It took', -121.07776999473572)\n",
      "('Average time per eval:', -114.97961406686187)\n",
      "Params testing 221: {'n_epoch': 6.645331777563111, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 135.55706575482034, 'dropout2': 0.6322905921415483}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4219 - acc: 0.1391 - val_loss: 2.3944 - val_acc: 0.1475\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4064 - acc: 0.1467 - val_loss: 2.3888 - val_acc: 0.1527\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4026 - acc: 0.1477 - val_loss: 2.3879 - val_acc: 0.1500\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4001 - acc: 0.1502 - val_loss: 2.3882 - val_acc: 0.1543\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3977 - acc: 0.1522 - val_loss: 2.3879 - val_acc: 0.1575\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3945 - acc: 0.1534 - val_loss: 2.3862 - val_acc: 0.1560\n",
      "('Log_loss:', 2.3862131260816448, 'It took', -153.1429648399353)\n",
      "('Average time per eval:', -115.1515210437345)\n",
      "Params testing 222: {'n_epoch': 5.075135240638338, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 60.354256648368995, 'dropout2': 0.5605831889021133}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4213 - acc: 0.1377 - val_loss: 2.4008 - val_acc: 0.1485\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4072 - acc: 0.1437 - val_loss: 2.3944 - val_acc: 0.1547\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4039 - acc: 0.1483 - val_loss: 2.3934 - val_acc: 0.1497\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4000 - acc: 0.1472 - val_loss: 2.3923 - val_acc: 0.1578\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3970 - acc: 0.1520 - val_loss: 2.3921 - val_acc: 0.1515\n",
      "('Log_loss:', 2.3920705739180206, 'It took', -122.52812790870667)\n",
      "('Average time per eval:', -115.18459999400939)\n",
      "Params testing 223: {'n_epoch': 5.530831751965873, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 113.99571500617655, 'dropout2': 0.41558586784096807}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4166 - acc: 0.1410 - val_loss: 2.3908 - val_acc: 0.1555\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4039 - acc: 0.1469 - val_loss: 2.3876 - val_acc: 0.1491\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3977 - acc: 0.1522 - val_loss: 2.3861 - val_acc: 0.1598\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3930 - acc: 0.1542 - val_loss: 2.3879 - val_acc: 0.1578\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3904 - acc: 0.1584 - val_loss: 2.3858 - val_acc: 0.1551\n",
      "('Log_loss:', 2.3857562862737804, 'It took', -123.21351504325867)\n",
      "('Average time per eval:', -115.22044335944312)\n",
      "Params testing 224: {'n_epoch': 6.861589118360071, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 227.2895559872849, 'dropout2': 0.6962689670329945}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4260 - acc: 0.1379 - val_loss: 2.3969 - val_acc: 0.1422\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4132 - acc: 0.1439 - val_loss: 2.3928 - val_acc: 0.1503\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4082 - acc: 0.1474 - val_loss: 2.3895 - val_acc: 0.1571\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4060 - acc: 0.1477 - val_loss: 2.3884 - val_acc: 0.1544\n",
      "Epoch 5/6\n",
      "30s - loss: 2.4050 - acc: 0.1477 - val_loss: 2.3857 - val_acc: 0.1572\n",
      "Epoch 6/6\n",
      "30s - loss: 2.4015 - acc: 0.1530 - val_loss: 2.3870 - val_acc: 0.1583\n",
      "('Log_loss:', 2.3869740856801922, 'It took', -158.5533549785614)\n",
      "('Average time per eval:', -115.41303407351175)\n",
      "Params testing 225: {'n_epoch': 6.444908444415111, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 84.25759871324021, 'dropout2': 0.5129049657398651}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4181 - acc: 0.1400 - val_loss: 2.3934 - val_acc: 0.1497\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4042 - acc: 0.1488 - val_loss: 2.3895 - val_acc: 0.1523\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3980 - acc: 0.1518 - val_loss: 2.3865 - val_acc: 0.1539\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3953 - acc: 0.1522 - val_loss: 2.3866 - val_acc: 0.1551\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3906 - acc: 0.1533 - val_loss: 2.3855 - val_acc: 0.1575\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3888 - acc: 0.1562 - val_loss: 2.3866 - val_acc: 0.1571\n",
      "('Log_loss:', 2.3866009524926852, 'It took', -151.96578288078308)\n",
      "('Average time per eval:', -115.57477189587281)\n",
      "Params testing 226: {'n_epoch': 6.166857704151093, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 73.2803588774215, 'dropout2': 0.6644936655161725}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4210 - acc: 0.1391 - val_loss: 2.3950 - val_acc: 0.1461\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4071 - acc: 0.1467 - val_loss: 2.3895 - val_acc: 0.1530\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4016 - acc: 0.1496 - val_loss: 2.3873 - val_acc: 0.1555\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3994 - acc: 0.1494 - val_loss: 2.3886 - val_acc: 0.1579\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3974 - acc: 0.1516 - val_loss: 2.3861 - val_acc: 0.1522\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3954 - acc: 0.1540 - val_loss: 2.3851 - val_acc: 0.1574\n",
      "('Log_loss:', 2.385057386319247, 'It took', -151.68130683898926)\n",
      "('Average time per eval:', -115.73383151697168)\n",
      "Params testing 227: {'n_epoch': 6.00006850439539, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 164.83695321395277, 'dropout2': 0.3547260952037014}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4168 - acc: 0.1429 - val_loss: 2.3916 - val_acc: 0.1548\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4035 - acc: 0.1478 - val_loss: 2.3873 - val_acc: 0.1576\n",
      "Epoch 3/6\n",
      "26s - loss: 2.3992 - acc: 0.1510 - val_loss: 2.3880 - val_acc: 0.1542\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3950 - acc: 0.1545 - val_loss: 2.3841 - val_acc: 0.1512\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3914 - acc: 0.1552 - val_loss: 2.3856 - val_acc: 0.1590\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3890 - acc: 0.1576 - val_loss: 2.3875 - val_acc: 0.1552\n",
      "('Log_loss:', 2.3874726431030644, 'It took', -154.52910804748535)\n",
      "('Average time per eval:', -115.90398623441395)\n",
      "Params testing 228: {'n_epoch': 5.2441988014344725, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 101.23265881006921, 'dropout2': 0.39226038565708693}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4162 - acc: 0.1396 - val_loss: 2.3997 - val_acc: 0.1457\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4035 - acc: 0.1478 - val_loss: 2.3953 - val_acc: 0.1546\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3994 - acc: 0.1514 - val_loss: 2.3920 - val_acc: 0.1559\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3948 - acc: 0.1564 - val_loss: 2.3923 - val_acc: 0.1586\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3925 - acc: 0.1563 - val_loss: 2.3914 - val_acc: 0.1544\n",
      "('Log_loss:', 2.3913967496519142, 'It took', -122.81097507476807)\n",
      "('Average time per eval:', -115.93414775252863)\n",
      "Params testing 229: {'n_epoch': 6.1083064171046875, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 157.08678017041592, 'dropout2': 0.4428196319990777}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4177 - acc: 0.1403 - val_loss: 2.3937 - val_acc: 0.1528\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4046 - acc: 0.1486 - val_loss: 2.3901 - val_acc: 0.1530\n",
      "Epoch 3/6\n",
      "26s - loss: 2.3988 - acc: 0.1517 - val_loss: 2.3872 - val_acc: 0.1579\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3953 - acc: 0.1545 - val_loss: 2.3872 - val_acc: 0.1591\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3923 - acc: 0.1558 - val_loss: 2.3869 - val_acc: 0.1562\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3898 - acc: 0.1571 - val_loss: 2.3864 - val_acc: 0.1527\n",
      "('Log_loss:', 2.3864164567970891, 'It took', -154.57783699035645)\n",
      "('Average time per eval:', -116.10216378854669)\n",
      "Params testing 230: {'n_epoch': 4.977563307406209, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 179.9643555345587, 'dropout2': 0.5355400267206454}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4204 - acc: 0.1389 - val_loss: 2.3936 - val_acc: 0.1575\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4070 - acc: 0.1471 - val_loss: 2.3891 - val_acc: 0.1530\n",
      "Epoch 3/4\n",
      "26s - loss: 2.4022 - acc: 0.1503 - val_loss: 2.3863 - val_acc: 0.1586\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3974 - acc: 0.1511 - val_loss: 2.3862 - val_acc: 0.1542\n",
      "('Log_loss:', 2.386164611461429, 'It took', -96.67611002922058)\n",
      "('Average time per eval:', -116.01806831359863)\n",
      "Params testing 231: {'n_epoch': 5.351450301450908, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 68.0322073266645, 'dropout2': 0.6460607547116703}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4211 - acc: 0.1393 - val_loss: 2.3945 - val_acc: 0.1512\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4071 - acc: 0.1455 - val_loss: 2.3916 - val_acc: 0.1481\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4011 - acc: 0.1481 - val_loss: 2.3895 - val_acc: 0.1578\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3978 - acc: 0.1518 - val_loss: 2.3868 - val_acc: 0.1560\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3957 - acc: 0.1518 - val_loss: 2.3874 - val_acc: 0.1511\n",
      "('Log_loss:', 2.3873923869577514, 'It took', -121.94282817840576)\n",
      "('Average time per eval:', -116.04360606649826)\n",
      "Params testing 232: {'n_epoch': 6.7760092183969505, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 56.95802993902608, 'dropout2': 0.5787057860456443}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4197 - acc: 0.1387 - val_loss: 2.3958 - val_acc: 0.1456\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4047 - acc: 0.1473 - val_loss: 2.3905 - val_acc: 0.1510\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4004 - acc: 0.1518 - val_loss: 2.3879 - val_acc: 0.1544\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3953 - acc: 0.1545 - val_loss: 2.3863 - val_acc: 0.1534\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3929 - acc: 0.1545 - val_loss: 2.3853 - val_acc: 0.1571\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3917 - acc: 0.1544 - val_loss: 2.3858 - val_acc: 0.1585\n",
      "('Log_loss:', 2.3857617085400902, 'It took', -151.1586730480194)\n",
      "('Average time per eval:', -116.19431450336276)\n",
      "Params testing 233: {'n_epoch': 4.881460515821111, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 50.31300583272936, 'dropout2': 0.36982397726465743}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4160 - acc: 0.1411 - val_loss: 2.3929 - val_acc: 0.1479\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4007 - acc: 0.1486 - val_loss: 2.3894 - val_acc: 0.1558\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3949 - acc: 0.1527 - val_loss: 2.3856 - val_acc: 0.1536\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3912 - acc: 0.1552 - val_loss: 2.3855 - val_acc: 0.1514\n",
      "('Log_loss:', 2.3855393464591463, 'It took', -92.43512606620789)\n",
      "('Average time per eval:', -116.09277950189052)\n",
      "Params testing 234: {'n_epoch': 5.773812488132708, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 63.362769987225846, 'dropout2': 0.42472344044064914}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4187 - acc: 0.1382 - val_loss: 2.3991 - val_acc: 0.1471\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4050 - acc: 0.1479 - val_loss: 2.3943 - val_acc: 0.1574\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3998 - acc: 0.1521 - val_loss: 2.3912 - val_acc: 0.1548\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3954 - acc: 0.1516 - val_loss: 2.3901 - val_acc: 0.1560\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3936 - acc: 0.1536 - val_loss: 2.3893 - val_acc: 0.1516\n",
      "('Log_loss:', 2.3892612128835116, 'It took', -121.44176602363586)\n",
      "('Average time per eval:', -116.11554114260572)\n",
      "Params testing 235: {'n_epoch': 6.564922264238615, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 81.92509282382625, 'dropout2': 0.6029408722297664}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4195 - acc: 0.1389 - val_loss: 2.3944 - val_acc: 0.1495\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4055 - acc: 0.1477 - val_loss: 2.3896 - val_acc: 0.1542\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4002 - acc: 0.1507 - val_loss: 2.3881 - val_acc: 0.1609\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3965 - acc: 0.1508 - val_loss: 2.3862 - val_acc: 0.1606\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3939 - acc: 0.1537 - val_loss: 2.3847 - val_acc: 0.1610\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3911 - acc: 0.1570 - val_loss: 2.3847 - val_acc: 0.1609\n",
      "('Log_loss:', 2.3846678238690395, 'It took', -152.08026504516602)\n",
      "('Average time per eval:', -116.26793403645694)\n",
      "Params testing 236: {'n_epoch': 6.293243987044894, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 110.57090997992005, 'dropout2': 0.49310662223531754}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4172 - acc: 0.1422 - val_loss: 2.3922 - val_acc: 0.1480\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4039 - acc: 0.1487 - val_loss: 2.3898 - val_acc: 0.1597\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3978 - acc: 0.1508 - val_loss: 2.3872 - val_acc: 0.1520\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3949 - acc: 0.1531 - val_loss: 2.3847 - val_acc: 0.1601\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3930 - acc: 0.1554 - val_loss: 2.3858 - val_acc: 0.1564\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3898 - acc: 0.1583 - val_loss: 2.3852 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3851787249779464, 'It took', -152.6677680015564)\n",
      "('Average time per eval:', -116.42151982975409)\n",
      "Params testing 237: {'n_epoch': 6.3874084040631836, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 125.38619829870643, 'dropout2': 0.4719373261695162}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4175 - acc: 0.1412 - val_loss: 2.3913 - val_acc: 0.1516\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4033 - acc: 0.1469 - val_loss: 2.3872 - val_acc: 0.1567\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3986 - acc: 0.1500 - val_loss: 2.3860 - val_acc: 0.1571\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3954 - acc: 0.1556 - val_loss: 2.3858 - val_acc: 0.1540\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3927 - acc: 0.1560 - val_loss: 2.3859 - val_acc: 0.1562\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3899 - acc: 0.1556 - val_loss: 2.3880 - val_acc: 0.1540\n",
      "('Log_loss:', 2.3880232625086824, 'It took', -153.46499514579773)\n",
      "('Average time per eval:', -116.57716468001614)\n",
      "Params testing 238: {'n_epoch': 5.6268719528565025, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 45.06614003122361, 'dropout2': 0.526804914969795}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4189 - acc: 0.1398 - val_loss: 2.3930 - val_acc: 0.1518\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4041 - acc: 0.1479 - val_loss: 2.3913 - val_acc: 0.1488\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3983 - acc: 0.1517 - val_loss: 2.3870 - val_acc: 0.1574\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3948 - acc: 0.1514 - val_loss: 2.3865 - val_acc: 0.1580\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3926 - acc: 0.1534 - val_loss: 2.3861 - val_acc: 0.1589\n",
      "('Log_loss:', 2.3860568988588264, 'It took', -121.82289505004883)\n",
      "('Average time per eval:', -116.59911333766442)\n",
      "Params testing 239: {'n_epoch': 5.883453783902185, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 147.4945698781184, 'dropout2': 0.62190883811225}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4205 - acc: 0.1376 - val_loss: 2.3939 - val_acc: 0.1567\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4079 - acc: 0.1473 - val_loss: 2.3902 - val_acc: 0.1507\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4027 - acc: 0.1496 - val_loss: 2.3869 - val_acc: 0.1615\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4000 - acc: 0.1518 - val_loss: 2.3847 - val_acc: 0.1597\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3975 - acc: 0.1522 - val_loss: 2.3867 - val_acc: 0.1528\n",
      "('Log_loss:', 2.3867292907293574, 'It took', -124.1576189994812)\n",
      "('Average time per eval:', -116.63060710628828)\n",
      "Params testing 240: {'n_epoch': 6.244805352053843, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 95.27246209889464, 'dropout2': 0.4568298940726714}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4184 - acc: 0.1393 - val_loss: 2.3994 - val_acc: 0.1505\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4058 - acc: 0.1482 - val_loss: 2.3947 - val_acc: 0.1572\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3997 - acc: 0.1504 - val_loss: 2.3935 - val_acc: 0.1567\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3969 - acc: 0.1514 - val_loss: 2.3912 - val_acc: 0.1574\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3950 - acc: 0.1542 - val_loss: 2.3893 - val_acc: 0.1554\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3917 - acc: 0.1544 - val_loss: 2.3898 - val_acc: 0.1590\n",
      "('Log_loss:', 2.3898205225201994, 'It took', -153.22072219848633)\n",
      "('Average time per eval:', -116.7824333050439)\n",
      "Params testing 241: {'n_epoch': 5.155367215623839, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 136.98836806434275, 'dropout2': 0.39988348236713267}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4169 - acc: 0.1425 - val_loss: 2.3942 - val_acc: 0.1575\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4039 - acc: 0.1481 - val_loss: 2.3898 - val_acc: 0.1563\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3980 - acc: 0.1528 - val_loss: 2.3875 - val_acc: 0.1554\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3945 - acc: 0.1529 - val_loss: 2.3875 - val_acc: 0.1579\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3909 - acc: 0.1562 - val_loss: 2.3890 - val_acc: 0.1594\n",
      "('Log_loss:', 2.3890038545413339, 'It took', -123.93845820426941)\n",
      "('Average time per eval:', -116.81200365586714)\n",
      "Params testing 242: {'n_epoch': 5.949654442717648, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 171.9051578120657, 'dropout2': 0.8595993161200424}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4338 - acc: 0.1322 - val_loss: 2.3959 - val_acc: 0.1520\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4191 - acc: 0.1422 - val_loss: 2.3925 - val_acc: 0.1564\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4142 - acc: 0.1428 - val_loss: 2.3914 - val_acc: 0.1540\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4128 - acc: 0.1442 - val_loss: 2.3899 - val_acc: 0.1563\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4119 - acc: 0.1444 - val_loss: 2.3885 - val_acc: 0.1579\n",
      "('Log_loss:', 2.3884659554183627, 'It took', -124.81607484817505)\n",
      "('Average time per eval:', -116.84494221651995)\n",
      "Params testing 243: {'n_epoch': 5.481710325600649, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 105.02637238076635, 'dropout2': 0.7136442682259294}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4227 - acc: 0.1369 - val_loss: 2.3963 - val_acc: 0.1477\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4092 - acc: 0.1451 - val_loss: 2.3901 - val_acc: 0.1563\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4052 - acc: 0.1482 - val_loss: 2.3902 - val_acc: 0.1539\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4007 - acc: 0.1488 - val_loss: 2.3881 - val_acc: 0.1552\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3994 - acc: 0.1499 - val_loss: 2.3873 - val_acc: 0.1589\n",
      "('Log_loss:', 2.3872986689924458, 'It took', -122.00916290283203)\n",
      "('Average time per eval:', -116.86610705148978)\n",
      "Params testing 244: {'n_epoch': 4.770512779408183, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 40.441708662629964, 'dropout2': 0.8058169008113738}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4292 - acc: 0.1325 - val_loss: 2.4005 - val_acc: 0.1430\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4118 - acc: 0.1436 - val_loss: 2.3955 - val_acc: 0.1492\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4076 - acc: 0.1464 - val_loss: 2.3918 - val_acc: 0.1505\n",
      "Epoch 4/4\n",
      "27s - loss: 2.4045 - acc: 0.1485 - val_loss: 2.3907 - val_acc: 0.1511\n",
      "('Log_loss:', 2.3906593993685377, 'It took', -92.96810102462769)\n",
      "('Average time per eval:', -116.76856416585494)\n",
      "Params testing 245: {'n_epoch': 3.0020903017336082, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 33.006346774284346, 'dropout2': 0.33142918399841415}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4157 - acc: 0.1404 - val_loss: 2.3929 - val_acc: 0.1547\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4008 - acc: 0.1490 - val_loss: 2.3903 - val_acc: 0.1531\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3950 - acc: 0.1511 - val_loss: 2.3864 - val_acc: 0.1571\n",
      "('Log_loss:', 2.386399393438686, 'It took', -64.39603400230408)\n",
      "('Average time per eval:', -116.55566769789874)\n",
      "Params testing 246: {'n_epoch': 6.499443451980034, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 89.83343087153489, 'dropout2': 0.547637931605694}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4204 - acc: 0.1388 - val_loss: 2.4031 - val_acc: 0.1547\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4071 - acc: 0.1466 - val_loss: 2.3942 - val_acc: 0.1531\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4019 - acc: 0.1488 - val_loss: 2.3933 - val_acc: 0.1583\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3999 - acc: 0.1504 - val_loss: 2.3959 - val_acc: 0.1570\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3968 - acc: 0.1521 - val_loss: 2.3961 - val_acc: 0.1491\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3955 - acc: 0.1520 - val_loss: 2.3920 - val_acc: 0.1516\n",
      "('Log_loss:', 2.3920451003969307, 'It took', -152.39850497245789)\n",
      "('Average time per eval:', -116.70078039555415)\n",
      "Params testing 247: {'n_epoch': 6.677398735526549, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 78.06459869896875, 'dropout2': 0.34625778161199294}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4151 - acc: 0.1431 - val_loss: 2.3935 - val_acc: 0.1539\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4013 - acc: 0.1489 - val_loss: 2.3874 - val_acc: 0.1546\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3963 - acc: 0.1514 - val_loss: 2.3848 - val_acc: 0.1556\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3906 - acc: 0.1540 - val_loss: 2.3875 - val_acc: 0.1597\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3881 - acc: 0.1584 - val_loss: 2.3875 - val_acc: 0.1542\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3846 - acc: 0.1605 - val_loss: 2.3860 - val_acc: 0.1550\n",
      "('Log_loss:', 2.3859965093423878, 'It took', -151.48604607582092)\n",
      "('Average time per eval:', -116.84104355977428)\n",
      "Params testing 248: {'n_epoch': 5.041590358244771, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 130.36167919583698, 'dropout2': 0.8977337037914124}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4372 - acc: 0.1316 - val_loss: 2.4011 - val_acc: 0.1449\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4210 - acc: 0.1398 - val_loss: 2.3973 - val_acc: 0.1519\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4184 - acc: 0.1425 - val_loss: 2.3941 - val_acc: 0.1515\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4159 - acc: 0.1414 - val_loss: 2.3933 - val_acc: 0.1523\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4159 - acc: 0.1440 - val_loss: 2.3924 - val_acc: 0.1528\n",
      "('Log_loss:', 2.3924472372876138, 'It took', -124.16315007209778)\n",
      "('Average time per eval:', -116.87044960619455)\n",
      "Params testing 249: {'n_epoch': 6.917793451385444, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 117.61489629338776, 'dropout2': 0.4339433129938831}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4173 - acc: 0.1400 - val_loss: 2.3916 - val_acc: 0.1505\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4036 - acc: 0.1480 - val_loss: 2.3884 - val_acc: 0.1619\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3980 - acc: 0.1515 - val_loss: 2.3880 - val_acc: 0.1538\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3940 - acc: 0.1548 - val_loss: 2.3847 - val_acc: 0.1586\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3908 - acc: 0.1564 - val_loss: 2.3871 - val_acc: 0.1540\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3874 - acc: 0.1581 - val_loss: 2.3860 - val_acc: 0.1564\n",
      "('Log_loss:', 2.3859837122107841, 'It took', -153.34091901779175)\n",
      "('Average time per eval:', -117.01633148002624)\n",
      "Params testing 250: {'n_epoch': 5.6856376491342715, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 206.59010949748625, 'dropout2': 0.5094394195988223}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4211 - acc: 0.1409 - val_loss: 2.3939 - val_acc: 0.1480\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4065 - acc: 0.1464 - val_loss: 2.3902 - val_acc: 0.1536\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4035 - acc: 0.1497 - val_loss: 2.3887 - val_acc: 0.1564\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3991 - acc: 0.1524 - val_loss: 2.3887 - val_acc: 0.1515\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3964 - acc: 0.1538 - val_loss: 2.3867 - val_acc: 0.1530\n",
      "('Log_loss:', 2.386703915245374, 'It took', -125.92846298217773)\n",
      "('Average time per eval:', -117.05183797623532)\n",
      "Params testing 251: {'n_epoch': 6.003097516543824, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 54.574410328943216, 'dropout2': 0.6762923629466936}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4215 - acc: 0.1368 - val_loss: 2.3961 - val_acc: 0.1491\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4069 - acc: 0.1444 - val_loss: 2.3902 - val_acc: 0.1534\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4011 - acc: 0.1488 - val_loss: 2.3897 - val_acc: 0.1510\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3998 - acc: 0.1479 - val_loss: 2.3898 - val_acc: 0.1564\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3977 - acc: 0.1508 - val_loss: 2.3861 - val_acc: 0.1515\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3962 - acc: 0.1504 - val_loss: 2.3867 - val_acc: 0.1558\n",
      "('Log_loss:', 2.3867413526944299, 'It took', -150.27156400680542)\n",
      "('Average time per eval:', -117.18366228588043)\n",
      "Params testing 252: {'n_epoch': 4.6280290206512955, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 69.98341108966605, 'dropout2': 0.5695036396191311}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4201 - acc: 0.1382 - val_loss: 2.3979 - val_acc: 0.1436\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4074 - acc: 0.1467 - val_loss: 2.3963 - val_acc: 0.1519\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4030 - acc: 0.1470 - val_loss: 2.3901 - val_acc: 0.1532\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3995 - acc: 0.1509 - val_loss: 2.3930 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3929580488219009, 'It took', -93.919517993927)\n",
      "('Average time per eval:', -117.09170914261709)\n",
      "Params testing 253: {'n_epoch': 3.5098643457029643, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 48.235700951766766, 'dropout2': 0.48128136199734195}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/3\n",
      "12s - loss: 2.4181 - acc: 0.1405 - val_loss: 2.3947 - val_acc: 0.1473\n",
      "Epoch 2/3\n",
      "21s - loss: 2.4040 - acc: 0.1492 - val_loss: 2.3896 - val_acc: 0.1546\n",
      "Epoch 3/3\n",
      "25s - loss: 2.3974 - acc: 0.1521 - val_loss: 2.3868 - val_acc: 0.1585\n",
      "('Log_loss:', 2.3867782856675279, 'It took', -65.2816469669342)\n",
      "('Average time per eval:', -116.88773251236893)\n",
      "Params testing 254: {'n_epoch': 5.807560872725221, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 21.395489914079302, 'dropout2': 0.6129299465221825}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4223 - acc: 0.1399 - val_loss: 2.3980 - val_acc: 0.1443\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4062 - acc: 0.1429 - val_loss: 2.3932 - val_acc: 0.1504\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4021 - acc: 0.1479 - val_loss: 2.3921 - val_acc: 0.1512\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3990 - acc: 0.1490 - val_loss: 2.3888 - val_acc: 0.1540\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3973 - acc: 0.1507 - val_loss: 2.3891 - val_acc: 0.1536\n",
      "('Log_loss:', 2.3891499240022225, 'It took', -120.44083595275879)\n",
      "('Average time per eval:', -116.90166624761095)\n",
      "Params testing 255: {'n_epoch': 4.360110862568329, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 152.21166054576562, 'dropout2': 0.5181655701482449}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4188 - acc: 0.1411 - val_loss: 2.3915 - val_acc: 0.1535\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4067 - acc: 0.1449 - val_loss: 2.3899 - val_acc: 0.1551\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3994 - acc: 0.1508 - val_loss: 2.3864 - val_acc: 0.1570\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3972 - acc: 0.1520 - val_loss: 2.3867 - val_acc: 0.1555\n",
      "('Log_loss:', 2.3866602366008292, 'It took', -95.29756999015808)\n",
      "('Average time per eval:', -116.81727524287999)\n",
      "Params testing 256: {'n_epoch': 6.176834619800184, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 64.00679681388159, 'dropout2': 0.3169221275342045}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4151 - acc: 0.1397 - val_loss: 2.3921 - val_acc: 0.1534\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4011 - acc: 0.1496 - val_loss: 2.3899 - val_acc: 0.1585\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3958 - acc: 0.1521 - val_loss: 2.3855 - val_acc: 0.1587\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3911 - acc: 0.1552 - val_loss: 2.3858 - val_acc: 0.1550\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3867 - acc: 0.1573 - val_loss: 2.3843 - val_acc: 0.1586\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3867 - acc: 0.1595 - val_loss: 2.3845 - val_acc: 0.1580\n",
      "('Log_loss:', 2.3845334472491473, 'It took', -151.66524600982666)\n",
      "('Average time per eval:', -116.95287045616121)\n",
      "Params testing 257: {'n_epoch': 6.325858069444461, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 193.50809953663457, 'dropout2': 0.4722809231908449}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4194 - acc: 0.1397 - val_loss: 2.3927 - val_acc: 0.1536\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4060 - acc: 0.1471 - val_loss: 2.3899 - val_acc: 0.1452\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4010 - acc: 0.1497 - val_loss: 2.3881 - val_acc: 0.1571\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3976 - acc: 0.1514 - val_loss: 2.3887 - val_acc: 0.1536\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3942 - acc: 0.1529 - val_loss: 2.3843 - val_acc: 0.1587\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3920 - acc: 0.1569 - val_loss: 2.3885 - val_acc: 0.1530\n",
      "('Log_loss:', 2.3884557273698945, 'It took', -156.76078510284424)\n",
      "('Average time per eval:', -117.1071646952814)\n",
      "Params testing 258: {'n_epoch': 6.429214452920501, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 162.28414739701577, 'dropout2': 0.49903188952982436}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4186 - acc: 0.1383 - val_loss: 2.3962 - val_acc: 0.1452\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4051 - acc: 0.1486 - val_loss: 2.3897 - val_acc: 0.1579\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4010 - acc: 0.1505 - val_loss: 2.3868 - val_acc: 0.1544\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3969 - acc: 0.1514 - val_loss: 2.3877 - val_acc: 0.1575\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3948 - acc: 0.1542 - val_loss: 2.3862 - val_acc: 0.1550\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3901 - acc: 0.1566 - val_loss: 2.3863 - val_acc: 0.1574\n",
      "('Log_loss:', 2.3862580396823772, 'It took', -155.15095281600952)\n",
      "('Average time per eval:', -117.25405190442059)\n",
      "Params testing 259: {'n_epoch': 6.039479912716278, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 182.52550407928695, 'dropout2': 0.5589348700216435}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4206 - acc: 0.1385 - val_loss: 2.3923 - val_acc: 0.1507\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4074 - acc: 0.1465 - val_loss: 2.3906 - val_acc: 0.1540\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4023 - acc: 0.1475 - val_loss: 2.3876 - val_acc: 0.1605\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3986 - acc: 0.1509 - val_loss: 2.3856 - val_acc: 0.1572\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3969 - acc: 0.1529 - val_loss: 2.3854 - val_acc: 0.1599\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3937 - acc: 0.1532 - val_loss: 2.3854 - val_acc: 0.1532\n",
      "('Log_loss:', 2.3854280380023698, 'It took', -157.44080519676208)\n",
      "('Average time per eval:', -117.4086163355754)\n",
      "Params testing 260: {'n_epoch': 6.132042317898748, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 151.6193642010956, 'dropout2': 0.5861465973682773}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4207 - acc: 0.1370 - val_loss: 2.3960 - val_acc: 0.1465\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4073 - acc: 0.1460 - val_loss: 2.3907 - val_acc: 0.1567\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4016 - acc: 0.1483 - val_loss: 2.3890 - val_acc: 0.1508\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3986 - acc: 0.1513 - val_loss: 2.3863 - val_acc: 0.1567\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3954 - acc: 0.1532 - val_loss: 2.3848 - val_acc: 0.1534\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3942 - acc: 0.1524 - val_loss: 2.3837 - val_acc: 0.1603\n",
      "('Log_loss:', 2.3837045710079798, 'It took', -154.88755011558533)\n",
      "('Average time per eval:', -117.55221377935446)\n",
      "Params testing 261: {'n_epoch': 6.127033862101504, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 176.3960885375705, 'dropout2': 0.6500374696670392}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4220 - acc: 0.1379 - val_loss: 2.3928 - val_acc: 0.1510\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4084 - acc: 0.1448 - val_loss: 2.3912 - val_acc: 0.1571\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4048 - acc: 0.1491 - val_loss: 2.3894 - val_acc: 0.1566\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4022 - acc: 0.1514 - val_loss: 2.3859 - val_acc: 0.1586\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3987 - acc: 0.1517 - val_loss: 2.3888 - val_acc: 0.1579\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3977 - acc: 0.1537 - val_loss: 2.3884 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3883689068579144, 'It took', -155.57960295677185)\n",
      "('Average time per eval:', -117.69735648249852)\n",
      "Params testing 262: {'n_epoch': 6.777512384147161, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 142.44323655604143, 'dropout2': 0.6375662466820559}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4236 - acc: 0.1362 - val_loss: 2.3942 - val_acc: 0.1475\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4078 - acc: 0.1458 - val_loss: 2.3898 - val_acc: 0.1540\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4032 - acc: 0.1483 - val_loss: 2.3886 - val_acc: 0.1562\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3995 - acc: 0.1540 - val_loss: 2.3852 - val_acc: 0.1601\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3975 - acc: 0.1528 - val_loss: 2.3875 - val_acc: 0.1583\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3957 - acc: 0.1542 - val_loss: 2.3840 - val_acc: 0.1583\n",
      "('Log_loss:', 2.3839776022741437, 'It took', -154.00642704963684)\n",
      "('Average time per eval:', -117.83541378050249)\n",
      "Params testing 263: {'n_epoch': 6.236372964236302, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 166.51384698469377, 'dropout2': 0.5884836038322546}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4200 - acc: 0.1387 - val_loss: 2.3933 - val_acc: 0.1550\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4077 - acc: 0.1465 - val_loss: 2.3880 - val_acc: 0.1554\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4030 - acc: 0.1500 - val_loss: 2.3880 - val_acc: 0.1563\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3991 - acc: 0.1522 - val_loss: 2.3881 - val_acc: 0.1580\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3971 - acc: 0.1527 - val_loss: 2.3854 - val_acc: 0.1576\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3948 - acc: 0.1550 - val_loss: 2.3859 - val_acc: 0.1539\n",
      "('Log_loss:', 2.3859332135366813, 'It took', -154.95157313346863)\n",
      "('Average time per eval:', -117.97600528868762)\n",
      "Params testing 264: {'n_epoch': 6.580597576529606, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 156.8369597446365, 'dropout2': 0.537376740066091}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4202 - acc: 0.1402 - val_loss: 2.3931 - val_acc: 0.1540\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4072 - acc: 0.1476 - val_loss: 2.3887 - val_acc: 0.1550\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4022 - acc: 0.1478 - val_loss: 2.3887 - val_acc: 0.1579\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3972 - acc: 0.1537 - val_loss: 2.3895 - val_acc: 0.1582\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3939 - acc: 0.1549 - val_loss: 2.3864 - val_acc: 0.1591\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3932 - acc: 0.1579 - val_loss: 2.3882 - val_acc: 0.1563\n",
      "('Log_loss:', 2.3882387997220804, 'It took', -155.62975406646729)\n",
      "('Average time per eval:', -118.1180949031182)\n",
      "Params testing 265: {'n_epoch': 5.934473704340262, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 121.67829341253447, 'dropout2': 0.5839405621511213}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4196 - acc: 0.1378 - val_loss: 2.3920 - val_acc: 0.1481\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4062 - acc: 0.1479 - val_loss: 2.3882 - val_acc: 0.1574\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4010 - acc: 0.1471 - val_loss: 2.3884 - val_acc: 0.1554\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3968 - acc: 0.1525 - val_loss: 2.3867 - val_acc: 0.1575\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3952 - acc: 0.1534 - val_loss: 2.3846 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3846007521977031, 'It took', -123.31519412994385)\n",
      "('Average time per eval:', -118.1376328656548)\n",
      "Params testing 266: {'n_epoch': 5.845058984593365, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 148.65398285933793, 'dropout2': 0.6070971668412268}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4209 - acc: 0.1368 - val_loss: 2.3945 - val_acc: 0.1479\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4069 - acc: 0.1471 - val_loss: 2.3884 - val_acc: 0.1567\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4027 - acc: 0.1509 - val_loss: 2.3879 - val_acc: 0.1574\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3999 - acc: 0.1515 - val_loss: 2.3878 - val_acc: 0.1542\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3974 - acc: 0.1543 - val_loss: 2.3867 - val_acc: 0.1603\n",
      "('Log_loss:', 2.3867281209263314, 'It took', -124.44772005081177)\n",
      "('Average time per eval:', -118.1612661465277)\n",
      "Params testing 267: {'n_epoch': 6.335248333274344, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 187.17878275204913, 'dropout2': 0.6601459211224225}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4232 - acc: 0.1356 - val_loss: 2.3943 - val_acc: 0.1511\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4107 - acc: 0.1439 - val_loss: 2.3901 - val_acc: 0.1518\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4072 - acc: 0.1477 - val_loss: 2.3872 - val_acc: 0.1547\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4028 - acc: 0.1479 - val_loss: 2.3878 - val_acc: 0.1534\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3996 - acc: 0.1525 - val_loss: 2.3849 - val_acc: 0.1568\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3988 - acc: 0.1513 - val_loss: 2.3855 - val_acc: 0.1575\n",
      "('Log_loss:', 2.3854851495986518, 'It took', -155.87918996810913)\n",
      "('Average time per eval:', -118.30200466024341)\n",
      "Params testing 268: {'n_epoch': 5.715885362056096, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 131.98121342625478, 'dropout2': 0.5550255245568061}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4195 - acc: 0.1399 - val_loss: 2.3940 - val_acc: 0.1477\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4061 - acc: 0.1458 - val_loss: 2.3894 - val_acc: 0.1576\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4004 - acc: 0.1495 - val_loss: 2.3898 - val_acc: 0.1504\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3980 - acc: 0.1514 - val_loss: 2.3882 - val_acc: 0.1554\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3943 - acc: 0.1547 - val_loss: 2.3862 - val_acc: 0.1563\n",
      "('Log_loss:', 2.3861572062908531, 'It took', -123.58066487312317)\n",
      "('Average time per eval:', -118.321627928865)\n",
      "Params testing 269: {'n_epoch': 6.501459738000587, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 141.7772899878685, 'dropout2': 0.6885706497939957}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4226 - acc: 0.1383 - val_loss: 2.3938 - val_acc: 0.1495\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4093 - acc: 0.1457 - val_loss: 2.3904 - val_acc: 0.1476\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4042 - acc: 0.1468 - val_loss: 2.3876 - val_acc: 0.1551\n",
      "Epoch 4/6\n",
      "27s - loss: 2.4023 - acc: 0.1503 - val_loss: 2.3854 - val_acc: 0.1548\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3995 - acc: 0.1533 - val_loss: 2.3863 - val_acc: 0.1578\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3984 - acc: 0.1500 - val_loss: 2.3843 - val_acc: 0.1550\n",
      "('Log_loss:', 2.3843463811291916, 'It took', -154.2039451599121)\n",
      "('Average time per eval:', -118.45452539567594)\n",
      "Params testing 270: {'n_epoch': 6.076932677324246, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 156.03127250311343, 'dropout2': 0.5954139529310086}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4202 - acc: 0.1387 - val_loss: 2.3929 - val_acc: 0.1563\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4071 - acc: 0.1479 - val_loss: 2.3899 - val_acc: 0.1555\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4022 - acc: 0.1515 - val_loss: 2.3898 - val_acc: 0.1591\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3986 - acc: 0.1518 - val_loss: 2.3871 - val_acc: 0.1585\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3940 - acc: 0.1529 - val_loss: 2.3868 - val_acc: 0.1558\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3940 - acc: 0.1544 - val_loss: 2.3867 - val_acc: 0.1567\n",
      "('Log_loss:', 2.3866636931592837, 'It took', -156.05716013908386)\n",
      "('Average time per eval:', -118.59328050191112)\n",
      "Params testing 271: {'n_epoch': 5.5208192991362885, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 139.7131464581238, 'dropout2': 0.5253640228999453}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4180 - acc: 0.1396 - val_loss: 2.3930 - val_acc: 0.1488\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4065 - acc: 0.1462 - val_loss: 2.3898 - val_acc: 0.1546\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4007 - acc: 0.1493 - val_loss: 2.3852 - val_acc: 0.1572\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3965 - acc: 0.1526 - val_loss: 2.3857 - val_acc: 0.1528\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3936 - acc: 0.1543 - val_loss: 2.3850 - val_acc: 0.1551\n",
      "('Log_loss:', 2.3850371149440184, 'It took', -123.8867130279541)\n",
      "('Average time per eval:', -118.61274164739777)\n",
      "Params testing 272: {'n_epoch': 6.71891807384575, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 126.80584779534087, 'dropout2': 0.5712860544677816}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4200 - acc: 0.1396 - val_loss: 2.3942 - val_acc: 0.1550\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4062 - acc: 0.1476 - val_loss: 2.3924 - val_acc: 0.1546\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4002 - acc: 0.1494 - val_loss: 2.3882 - val_acc: 0.1586\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3980 - acc: 0.1515 - val_loss: 2.3854 - val_acc: 0.1543\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3939 - acc: 0.1554 - val_loss: 2.3856 - val_acc: 0.1602\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3924 - acc: 0.1556 - val_loss: 2.3863 - val_acc: 0.1559\n",
      "('Log_loss:', 2.3862582863638555, 'It took', -154.17580795288086)\n",
      "('Average time per eval:', -118.74300928604909)\n",
      "Params testing 273: {'n_epoch': 5.611933265278789, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 171.44515676683926, 'dropout2': 0.5444204170787909}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4200 - acc: 0.1367 - val_loss: 2.3934 - val_acc: 0.1500\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4072 - acc: 0.1466 - val_loss: 2.3887 - val_acc: 0.1532\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4005 - acc: 0.1497 - val_loss: 2.3883 - val_acc: 0.1572\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3976 - acc: 0.1525 - val_loss: 2.3875 - val_acc: 0.1540\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3952 - acc: 0.1537 - val_loss: 2.3845 - val_acc: 0.1618\n",
      "('Log_loss:', 2.3845059765493328, 'It took', -125.59023714065552)\n",
      "('Average time per eval:', -118.76799916524956)\n",
      "Params testing 274: {'n_epoch': 6.622984209459512, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 199.97473133305851, 'dropout2': 0.6261294784053155}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4226 - acc: 0.1386 - val_loss: 2.3932 - val_acc: 0.1528\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4097 - acc: 0.1442 - val_loss: 2.3918 - val_acc: 0.1582\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4050 - acc: 0.1495 - val_loss: 2.3868 - val_acc: 0.1571\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4021 - acc: 0.1503 - val_loss: 2.3883 - val_acc: 0.1582\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3994 - acc: 0.1513 - val_loss: 2.3863 - val_acc: 0.1595\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3965 - acc: 0.1523 - val_loss: 2.3862 - val_acc: 0.1536\n",
      "('Log_loss:', 2.3861808040299812, 'It took', -156.5150179862976)\n",
      "('Average time per eval:', -118.90526104753668)\n",
      "Params testing 275: {'n_epoch': 6.186269104616727, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 153.1126199024682, 'dropout2': 0.45930598200140765}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4189 - acc: 0.1401 - val_loss: 2.3926 - val_acc: 0.1530\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4044 - acc: 0.1488 - val_loss: 2.3901 - val_acc: 0.1536\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3983 - acc: 0.1532 - val_loss: 2.3871 - val_acc: 0.1563\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3966 - acc: 0.1524 - val_loss: 2.3862 - val_acc: 0.1556\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3933 - acc: 0.1525 - val_loss: 2.3867 - val_acc: 0.1522\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3898 - acc: 0.1584 - val_loss: 2.3862 - val_acc: 0.1570\n",
      "('Log_loss:', 2.3862056569634129, 'It took', -155.82109689712524)\n",
      "('Average time per eval:', -119.03901407252188)\n",
      "Params testing 276: {'n_epoch': 5.317695866370732, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 241.22094282705984, 'dropout2': 0.7269091163144279}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "14s - loss: 2.4287 - acc: 0.1349 - val_loss: 2.3944 - val_acc: 0.1519\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4142 - acc: 0.1442 - val_loss: 2.3919 - val_acc: 0.1476\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4097 - acc: 0.1483 - val_loss: 2.3886 - val_acc: 0.1558\n",
      "Epoch 4/5\n",
      "28s - loss: 2.4080 - acc: 0.1463 - val_loss: 2.3867 - val_acc: 0.1556\n",
      "Epoch 5/5\n",
      "29s - loss: 2.4053 - acc: 0.1484 - val_loss: 2.3879 - val_acc: 0.1568\n",
      "('Log_loss:', 2.387856759767637, 'It took', -129.39192295074463)\n",
      "('Average time per eval:', -119.07638919138306)\n",
      "Params testing 277: {'n_epoch': 6.416374872649349, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 160.9486046886189, 'dropout2': 0.4089492740152379}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4189 - acc: 0.1411 - val_loss: 2.3951 - val_acc: 0.1496\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4038 - acc: 0.1482 - val_loss: 2.3929 - val_acc: 0.1500\n",
      "Epoch 3/6\n",
      "26s - loss: 2.3991 - acc: 0.1511 - val_loss: 2.3894 - val_acc: 0.1548\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3951 - acc: 0.1530 - val_loss: 2.3876 - val_acc: 0.1578\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3919 - acc: 0.1550 - val_loss: 2.3846 - val_acc: 0.1580\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3888 - acc: 0.1579 - val_loss: 2.3873 - val_acc: 0.1609\n",
      "('Log_loss:', 2.3872991137519906, 'It took', -155.44401121139526)\n",
      "('Average time per eval:', -119.2072079713396)\n",
      "Params testing 278: {'n_epoch': 5.41960667470004, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 110.68153477223399, 'dropout2': 0.499421530806484}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4179 - acc: 0.1401 - val_loss: 2.3922 - val_acc: 0.1524\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4033 - acc: 0.1476 - val_loss: 2.3899 - val_acc: 0.1540\n",
      "Epoch 3/5\n",
      "26s - loss: 2.3985 - acc: 0.1502 - val_loss: 2.3864 - val_acc: 0.1585\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3957 - acc: 0.1517 - val_loss: 2.3866 - val_acc: 0.1567\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3920 - acc: 0.1533 - val_loss: 2.3846 - val_acc: 0.1528\n",
      "('Log_loss:', 2.384585379899554, 'It took', -124.30758285522461)\n",
      "('Average time per eval:', -119.22548888148373)\n",
      "Params testing 279: {'n_epoch': 6.8682637289750295, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 178.68467795809585, 'dropout2': 0.7025329642469569}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4234 - acc: 0.1392 - val_loss: 2.3965 - val_acc: 0.1416\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4110 - acc: 0.1443 - val_loss: 2.3907 - val_acc: 0.1515\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4076 - acc: 0.1465 - val_loss: 2.3913 - val_acc: 0.1488\n",
      "Epoch 4/6\n",
      "28s - loss: 2.4031 - acc: 0.1502 - val_loss: 2.3879 - val_acc: 0.1550\n",
      "Epoch 5/6\n",
      "29s - loss: 2.4019 - acc: 0.1482 - val_loss: 2.3877 - val_acc: 0.1587\n",
      "Epoch 6/6\n",
      "30s - loss: 2.4002 - acc: 0.1518 - val_loss: 2.3852 - val_acc: 0.1576\n",
      "('Log_loss:', 2.3852414350518587, 'It took', -155.76213097572327)\n",
      "('Average time per eval:', -119.3559768821512)\n",
      "Params testing 280: {'n_epoch': 6.282637182007075, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 134.87230140961634, 'dropout2': 0.5767460124857594}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4205 - acc: 0.1376 - val_loss: 2.3913 - val_acc: 0.1540\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4074 - acc: 0.1473 - val_loss: 2.3876 - val_acc: 0.1515\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4028 - acc: 0.1483 - val_loss: 2.3877 - val_acc: 0.1571\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3970 - acc: 0.1524 - val_loss: 2.3862 - val_acc: 0.1579\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3955 - acc: 0.1533 - val_loss: 2.3854 - val_acc: 0.1560\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3929 - acc: 0.1549 - val_loss: 2.3867 - val_acc: 0.1542\n",
      "('Log_loss:', 2.386696379305433, 'It took', -153.69445300102234)\n",
      "('Average time per eval:', -119.47817786138677)\n",
      "Params testing 281: {'n_epoch': 5.9782828951452895, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 99.64331652398448, 'dropout2': 0.6719915079568851}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4238 - acc: 0.1366 - val_loss: 2.4017 - val_acc: 0.1524\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4102 - acc: 0.1447 - val_loss: 2.3999 - val_acc: 0.1562\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4065 - acc: 0.1467 - val_loss: 2.3934 - val_acc: 0.1552\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4039 - acc: 0.1475 - val_loss: 2.3988 - val_acc: 0.1514\n",
      "Epoch 5/5\n",
      "28s - loss: 2.4024 - acc: 0.1480 - val_loss: 2.3964 - val_acc: 0.1550\n",
      "('Log_loss:', 2.396351920210769, 'It took', -122.77284693717957)\n",
      "('Average time per eval:', -119.48986108167797)\n",
      "Params testing 282: {'n_epoch': 5.877304801053051, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 146.10934666801677, 'dropout2': 0.44589495041165317}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4176 - acc: 0.1394 - val_loss: 2.3941 - val_acc: 0.1520\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4047 - acc: 0.1466 - val_loss: 2.3923 - val_acc: 0.1547\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3989 - acc: 0.1508 - val_loss: 2.3885 - val_acc: 0.1578\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3947 - acc: 0.1529 - val_loss: 2.3862 - val_acc: 0.1572\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3922 - acc: 0.1537 - val_loss: 2.3864 - val_acc: 0.1520\n",
      "('Log_loss:', 2.3864467957688502, 'It took', -124.27755999565125)\n",
      "('Average time per eval:', -119.50677874231506)\n",
      "Params testing 283: {'n_epoch': 5.766763503193622, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 167.57941405171707, 'dropout2': 0.4863250521431768}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4190 - acc: 0.1381 - val_loss: 2.3943 - val_acc: 0.1477\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4056 - acc: 0.1475 - val_loss: 2.3896 - val_acc: 0.1522\n",
      "Epoch 3/5\n",
      "26s - loss: 2.4005 - acc: 0.1519 - val_loss: 2.3867 - val_acc: 0.1599\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3973 - acc: 0.1507 - val_loss: 2.3867 - val_acc: 0.1589\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3938 - acc: 0.1555 - val_loss: 2.3879 - val_acc: 0.1575\n",
      "('Log_loss:', 2.3879489134503151, 'It took', -125.85706400871277)\n",
      "('Average time per eval:', -119.52913889834579)\n",
      "Params testing 284: {'n_epoch': 6.122914429092801, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 117.47307263154757, 'dropout2': 0.5093196914717532}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4169 - acc: 0.1410 - val_loss: 2.3924 - val_acc: 0.1531\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4058 - acc: 0.1475 - val_loss: 2.3909 - val_acc: 0.1572\n",
      "Epoch 3/6\n",
      "25s - loss: 2.3995 - acc: 0.1502 - val_loss: 2.3881 - val_acc: 0.1576\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3965 - acc: 0.1520 - val_loss: 2.3861 - val_acc: 0.1552\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3922 - acc: 0.1542 - val_loss: 2.3857 - val_acc: 0.1593\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3899 - acc: 0.1583 - val_loss: 2.3862 - val_acc: 0.1567\n",
      "('Log_loss:', 2.3862361001355255, 'It took', -154.27744007110596)\n",
      "('Average time per eval:', -119.65106275876363)\n",
      "Params testing 285: {'n_epoch': 4.8142141746797815, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 37.790970431760485, 'dropout2': 0.3796336427014445}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4165 - acc: 0.1395 - val_loss: 2.3942 - val_acc: 0.1484\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4022 - acc: 0.1480 - val_loss: 2.3895 - val_acc: 0.1518\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3960 - acc: 0.1520 - val_loss: 2.3868 - val_acc: 0.1568\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3915 - acc: 0.1533 - val_loss: 2.3880 - val_acc: 0.1576\n",
      "('Log_loss:', 2.3879830848905632, 'It took', -93.15441513061523)\n",
      "('Average time per eval:', -119.55841713351803)\n",
      "Params testing 286: {'n_epoch': 4.237004607508361, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 16.445603935146284, 'dropout2': 0.6326679303354292}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4237 - acc: 0.1365 - val_loss: 2.3992 - val_acc: 0.1443\n",
      "Epoch 2/4\n",
      "21s - loss: 2.4074 - acc: 0.1455 - val_loss: 2.3924 - val_acc: 0.1531\n",
      "Epoch 3/4\n",
      "25s - loss: 2.4028 - acc: 0.1456 - val_loss: 2.3920 - val_acc: 0.1555\n",
      "Epoch 4/4\n",
      "27s - loss: 2.4003 - acc: 0.1488 - val_loss: 2.3895 - val_acc: 0.1548\n",
      "('Log_loss:', 2.3894521909075577, 'It took', -92.22158598899841)\n",
      "('Average time per eval:', -119.46316684746161)\n",
      "Params testing 287: {'n_epoch': 4.4698490940114075, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 124.28463395721363, 'dropout2': 0.5995246527683296}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4204 - acc: 0.1398 - val_loss: 2.3931 - val_acc: 0.1500\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4069 - acc: 0.1465 - val_loss: 2.3909 - val_acc: 0.1555\n",
      "Epoch 3/4\n",
      "26s - loss: 2.4016 - acc: 0.1497 - val_loss: 2.3869 - val_acc: 0.1582\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3980 - acc: 0.1512 - val_loss: 2.3858 - val_acc: 0.1602\n",
      "('Log_loss:', 2.3858166190755932, 'It took', -94.89737200737)\n",
      "('Average time per eval:', -119.37786894539992)\n",
      "Params testing 288: {'n_epoch': 6.673804742694295, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 30.426584025110643, 'dropout2': 0.5552763454388752}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4221 - acc: 0.1367 - val_loss: 2.3996 - val_acc: 0.1451\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4083 - acc: 0.1451 - val_loss: 2.3948 - val_acc: 0.1495\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4039 - acc: 0.1466 - val_loss: 2.3933 - val_acc: 0.1488\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3999 - acc: 0.1485 - val_loss: 2.3938 - val_acc: 0.1505\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3995 - acc: 0.1508 - val_loss: 2.3922 - val_acc: 0.1534\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3971 - acc: 0.1514 - val_loss: 2.3917 - val_acc: 0.1539\n",
      "('Log_loss:', 2.3917065337040837, 'It took', -150.281564950943)\n",
      "('Average time per eval:', -119.48480214297153)\n",
      "Params testing 289: {'n_epoch': 6.48881781225266, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 74.07310028599538, 'dropout2': 0.6564214644034714}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4221 - acc: 0.1368 - val_loss: 2.3931 - val_acc: 0.1539\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4063 - acc: 0.1454 - val_loss: 2.3901 - val_acc: 0.1562\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4016 - acc: 0.1478 - val_loss: 2.3873 - val_acc: 0.1590\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3994 - acc: 0.1493 - val_loss: 2.3884 - val_acc: 0.1579\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3976 - acc: 0.1515 - val_loss: 2.3868 - val_acc: 0.1613\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3945 - acc: 0.1529 - val_loss: 2.3853 - val_acc: 0.1556\n",
      "('Log_loss:', 2.3853116921946795, 'It took', -152.1712589263916)\n",
      "('Average time per eval:', -119.59751405962582)\n",
      "Params testing 290: {'n_epoch': 6.964196388605964, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 105.7388492312624, 'dropout2': 0.5274457943372531}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4187 - acc: 0.1385 - val_loss: 2.3925 - val_acc: 0.1477\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4043 - acc: 0.1471 - val_loss: 2.3888 - val_acc: 0.1534\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4005 - acc: 0.1491 - val_loss: 2.3870 - val_acc: 0.1560\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3945 - acc: 0.1551 - val_loss: 2.3867 - val_acc: 0.1571\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3922 - acc: 0.1565 - val_loss: 2.3873 - val_acc: 0.1567\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3900 - acc: 0.1562 - val_loss: 2.3857 - val_acc: 0.1578\n",
      "('Log_loss:', 2.3856564753023002, 'It took', -152.46781396865845)\n",
      "('Average time per eval:', -119.7104704134243)\n",
      "Params testing 291: {'n_epoch': 5.136489151063462, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 56.97906186427396, 'dropout2': 0.5380855223608814}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4194 - acc: 0.1391 - val_loss: 2.3926 - val_acc: 0.1536\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4032 - acc: 0.1467 - val_loss: 2.3894 - val_acc: 0.1586\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3992 - acc: 0.1510 - val_loss: 2.3866 - val_acc: 0.1570\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3949 - acc: 0.1523 - val_loss: 2.3858 - val_acc: 0.1586\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3932 - acc: 0.1526 - val_loss: 2.3852 - val_acc: 0.1564\n",
      "('Log_loss:', 2.3851957312385168, 'It took', -121.07545590400696)\n",
      "('Average time per eval:', -119.71514501800276)\n",
      "Params testing 292: {'n_epoch': 5.259653797111634, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 174.99266919247492, 'dropout2': 0.4188919038702991}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "13s - loss: 2.4184 - acc: 0.1375 - val_loss: 2.3949 - val_acc: 0.1472\n",
      "Epoch 2/5\n",
      "22s - loss: 2.4050 - acc: 0.1469 - val_loss: 2.3880 - val_acc: 0.1523\n",
      "Epoch 3/5\n",
      "26s - loss: 2.3994 - acc: 0.1495 - val_loss: 2.3897 - val_acc: 0.1564\n",
      "Epoch 4/5\n",
      "28s - loss: 2.3965 - acc: 0.1534 - val_loss: 2.3858 - val_acc: 0.1554\n",
      "Epoch 5/5\n",
      "29s - loss: 2.3922 - acc: 0.1558 - val_loss: 2.3844 - val_acc: 0.1551\n",
      "('Log_loss:', 2.3844303871299637, 'It took', -126.49326920509338)\n",
      "('Average time per eval:', -119.73827854358295)\n",
      "Params testing 293: {'n_epoch': 6.059050521827341, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 86.37886544499597, 'dropout2': 0.6159351819939436}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "12s - loss: 2.4207 - acc: 0.1395 - val_loss: 2.3971 - val_acc: 0.1531\n",
      "Epoch 2/6\n",
      "21s - loss: 2.4070 - acc: 0.1471 - val_loss: 2.3922 - val_acc: 0.1556\n",
      "Epoch 3/6\n",
      "25s - loss: 2.4003 - acc: 0.1491 - val_loss: 2.3862 - val_acc: 0.1570\n",
      "Epoch 4/6\n",
      "27s - loss: 2.3975 - acc: 0.1519 - val_loss: 2.3875 - val_acc: 0.1526\n",
      "Epoch 5/6\n",
      "28s - loss: 2.3953 - acc: 0.1521 - val_loss: 2.3860 - val_acc: 0.1567\n",
      "Epoch 6/6\n",
      "29s - loss: 2.3946 - acc: 0.1535 - val_loss: 2.3839 - val_acc: 0.1585\n",
      "('Log_loss:', 2.3839255330104661, 'It took', -152.052551984787)\n",
      "('Average time per eval:', -119.84819103828093)\n",
      "Params testing 294: {'n_epoch': 4.930900610012465, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'relu', 'units2': 191.10322322056933, 'dropout2': 0.4912198705628441}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "13s - loss: 2.4176 - acc: 0.1400 - val_loss: 2.3943 - val_acc: 0.1530\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4068 - acc: 0.1469 - val_loss: 2.3979 - val_acc: 0.1566\n",
      "Epoch 3/4\n",
      "26s - loss: 2.4008 - acc: 0.1492 - val_loss: 2.3974 - val_acc: 0.1564\n",
      "Epoch 4/4\n",
      "28s - loss: 2.3984 - acc: 0.1537 - val_loss: 2.3930 - val_acc: 0.1528\n",
      "('Log_loss:', 2.3930372618939133, 'It took', -96.4001100063324)\n",
      "('Average time per eval:', -119.76870601460085)\n",
      "Params testing 295: {'n_epoch': 5.665563299091516, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 95.60012235410798, 'dropout2': 0.7466287208717897}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4234 - acc: 0.1383 - val_loss: 2.3948 - val_acc: 0.1487\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4100 - acc: 0.1445 - val_loss: 2.3903 - val_acc: 0.1531\n",
      "Epoch 3/5\n",
      "25s - loss: 2.4050 - acc: 0.1473 - val_loss: 2.3894 - val_acc: 0.1560\n",
      "Epoch 4/5\n",
      "27s - loss: 2.4019 - acc: 0.1475 - val_loss: 2.3899 - val_acc: 0.1571\n",
      "Epoch 5/5\n",
      "28s - loss: 2.4005 - acc: 0.1501 - val_loss: 2.3868 - val_acc: 0.1575\n",
      "('Log_loss:', 2.3868405889672824, 'It took', -122.54738807678223)\n",
      "('Average time per eval:', -119.77809345077824)\n",
      "Params testing 296: {'n_epoch': 6.2858669553251145, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 213.31770356123266, 'dropout2': 0.5646561565437112}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/6\n",
      "13s - loss: 2.4227 - acc: 0.1380 - val_loss: 2.3946 - val_acc: 0.1518\n",
      "Epoch 2/6\n",
      "22s - loss: 2.4089 - acc: 0.1461 - val_loss: 2.3897 - val_acc: 0.1550\n",
      "Epoch 3/6\n",
      "26s - loss: 2.4028 - acc: 0.1485 - val_loss: 2.3873 - val_acc: 0.1560\n",
      "Epoch 4/6\n",
      "28s - loss: 2.3998 - acc: 0.1508 - val_loss: 2.3894 - val_acc: 0.1544\n",
      "Epoch 5/6\n",
      "29s - loss: 2.3982 - acc: 0.1516 - val_loss: 2.3871 - val_acc: 0.1575\n",
      "Epoch 6/6\n",
      "30s - loss: 2.3963 - acc: 0.1539 - val_loss: 2.3872 - val_acc: 0.1518\n",
      "('Log_loss:', 2.3872344480134386, 'It took', -156.83683395385742)\n",
      "('Average time per eval:', -119.90287035161799)\n",
      "Params testing 297: {'n_epoch': 4.71185018488241, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 138.28261495140168, 'dropout2': 0.36258630857123797}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/4\n",
      "12s - loss: 2.4168 - acc: 0.1401 - val_loss: 2.3923 - val_acc: 0.1476\n",
      "Epoch 2/4\n",
      "22s - loss: 2.4036 - acc: 0.1481 - val_loss: 2.3883 - val_acc: 0.1589\n",
      "Epoch 3/4\n",
      "25s - loss: 2.3976 - acc: 0.1511 - val_loss: 2.3893 - val_acc: 0.1574\n",
      "Epoch 4/4\n",
      "27s - loss: 2.3936 - acc: 0.1550 - val_loss: 2.3881 - val_acc: 0.1534\n",
      "('Log_loss:', 2.3880917131501613, 'It took', -94.69336009025574)\n",
      "('Average time per eval:', -119.81827467639974)\n",
      "Params testing 298: {'n_epoch': 5.559671058793409, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 42.63946302913833, 'dropout2': 0.5860284119391634}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4212 - acc: 0.1393 - val_loss: 2.3938 - val_acc: 0.1497\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4051 - acc: 0.1467 - val_loss: 2.3910 - val_acc: 0.1560\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3979 - acc: 0.1523 - val_loss: 2.3883 - val_acc: 0.1563\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3948 - acc: 0.1559 - val_loss: 2.3864 - val_acc: 0.1576\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3935 - acc: 0.1509 - val_loss: 2.3856 - val_acc: 0.1563\n",
      "('Log_loss:', 2.3856043833016773, 'It took', -121.23552322387695)\n",
      "('Average time per eval:', -119.82301463411005)\n",
      "Params testing 299: {'n_epoch': 5.451189957196775, 'optimizer': 'adam', 'layer_1': {'units1': 150, 'activation1': 'tanh', 'on1': True, 'dropout1': 0.8}, 'layer_3': {'activation3': 'relu', 'on3': False, 'units3': 16, 'dropout3': 0.25}, 'layer_2': {'on2': True, 'activation2': 'tanh', 'units2': 49.87585390660537, 'dropout2': 0.46545913969111863}, 'batch_size': 32}\n",
      "Train on 67179 samples, validate on 7466 samples\n",
      "Epoch 1/5\n",
      "12s - loss: 2.4173 - acc: 0.1388 - val_loss: 2.3942 - val_acc: 0.1487\n",
      "Epoch 2/5\n",
      "21s - loss: 2.4022 - acc: 0.1479 - val_loss: 2.3885 - val_acc: 0.1566\n",
      "Epoch 3/5\n",
      "25s - loss: 2.3966 - acc: 0.1508 - val_loss: 2.3856 - val_acc: 0.1587\n",
      "Epoch 4/5\n",
      "27s - loss: 2.3945 - acc: 0.1531 - val_loss: 2.3872 - val_acc: 0.1598\n",
      "Epoch 5/5\n",
      "28s - loss: 2.3914 - acc: 0.1521 - val_loss: 2.3858 - val_acc: 0.1532\n",
      "('Log_loss:', 2.385848819618726, 'It took', -121.48171591758728)\n",
      "('Average time per eval:', -119.82854363203049)\n",
      "best: \n",
      "{'activation2': 1, 'units2': 141.67406062812924, 'n_epoch': 6.130411162879328, 'dropout2': 0.5475628967155637}\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "times = []\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, params, algo=tpe.suggest, max_evals=300, trials=trials)\n",
    "print 'best: '\n",
    "print best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 n_epoch 2.41001885903\n",
      "1 activation2 2.41001885903\n",
      "2 units2 2.41001885903\n",
      "3 dropout2 2.41001885903\n",
      "{'activation2': 1, 'units2': 141.67406062812924, 'n_epoch': 6.1304111628793283, 'dropout2': 0.54756289671556369}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA64AAAOoCAYAAADRX6lvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABQNMMVAACAov0/aqWdRulxDvYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1366af6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = trials.trials[0]['misc']['vals'].keys()\n",
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(16,16))\n",
    "cmap = plt.cm.jet\n",
    "par_best_score = {}\n",
    "for i, val in enumerate(parameters):\n",
    "    xs = np.array([t['misc']['vals'][val] for t in trials.trials if 'loss' in t['result']]).ravel()\n",
    "    ys = [t['result']['loss'] for t in trials.trials if 'loss' in t['result']]\n",
    "    \n",
    "    par_best_score[val] = xs[ys.index(min(ys))]\n",
    "    #print trials.trials[ys.index(max(ys))]\n",
    "    print i, val, max(ys)\n",
    "    #xs, ys = zip(sorted(xs), sorted(ys))\n",
    "    #ys = np.array(ys)\n",
    "    axes[i/3,i%3].scatter(xs, ys, s=20, linewidth=0.01, alpha=0.5, c=cmap(float(i)/len(parameters)))\n",
    "    axes[i/3,i%3].set_title(val)\n",
    "print par_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 n_epoch 2.38285702477\n",
      "1 activation2 2.38285702477\n",
      "2 units2 2.38285702477\n",
      "3 dropout2 2.38285702477\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAOoCAYAAADVqnlSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4a0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x42652a7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(16,16))\n",
    "cmap = plt.cm.jet\n",
    "par_best_score = {}\n",
    "for i, val in enumerate(parameters):\n",
    "    xs = np.array([t['misc']['vals'][val] for t in trials.trials if 'loss' in t['result']]).ravel()\n",
    "    ys = [t['result']['loss'] for t in trials.trials if 'loss' in t['result']]\n",
    "    \n",
    "    #par_best_score[val] = xs[ys.index(max(ys))]\n",
    "    #print trials.trials[ys.index(max(ys))]\n",
    "    print i, val, min(ys)\n",
    "    #xs, ys = zip(sorted(xs), sorted(ys))\n",
    "    #ys = np.array(ys)\n",
    "    a=pd.DataFrame({val:xs,'loss':ys})\n",
    "    \n",
    "    if a[val].nunique()>10:\n",
    "        a[val] = pd.cut(a[val], 10, labels=np.linspace(a[val].min(),a[val].max(),10))\n",
    "    #a[val] = a[val].round(0)\n",
    "    (a.groupby(val).agg('mean')).plot(kind='bar',ylim=[2.36, 2.41], ax=axes[i/3,i%3])\n",
    "    axes[i/3,i%3].set_title(val)\n",
    "print par_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1, 1.2, 1.23, 1.3, 1.56]\n",
    "bins=np.linspace(min(x),max(x),10)\n",
    "a=pd.Series(x)\n",
    "pd.cut(a, 10, labels=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=pd.DataFrame({'activation2':xs,'loss':ys})\n",
    "a.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(a.groupby('activation2').agg('mean')).plot(kind='bar', xlim=[-2.38, -2.42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,4))\n",
    "drs = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "optim = ['adamax', 'adam', 'nadam', 'adadelta']\n",
    "cs = 'grbycm'\n",
    "for i,dr in enumerate(optim):\n",
    "    hist_all = prediction_hist(tag=optim)\n",
    "    \n",
    "    model=baseline_model(params)\n",
    "    \n",
    "    fit=model.fit(X_train.todense(), y_train, nb_epoch = 7, batch_size=32,\n",
    "                             validation_data=(X_val.todense(), y_val), verbose=2,\n",
    "                 callbacks=[hist_all]) \n",
    "    \n",
    "    plot_loss_progress(hist_all, ax1=ax1, ax2=ax2, c=cs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,4))\n",
    "\n",
    "cs = 'grbycm'\n",
    "plot_loss_progress(hist_all, ax1=ax1, ax2=ax2, c=cs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print hist_generator.predhist.head()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,4))\n",
    "plot_loss_progress(hist_all, ax1=ax1, ax2=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 32, False), val_samples=X_val.shape[0])\n",
    "scores = model.predict_generator(generator=batch_generatorp(Xtest, 32, False), val_samples=Xtest.shape[0])\n",
    "\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basescore = 2.27506319419 (2.2751433925) with weight in batch_generator: 2.2751433925, 2.2751433925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(scores, index = gatest['device_id'], columns=targetencoder.classes_)\n",
    "pred.to_csv('A_keras_model_on_apps_and_labels.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fit on complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=baseline_model()\n",
    "fit=model.fit(X_train.todense(), y_train, nb_epoch = 20, batch_size=32,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 32, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
