{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack and ensemble various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from keras.models import load_model, Sequential\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from ml_toolbox.kaggle import KaggleResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble_preds(preds, scores, w=None):\n",
    "    # preds: numpy array (n, m, k), n: samples, m: classes, k: models\n",
    "    # scores: numpy array\n",
    "    # w: 0, None -> mean\n",
    "    # w==1: weighted by score\n",
    "    # w==2: weighted by rank\n",
    "    if not w or w==0:\n",
    "        return preds.sum(axis=2)/preds.shape[2]\n",
    "    \n",
    "    if w==1:\n",
    "        tmp = np.zeros(preds.shape)\n",
    "        \n",
    "        for i in range(preds.shape[2]):\n",
    "            tmp[:,:,i] = preds[:,:,i] * (1/scores[i])\n",
    "            \n",
    "        return tmp.sum(axis=2) / np.divide(1,scores).sum()\n",
    "    \n",
    "    if w==2:\n",
    "        w = pd.Series(scores).rank(ascending=False)\n",
    "        \n",
    "        tmp = np.zeros(preds.shape)\n",
    "        \n",
    "        for i in range(preds.shape[2]):\n",
    "            tmp[:,:,i] = preds[:,:,i] * w[i]\n",
    "            \n",
    "        return tmp.sum(axis=2) / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data_ori/'\n",
    "feat_dir = './data/'\n",
    "out_dir = './model_stack/'\n",
    "\n",
    "description = 'ensemble models level 0 V0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_feature_file(fname, samples='train'):\n",
    "    if fname[-3:] == 'csv':\n",
    "        if samples=='train':\n",
    "            X = gatrain[['device_id']].merge( pd.read_csv(os.path.join(feat_dir, fname)), \n",
    "                                             on='device_id', \n",
    "                                             how='left')\n",
    "        else:\n",
    "            X = gatest[['device_id']].merge( pd.read_csv(os.path.join(feat_dir, fname)), \n",
    "                                            on='device_id', \n",
    "                                            how='left')\n",
    "            \n",
    "        X.drop('device_id', axis=1, inplace=True)\n",
    "        X.fillna(0, inplace=True)\n",
    "        \n",
    "        for c in X.columns:\n",
    "            if X[c].max()>1:\n",
    "                X[c] = MinMaxScaler().fit_transform(X)\n",
    "            \n",
    "        #print X.shape\n",
    "        return csr_matrix(X.values)\n",
    "    else:\n",
    "        # Assume it is a pickle file\n",
    "        with open(os.path.join(feat_dir, '{}_{}.pickle'.format(fname,samples)), 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CV sets\n",
    "train_cv = pd.read_csv(os.path.join(data_dir, 'gender_age_train_cv.csv'))\n",
    "test_cv = pd.read_csv(os.path.join(data_dir, 'gender_age_test_cv.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7416, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv('./data_ori/gender_age_train.csv')\n",
    "gatest = pd.read_csv('./data_ori/gender_age_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = gatrain['group']\n",
    "letarget = LabelEncoder().fit(y)\n",
    "y = letarget.transform(y)\n",
    "n_classes = len(letarget.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 models\n",
    "Models only on brand and device model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_files = ['features_brand_model_bag',\n",
    "                 'features_brand_bag',\n",
    "                 'features_brand_model.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = hstack([open_feature_file(f) for f in feature_files], format='csr')\n",
    "Xtest = hstack([open_feature_file(f,'test') for f in feature_files], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val = Xtrain[train_cv.sample_nr.values, :], Xtrain[test_cv.sample_nr.values, :]\n",
    "y_train, y_val = y[train_cv.sample_nr], y[test_cv.sample_nr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelsfile = './model_0_nn/models_nn_0_V1_2016-08-19-17-01_2.3878_2.3870.pickle'\n",
    "nfeatures = range(1800)\n",
    "\n",
    "with open(modelsfile, 'rb') as f:\n",
    "    nn_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 2.4344\n",
      "Other score: 2.3885\n",
      "CV Score: 2.4398\n",
      "Other score: 2.3866\n",
      "CV Score: 2.4412\n",
      "Other score: 2.3924\n",
      "CV Score: 2.4479\n",
      "Other score: 2.3842\n",
      "CV Score: 2.4472\n",
      "Other score: 2.3871\n"
     ]
    }
   ],
   "source": [
    "preds_val_0_nn = np.zeros((X_val.shape[0], 12, 5))\n",
    "preds_test_0_nn = np.zeros((Xtest.shape[0], 12, 5))\n",
    "\n",
    "scores = np.zeros(5)\n",
    "\n",
    "for i,m in enumerate(nn_models['models']):\n",
    "    model = load_model(m['model']) \n",
    "    \n",
    "    pred_val = model.predict_proba(X_val[:, nfeatures].todense(), batch_size = 128, verbose = 0)\n",
    "    \n",
    "    pred_test = model.predict_proba(Xtest[:, nfeatures].todense(), batch_size = 128, verbose = 0)\n",
    "    \n",
    "    score = log_loss(y_val, pred_val)\n",
    "    \n",
    "    print('CV Score: {:.4f}'.format(score))\n",
    "    print('Other score: {:.4f}'.format(m['score']))\n",
    "    \n",
    "    preds_val_0_nn[:, :, i] = pred_val\n",
    "    preds_test_0_nn[:, :, i] = pred_test\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Average score: {:.4f}'.format(np.mean(scores)))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_nn, scores, w=0))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_nn, scores, w=1))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_nn, scores, w=2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score_0_nn = log_loss(y_val, ensemble_preds(preds_val_0_nn, scores, w=2))\n",
    "preds_val_0_nn = ensemble_preds(preds_val_0_nn, scores, w=2)\n",
    "preds_test_0_nn = ensemble_preds(preds_test_0_nn, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelsfile = './model_0_logistic/models_logistic_0_V3_2016-08-18-16-17_2.3903_-1.0000.pickle'\n",
    "\n",
    "with open(modelsfile, 'rb') as f:\n",
    "    log_models = pickle.load(f)\n",
    "    \n",
    "nfeatures = range(1803)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_val_0_log = np.zeros((X_val.shape[0], 12, 5))\n",
    "preds_test_0_log = np.zeros((Xtest.shape[0], 12, 5))\n",
    "\n",
    "scores = np.zeros(5)\n",
    "\n",
    "for i,m in enumerate(log_models['models']):\n",
    "    clf = m['model']\n",
    "    \n",
    "    pred_val = clf.predict_proba(X_val[:, nfeatures])\n",
    "    \n",
    "    pred_test = clf.predict_proba(Xtest[:, nfeatures])\n",
    "    \n",
    "    score = log_loss(y_val, pred_val)\n",
    "    \n",
    "    print('CV Score: {:.4f}'.format(score))\n",
    "    print('Other score: {:.4f}'.format(m['score']))\n",
    "    \n",
    "    preds_val_0_log[:, :, i] = pred_val\n",
    "    preds_test_0_log[:, :, i] = pred_test\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Average score: {:.4f}'.format(np.mean(scores)))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_log, scores, w=0))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_log, scores, w=1))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_log, scores, w=2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score_0_log = log_loss(y_val, ensemble_preds(preds_val_0_log, scores, w=2))\n",
    "preds_val_0_log = ensemble_preds(preds_val_0_log, scores, w=2)\n",
    "preds_test_0_log = ensemble_preds(preds_test_0_log, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelsfile = './model_0_xgboost/models_xgboost_0_V3_2016-08-19-16-36_2.3905_2.3902.pickle'\n",
    "\n",
    "with open(modelsfile, 'rb') as f:\n",
    "    xgb_models = pickle.load(f)\n",
    "    \n",
    "nfeatures = xgb_models['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_val_0_xgb = np.zeros((X_val.shape[0], 12, 5))\n",
    "preds_test_0_xgb = np.zeros((Xtest.shape[0], 12, 5))\n",
    "\n",
    "scores = np.zeros(5)\n",
    "\n",
    "for i,m in enumerate(xgb_models['models']):\n",
    "    clf = m['model']\n",
    "    \n",
    "    pred_val = clf.predict(xgb.DMatrix(X_val[:, nfeatures]))\n",
    "    \n",
    "    pred_test = clf.predict(xgb.DMatrix(Xtest[:, nfeatures]))\n",
    "    \n",
    "    score = log_loss(y_val, pred_val)\n",
    "    \n",
    "    print('CV Score: {:.4f}'.format(score))\n",
    "    print('Other score: {:.4f}'.format(m['score']))\n",
    "    \n",
    "    preds_val_0_xgb[:, :, i] = pred_val\n",
    "    preds_test_0_xgb[:, :, i] = pred_test\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Average score: {:.4f}'.format(np.mean(scores)))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_xgb, scores, w=0))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_xgb, scores, w=1))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_xgb, scores, w=2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score_0_xgb = log_loss(y_val, ensemble_preds(preds_val_0_xgb, scores, w=2))\n",
    "preds_val_0_xgb = ensemble_preds(preds_val_0_xgb, scores, w=2)\n",
    "preds_test_0_xgb = ensemble_preds(preds_test_0_xgb, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenderAgeGroupProbCombined(object):\n",
    "    def __init__(self, prior_weight=(30,20), w=(1,1.3), by=('brand', 'model')):\n",
    "        self.prior_weight_brand = prior_weight[0]\n",
    "        self.prior_weight_model = prior_weight[1]\n",
    "        \n",
    "        self.w_brand = w[0]\n",
    "        self.w_model = w[1]\n",
    "        \n",
    "        self.by_brand = by[0]\n",
    "        self.by_model = by[1]\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \n",
    "        self.prior_brand = df['group'].value_counts().sort_index()/df.shape[0]\n",
    "        self.prior_model = df['group'].value_counts().sort_index()/df.shape[0]\n",
    "        \n",
    "        c_brand = df.groupby([self.by_brand, 'group']).size().unstack().fillna(0)\n",
    "        c_model = df.groupby([self.by_model, 'group']).size().unstack().fillna(0)\n",
    "        \n",
    "        self.prob_brand = (c_brand.add(self.prior_weight_brand*self.prior_brand)).div(c_brand.sum(axis=1)+self.prior_weight_brand, axis=0)\n",
    "        self.prob_model = (c_model.add(self.prior_weight_model*self.prior_model)).div(c_model.sum(axis=1)+self.prior_weight_model, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, df):\n",
    "        pred_brand = df[[self.by_brand]].merge(self.prob_brand, \n",
    "                                   how='left', \n",
    "                                   left_on=self.by_brand, \n",
    "                                   right_index=True).fillna(self.prior_brand)[self.prob_brand.columns]\n",
    "        pred_model = df[[self.by_model]].merge(self.prob_model, \n",
    "                                   how='left', \n",
    "                                   left_on=self.by_model, \n",
    "                                   right_index=True).fillna(self.prior_model)[self.prob_model.columns]\n",
    "        \n",
    "        pred_brand.loc[pred_brand.iloc[:,0].isnull(),:] = self.prior_brand\n",
    "        pred_model.loc[pred_model.iloc[:,0].isnull(),:] = self.prior_model\n",
    "        return ((pred_brand*self.w_brand + pred_model*self.w_model) / (self.w_brand + self.w_model)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68487</th>\n",
       "      <td>-1000369272589010951</td>\n",
       "      <td>F</td>\n",
       "      <td>26</td>\n",
       "      <td>F24-26</td>\n",
       "      <td>13</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>-1000572055892391496</td>\n",
       "      <td>F</td>\n",
       "      <td>27</td>\n",
       "      <td>F27-28</td>\n",
       "      <td>7</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56524</th>\n",
       "      <td>-1000643208750517791</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>M29-31</td>\n",
       "      <td>120</td>\n",
       "      <td>1581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 device_id gender  age   group  brand  model\n",
       "68487 -1000369272589010951      F   26  F24-26     13    254\n",
       "1280  -1000572055892391496      F   27  F27-28      7    141\n",
       "56524 -1000643208750517791      M   29  M29-31    120   1581"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone = pd.read_csv('./data_ori/phone_brand_device_model.csv',encoding='utf-8')\n",
    "phone = phone.drop_duplicates('device_id', keep='first')\n",
    "\n",
    "lebrand = LabelEncoder().fit(phone.phone_brand)\n",
    "phone['brand'] = lebrand.transform(phone.phone_brand)\n",
    "m = phone.phone_brand.str.cat(phone.device_model)\n",
    "lemodel = LabelEncoder().fit(m)\n",
    "phone['model'] = lemodel.transform(m)\n",
    "\n",
    "Xtrain_bay = gatrain.merge(phone[['device_id','brand','model']], how='left',on='device_id')\n",
    "Xtest_bay = gatest.merge(phone[['device_id','brand','model']], how='left',on='device_id')\n",
    "\n",
    "X_train_bay, X_val_bay = Xtrain_bay.loc[train_cv.sample_nr.values, :], Xtrain_bay.loc[test_cv.sample_nr.values, :]\n",
    "\n",
    "X_train_bay.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelsfile = './model_0_bayes/models_bayes_0_V1_2016-08-19-22-13_2.3892_-1.0000.pickle'\n",
    "\n",
    "with open(modelsfile, 'rb') as f:\n",
    "    bay_models = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 2.3892\n",
      "Other score: 2.3892\n"
     ]
    }
   ],
   "source": [
    "preds_val_0_bay = np.zeros((X_val.shape[0], 12, 1))\n",
    "preds_test_0_bay = np.zeros((Xtest.shape[0], 12, 1))\n",
    "\n",
    "scores = np.zeros(1)\n",
    "\n",
    "for i,m in enumerate(bay_models['models']):\n",
    "    clf = m['model']\n",
    "    \n",
    "    pred_val = clf.predict_proba(X_val_bay)\n",
    "    \n",
    "    pred_test = clf.predict_proba(Xtest_bay)\n",
    "    \n",
    "    score = log_loss(y_val, pred_val)\n",
    "    \n",
    "    print('CV Score: {:.4f}'.format(score))\n",
    "    print('Other score: {:.4f}'.format(m['score']))\n",
    "    \n",
    "    preds_val_0_bay[:, :, i] = pred_val\n",
    "    preds_test_0_bay[:, :, i] = pred_test\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 2.3892\n",
      "Average score: 2.3892\n",
      "Average score: 2.3892\n",
      "Average score: 2.3892\n"
     ]
    }
   ],
   "source": [
    "print('Average score: {:.4f}'.format(np.mean(scores)))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_bay, scores, w=0))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_bay, scores, w=1))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_bay, scores, w=2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_score_0_bay = score\n",
    "preds_val_0_bay = ensemble_preds(preds_val_0_bay, scores, w=2)\n",
    "preds_test_0_bay = ensemble_preds(preds_test_0_bay, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge level 0 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_all = np.zeros((X_val.shape[0], 12, 4))\n",
    "preds_all[:,:,0] = preds_val_0_nn\n",
    "preds_all[:,:,1] = preds_val_0_log\n",
    "preds_all[:,:,2] = preds_val_0_xgb\n",
    "preds_all[:,:,3] = preds_val_0_bay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = [1,4,3,2]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_all_val = ensemble_preds(preds_all, scores, w=2)\n",
    "cv_score_all = log_loss(y_val, preds_all_val)\n",
    "print(cv_score_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_all_test = np.zeros((Xtest.shape[0], 12, 4))\n",
    "preds_all_test[:,:,0] = preds_test_0_nn\n",
    "preds_all_test[:,:,1] = preds_test_0_log\n",
    "preds_all_test[:,:,2] = preds_test_0_xgb\n",
    "preds_all_test[:,:,3] = preds_test_0_bay\n",
    "preds_all_test = ensemble_preds(preds_all_test, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kag = KaggleResult(preds_all_test, \n",
    "                   gatest.device_id.values, \n",
    "                   cv_score=cv_score_all, \n",
    "                   description=description, \n",
    "                   subdir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if kag.validate()[0]:\n",
    "    kag.upload()\n",
    "print kag.lb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Store predictions using all models on train set for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = load_model(nn_models['models'][0]['model']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isinstance(clf, Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isinstance(nn_models['models'][0],'str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_using_stored_models(models_file, X, y=None, scorer=log_loss):\n",
    "\n",
    "    with open(models_file, 'rb') as f:\n",
    "        models = pickle.load(f)\n",
    "        \n",
    "    n_models = len(models['models'])\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "    preds = np.zeros((X.shape[0], n_classes, n_models))\n",
    "    scores = np.zeros(n_models)\n",
    "\n",
    "    for i,m in enumerate(models['models']):\n",
    "        if isinstance(m['model'],str):\n",
    "            # Assume link to nn model file\n",
    "            clf = load_model(m['model']) \n",
    "        else:\n",
    "            clf = m['model']\n",
    "        \n",
    "        if isinstance(clf, Sequential):\n",
    "            ## NN network\n",
    "            pred = clf.predict_proba(X.todense(), batch_size = 128, verbose = 0)\n",
    "        elif isinstance(clf, xgb.Booster):\n",
    "            ## XGBoost\n",
    "            pred = clf.predict(xgb.DMatrix(X))\n",
    "        else:\n",
    "            pred = clf.predict_proba(X)\n",
    "        \n",
    "        if y.size:\n",
    "            score = scorer(y, pred)\n",
    "            print('CV score: {:.4f}, calculated score: {:4f} '.format(m['score'], score))\n",
    "            \n",
    "            scores[i] = score\n",
    "            \n",
    "        preds[:, :, i] = pred\n",
    "        \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = './model_0_logistic/models_logistic_0_V3_2016-08-18-16-17_2.3903_-1.0000.pickle'\n",
    "a,b = predict_using_stored_models(f, X_val[:, 0:1803], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelfiles = ['./model_0_nn/models_nn_0_V1_2016-08-19-17-01_2.3878_2.3870.pickle',\n",
    "              './model_0_logistic/models_logistic_0_V3_2016-08-18-16-17_2.3903_-1.0000.pickle',\n",
    "              './model_0_xgboost/models_xgboost_0_V3_2016-08-19-16-36_2.3905_2.3902.pickle']\n",
    "nf = [1800, 1803, 1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 2.3885, calculated score: 2.434427 \n",
      "CV score: 2.3866, calculated score: 2.439837 \n",
      "CV score: 2.3924, calculated score: 2.441217 \n",
      "CV score: 2.3842, calculated score: 2.447946 \n",
      "CV score: 2.3871, calculated score: 2.447213 \n",
      "CV score: 2.3904, calculated score: 2.439151 \n",
      "CV score: 2.3903, calculated score: 2.441327 \n",
      "CV score: 2.3902, calculated score: 2.442071 \n",
      "CV score: 2.3904, calculated score: 2.443019 \n",
      "CV score: 2.3906, calculated score: 2.446342 \n",
      "CV score: 2.3870, calculated score: 2.450949 \n",
      "CV score: 2.3912, calculated score: 2.449825 \n",
      "CV score: 2.3957, calculated score: 2.449556 \n",
      "CV score: 2.3888, calculated score: 2.447271 \n",
      "CV score: 2.3898, calculated score: 2.450122 \n"
     ]
    }
   ],
   "source": [
    "for f,n in zip(modelfiles, nf):\n",
    "    a,b = predict_using_stored_models(f, X_val[:, range(n)], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 2.3892, calculated score: 2.389166 \n"
     ]
    }
   ],
   "source": [
    "a,b = predict_using_stored_models('./model_0_bayes/models_bayes_0_V1_2016-08-19-22-13_2.3892_-1.0000.pickle', X_val_bay, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 2.3885, calculated score: 2.436351 \n",
      "CV score: 2.3866, calculated score: 2.441388 \n",
      "CV score: 2.3924, calculated score: 2.440594 \n",
      "CV score: 2.3842, calculated score: 2.447777 \n",
      "CV score: 2.3871, calculated score: 2.444650 \n",
      "CV score: 2.3904, calculated score: 2.439844 \n",
      "CV score: 2.3903, calculated score: 2.441916 \n",
      "CV score: 2.3902, calculated score: 2.442943 \n",
      "CV score: 2.3904, calculated score: 2.443930 \n",
      "CV score: 2.3906, calculated score: 2.447259 \n",
      "CV score: 2.3870, calculated score: 2.451598 \n",
      "CV score: 2.3912, calculated score: 2.451175 \n",
      "CV score: 2.3957, calculated score: 2.450838 \n",
      "CV score: 2.3888, calculated score: 2.448749 \n",
      "CV score: 2.3898, calculated score: 2.451016 \n"
     ]
    }
   ],
   "source": [
    "for f,n in zip(modelfiles, nf):\n",
    "    a,b = predict_using_stored_models(f, Xtrain[:, range(n)], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ensemble_preds(a, b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7416, 12)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
