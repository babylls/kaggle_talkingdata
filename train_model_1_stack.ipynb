{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack and ensemble various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from ml_toolbox.kaggle import KaggleResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_preds(preds, scores, w=None):\n",
    "    # preds: numpy array (n, m, k), n: samples, m: classes, k: models\n",
    "    # scores: numpy array\n",
    "    # w: 0, None -> mean\n",
    "    # w==1: weighted by score\n",
    "    # w==2: weighted by rank\n",
    "    if not w or w==0:\n",
    "        return preds.sum(axis=2)/preds.shape[2]\n",
    "    \n",
    "    if w==1:\n",
    "        tmp = np.zeros(preds.shape)\n",
    "        \n",
    "        for i in range(preds.shape[2]):\n",
    "            tmp[:,:,i] = preds[:,:,i] * (1/scores[i])\n",
    "            \n",
    "        return tmp.sum(axis=2) / np.divide(1,scores).sum()\n",
    "    \n",
    "    if w==2:\n",
    "        w = pd.Series(scores).rank(ascending=False)\n",
    "        \n",
    "        tmp = np.zeros(preds.shape)\n",
    "        \n",
    "        for i in range(preds.shape[2]):\n",
    "            tmp[:,:,i] = preds[:,:,i] * w[i]\n",
    "            \n",
    "        return tmp.sum(axis=2) / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data_ori/'\n",
    "feat_dir = './data/'\n",
    "out_dir = './model_stack/'\n",
    "\n",
    "description = 'ensemble models level 1 V0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_feature_file(fname, samples='train'):\n",
    "    if fname[-3:] == 'csv':\n",
    "        if samples=='train':\n",
    "            X = gatrain[['device_id']].merge( pd.read_csv(os.path.join(feat_dir, fname)), \n",
    "                                             on='device_id', \n",
    "                                             how='left')\n",
    "        else:\n",
    "            X = gatest[['device_id']].merge( pd.read_csv(os.path.join(feat_dir, fname)), \n",
    "                                            on='device_id', \n",
    "                                            how='left')\n",
    "            \n",
    "        X.drop('device_id', axis=1, inplace=True)\n",
    "        X.fillna(0, inplace=True)\n",
    "        \n",
    "        for c in X.columns:\n",
    "            if X[c].max()>1:\n",
    "                X[c] = MinMaxScaler().fit_transform(X)\n",
    "            \n",
    "        #print X.shape\n",
    "        return csr_matrix(X.values)\n",
    "    else:\n",
    "        # Assume it is a pickle file\n",
    "        with open(os.path.join(feat_dir, '{}_{}.pickle'.format(fname,samples)), 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CV sets\n",
    "train_cv = pd.read_csv(os.path.join(data_dir, 'gender_age_train_cv.csv'))\n",
    "test_cv = pd.read_csv(os.path.join(data_dir, 'gender_age_test_cv.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv('./data_ori/gender_age_train.csv')\n",
    "gatest = pd.read_csv('./data_ori/gender_age_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = gatrain['group']\n",
    "letarget = LabelEncoder().fit(y)\n",
    "y = letarget.transform(y)\n",
    "n_classes = len(letarget.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 models\n",
    "Models only on brand and device model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_files = ['features_brand_model_bag',\n",
    "                 'features_brand_bag',\n",
    "                 'features_appid_installed',\n",
    "                 'features_label_app_installed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = hstack([open_feature_file(f) for f in feature_files], format='csr')\n",
    "Xtest = hstack([open_feature_file(f,'test') for f in feature_files], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With selection (15xxx features): 2.27427\n",
    "# Without selection (21527 features): 2.27427\n",
    "selector = VarianceThreshold().fit(Xtrain)\n",
    "Xtrain = selector.transform(Xtrain)\n",
    "Xtest = selector.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val = Xtrain[train_cv.sample_nr.values, :], Xtrain[test_cv.sample_nr.values, :]\n",
    "y_train, y_val = y[train_cv.sample_nr], y[test_cv.sample_nr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67229, 15853)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelsfile = './model_1_nn/models_nn_1_V2_2016-08-20-00-47_2.2521_-1.0000.pickle'\n",
    "\n",
    "with open(modelsfile, 'rb') as f:\n",
    "    nn_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 2.2529\n",
      "Other score: 2.2529\n",
      "CV Score: 2.2508\n",
      "Other score: 2.2508\n",
      "CV Score: 2.2507\n",
      "Other score: 2.2507\n",
      "CV Score: 2.2546\n",
      "Other score: 2.2546\n",
      "CV Score: 2.2521\n",
      "Other score: 2.2521\n"
     ]
    }
   ],
   "source": [
    "preds_val_0_nn = np.zeros((X_val.shape[0], 12, 5))\n",
    "preds_test_0_nn = np.zeros((Xtest.shape[0], 12, 5))\n",
    "\n",
    "scores = np.zeros(5)\n",
    "\n",
    "for i,m in enumerate(nn_models['models']):\n",
    "    model = load_model(m['model']) \n",
    "    \n",
    "    pred_val = model.predict_proba(X_val.todense(), batch_size = 128, verbose = 0)\n",
    "    \n",
    "    pred_test = model.predict_proba(Xtest.todense(), batch_size = 128, verbose = 0)\n",
    "    \n",
    "    score = log_loss(y_val, pred_val)\n",
    "    \n",
    "    print('CV Score: {:.4f}'.format(score))\n",
    "    print('Other score: {:.4f}'.format(m['score']))\n",
    "    \n",
    "    preds_val_0_nn[:, :, i] = pred_val\n",
    "    preds_test_0_nn[:, :, i] = pred_test\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 2.2522\n",
      "Average score: 2.2491\n",
      "Average score: 2.2491\n",
      "Average score: 2.2488\n"
     ]
    }
   ],
   "source": [
    "print('Average score: {:.4f}'.format(np.mean(scores)))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_nn, scores, w=0))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_nn, scores, w=1))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_nn, scores, w=2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score_0_nn = log_loss(y_val, ensemble_preds(preds_val_0_nn, scores, w=2))\n",
    "preds_val_0_nn = ensemble_preds(preds_val_0_nn, scores, w=2)\n",
    "preds_test_0_nn = ensemble_preds(preds_test_0_nn, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelsfile = './model_0_logistic/models_logistic_0_V3_2016-08-18-16-17_2.3903_-1.0000.pickle'\n",
    "\n",
    "with open(modelsfile, 'rb') as f:\n",
    "    log_models = pickle.load(f)\n",
    "    \n",
    "nfeatures = range(1803)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_val_0_log = np.zeros((X_val.shape[0], 12, 5))\n",
    "preds_test_0_log = np.zeros((Xtest.shape[0], 12, 5))\n",
    "\n",
    "scores = np.zeros(5)\n",
    "\n",
    "for i,m in enumerate(log_models['models']):\n",
    "    clf = m['model']\n",
    "    \n",
    "    pred_val = clf.predict_proba(X_val[:, nfeatures])\n",
    "    \n",
    "    pred_test = clf.predict_proba(Xtest[:, nfeatures])\n",
    "    \n",
    "    score = log_loss(y_val, pred_val)\n",
    "    \n",
    "    print('CV Score: {:.4f}'.format(score))\n",
    "    print('Other score: {:.4f}'.format(m['score']))\n",
    "    \n",
    "    preds_val_0_log[:, :, i] = pred_val\n",
    "    preds_test_0_log[:, :, i] = pred_test\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Average score: {:.4f}'.format(np.mean(scores)))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_log, scores, w=0))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_log, scores, w=1))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_log, scores, w=2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score_0_log = log_loss(y_val, ensemble_preds(preds_val_0_log, scores, w=2))\n",
    "preds_val_0_log = ensemble_preds(preds_val_0_log, scores, w=2)\n",
    "preds_test_0_log = ensemble_preds(preds_test_0_log, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelsfile = './model_1_xgboost/models_xgboost_0_V2_2016-08-18-21-24_2.2740_2.26536.pickle'\n",
    "\n",
    "with open(modelsfile, 'rb') as f:\n",
    "    xgb_models = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 2.4481\n",
      "Other score: 2.2759\n",
      "CV Score: 2.4483\n",
      "Other score: 2.2735\n",
      "CV Score: 2.4481\n",
      "Other score: 2.2692\n",
      "CV Score: 2.4489\n",
      "Other score: 2.2782\n",
      "CV Score: 2.4482\n",
      "Other score: 2.2733\n"
     ]
    }
   ],
   "source": [
    "preds_val_0_xgb = np.zeros((X_val.shape[0], 12, 5))\n",
    "preds_test_0_xgb = np.zeros((Xtest.shape[0], 12, 5))\n",
    "\n",
    "scores = np.zeros(5)\n",
    "\n",
    "for i,m in enumerate(xgb_models['models']):\n",
    "    clf = m['model']\n",
    "    \n",
    "    pred_val = clf.predict(xgb.DMatrix(X_val[:, nfeatures]))\n",
    "    \n",
    "    pred_test = clf.predict(xgb.DMatrix(Xtest[:, nfeatures]))\n",
    "    \n",
    "    score = log_loss(y_val, pred_val)\n",
    "    \n",
    "    print('CV Score: {:.4f}'.format(score))\n",
    "    print('Other score: {:.4f}'.format(m['score']))\n",
    "    \n",
    "    preds_val_0_xgb[:, :, i] = pred_val\n",
    "    preds_test_0_xgb[:, :, i] = pred_test\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 2.4483\n",
      "Average score: 2.4480\n",
      "Average score: 2.4480\n",
      "Average score: 2.4479\n"
     ]
    }
   ],
   "source": [
    "print('Average score: {:.4f}'.format(np.mean(scores)))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_xgb, scores, w=0))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_xgb, scores, w=1))))\n",
    "print('Average score: {:.4f}'.format(log_loss(y_val, \n",
    "                                              ensemble_preds(preds_val_0_xgb, scores, w=2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score_0_xgb = log_loss(y_val, ensemble_preds(preds_val_0_xgb, scores, w=2))\n",
    "preds_val_0_xgb = ensemble_preds(preds_val_0_xgb, scores, w=2)\n",
    "preds_test_0_xgb = ensemble_preds(preds_test_0_xgb, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge level 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_all = np.zeros((X_val.shape[0], 12, 1))\n",
    "preds_all[:,:,0] = preds_val_0_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [1]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.248813447\n"
     ]
    }
   ],
   "source": [
    "preds_all_val = ensemble_preds(preds_all, scores, w=2)\n",
    "cv_score_all = log_loss(y_val, preds_all_val)\n",
    "print(cv_score_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_all_test = np.zeros((Xtest.shape[0], 12, 1))\n",
    "preds_all_test[:,:,0] = preds_test_0_nn\n",
    "preds_all_test = ensemble_preds(preds_all_test, scores, w=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kag = KaggleResult(preds_all_test, \n",
    "                   gatest.device_id.values, \n",
    "                   cv_score=cv_score_all, \n",
    "                   description=description, \n",
    "                   subdir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24184\n"
     ]
    }
   ],
   "source": [
    "if kag.validate()[0]:\n",
    "    kag.upload()\n",
    "print kag.lb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Merge with level 0  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_lvl_0 = pd.read_csv(os.path.join(out_dir,'submission_2.3713_2016-08-19-22-36_2.3859.csv'), index_col='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CV sets with events\n",
    "train_cv_w = pd.read_csv(os.path.join(data_dir, 'gender_age_train_cv_w.csv'))\n",
    "test_cv_w = pd.read_csv(os.path.join(data_dir, 'gender_age_val_cv_w.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>sample_nr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002079943728939269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1547860181818787117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7374582448058474277</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6220210354783429585</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6873889408535437611</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             device_id  sample_nr\n",
       "0  1002079943728939269          0\n",
       "1 -1547860181818787117          1\n",
       "2  7374582448058474277          2\n",
       "3 -6220210354783429585          3\n",
       "4  6873889408535437611         10"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cv_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_lvl_0.iloc[test_cv_w.sample_nr.values,:] = preds_all_test[test_cv_w.sample_nr.values,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kag = KaggleResult(preds_lvl_0, \n",
    "                   gatest.device_id.values, \n",
    "                   cv_score=cv_score_all, \n",
    "                   description='models lvl 0 all - lvl 1 nn', \n",
    "                   subdir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23768\n"
     ]
    }
   ],
   "source": [
    "if kag.validate()[0]:\n",
    "    kag.upload()\n",
    "print kag.lb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
